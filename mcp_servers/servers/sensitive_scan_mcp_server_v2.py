#!/usr/bin/env python3
"""
Sensitive Data Scanner MCP Server V2
æ•æ„Ÿæ•°æ®æ‰«æMCPæœåŠ¡å™¨ V2 ç‰ˆæœ¬
ä½¿ç”¨ LangExtract è¿›è¡Œç²¾ç¡®çš„æ•æ„Ÿä¿¡æ¯æå–å’Œå¯è§†åŒ–
"""

import json
import logging
import os
import asyncio
import re
import aiofiles
from pathlib import Path
from asyncio import Semaphore
from typing import Dict, Any, Optional, List
from datetime import datetime
from fastmcp import FastMCP
from base_config import MCPServerConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºMCPæœåŠ¡å™¨å®ä¾‹
mcp = FastMCP("Sensitive Data Scanner Server V2")

# åŠ è½½é…ç½®
config = MCPServerConfig('sensitive_scan_server_v2')

# è·å–LangExtracté…ç½®
LANGEXTRACT_PROVIDER = config.get('langextract_provider', 'gemini')  # gemini, openai, custom
LANGEXTRACT_MODEL = config.get('langextract_model', 'gemini-2.0-flash-exp')
LANGEXTRACT_API_KEY = config.get('langextract_api_key', '')
LANGEXTRACT_BASE_URL = config.get('langextract_base_url', '')  # ç”¨äºè‡ªå®šä¹‰APIåœ°å€
VISUALIZATION_OUTPUT_DIR = config.get('visualization_output_dir', '/tmp/scan_visualizations')

# è·å–åˆ†å—å¤§å°é…ç½®ï¼ˆé»˜è®¤10000å­—ç¬¦ï¼‰
CHUNK_SIZE = config.get('chunk_size', 10000)

# è·å–æ–‡ä»¶å¹¶å‘åº¦é…ç½®ï¼ˆé»˜è®¤3ä¸ªæ–‡ä»¶åŒæ—¶æ‰«æï¼‰
FILE_CONCURRENCY = config.get('file_concurrency', 3)

# è·å–æ–‡æ¡£å­˜å‚¨è·¯å¾„é…ç½®
DOCUMENT_STORAGE_PATH = config.get('document_storage_path', '/tmp/documents/uploads')

logger.info(f"æ–‡æ¡£å­˜å‚¨è·¯å¾„é…ç½®: DOCUMENT_STORAGE_PATH = {DOCUMENT_STORAGE_PATH}")

# åˆå§‹åŒ–LangExtractæ‰«æå™¨
try:
    from langextract_sensitive_scanner import LangExtractSensitiveScanner
    
    # å‡†å¤‡APIå¯†é’¥
    api_key = LANGEXTRACT_API_KEY
    if not api_key:
        # æ ¹æ®æä¾›å•†ä»ç¯å¢ƒå˜é‡è¯»å–
        if LANGEXTRACT_PROVIDER == 'gemini':
            api_key = os.environ.get('GOOGLE_API_KEY')
        elif LANGEXTRACT_PROVIDER == 'openai' or LANGEXTRACT_PROVIDER == 'custom':
            api_key = os.environ.get('OPENAI_API_KEY')
    
    langextract_scanner = LangExtractSensitiveScanner(
        model_id=LANGEXTRACT_MODEL,
        api_key=api_key,
        provider=LANGEXTRACT_PROVIDER,
        base_url=LANGEXTRACT_BASE_URL if LANGEXTRACT_PROVIDER == 'custom' else None,
        enable_visualization=True  # å¯ç”¨å¯è§†åŒ–ä»¥æ”¯æŒåŸç”ŸæŠ¥å‘Šç”Ÿæˆ
    )
    logger.info(f"LangExtract æ‰«æå™¨å·²åˆå§‹åŒ– (æä¾›å•†: {LANGEXTRACT_PROVIDER}, æ¨¡å‹: {LANGEXTRACT_MODEL})")
except ImportError as e:
    logger.error(f"æ— æ³•å¯¼å…¥ LangExtract: {str(e)}")
    logger.error("è¯·ç¡®ä¿ langextract_sensitive_scanner.py åœ¨åŒç›®å½•ä¸‹")
    raise ImportError(f"æ— æ³•å¯¼å…¥ LangExtract æ¨¡å—: {str(e)}")
except Exception as e:
    logger.error(f"åˆå§‹åŒ– LangExtract æ‰«æå™¨å¤±è´¥: {str(e)}")
    raise Exception(f"LangExtract æ‰«æå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")


async def read_content_from_file(file_path: str) -> str:
    """ä»æ–‡ä»¶è¯»å–å†…å®¹"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'cp1252', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            async with aiofiles.open(file_path, 'r', encoding=encoding) as f:
                content = await f.read()
                logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç ä»æ–‡ä»¶è¯»å–å†…å®¹: {file_path}")
                return content
        except FileNotFoundError:
            logger.error(f"è§£ææ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return ""
        except UnicodeDecodeError:
            continue
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                logger.error(f"è§£ææ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return ""
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é”™è¯¯å¤„ç†ç­–ç•¥
    try:
        async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = await f.read()
            logger.warning(f"ä½¿ç”¨ utf-8 ç¼–ç ï¼ˆå¿½ç•¥é”™è¯¯ï¼‰è¯»å–æ–‡ä»¶: {file_path}")
            return content
    except FileNotFoundError:
        logger.error(f"è§£ææ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return ""
    except Exception as e:
        logger.error(f"è¯»å–è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return ""


async def get_file_content_from_filesystem(file_id: str) -> Dict[str, Any]:
    """ä»æ–‡ä»¶ç³»ç»Ÿè·å–æ–‡ä»¶å†…å®¹"""
    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        base_path = Path(DOCUMENT_STORAGE_PATH)
        
        # æŸ¥æ‰¾åŸå§‹æ–‡ä»¶å’Œè§£æåçš„æ–‡ä»¶
        original_file_pattern = f"{file_id}.*"
        parsed_file_path = base_path / f"{file_id}.parse.txt"
        
        # è·å–æ–‡ä»¶å…ƒæ•°æ®
        file_metadata = {}
        file_name = f"document_{file_id}"
        file_type = "unknown"
        file_size = 0
        
        # æŸ¥æ‰¾åŸå§‹æ–‡ä»¶è·å–æ–‡ä»¶ä¿¡æ¯
        if not file_name.startswith("document_"):
            for file_path in base_path.glob(original_file_pattern):
                if not str(file_path).endswith('.parse.txt'):
                    file_name = file_path.name
                    file_type = file_path.suffix[1:] if file_path.suffix else 'unknown'
                    file_size = file_path.stat().st_size
                    break
        
        # æ£€æŸ¥è§£æåçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not parsed_file_path.exists():
            # å°è¯•ç›´æ¥è¯»å–åŸå§‹æ–‡æœ¬æ–‡ä»¶
            for file_path in base_path.glob(original_file_pattern):
                if file_path.suffix in ['.txt', '.md']:
                    content = await read_content_from_file(str(file_path))
                    if content:
                        break
            else:
                logger.error(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {file_id}")
                return {
                    'success': False,
                    'content': '',
                    'error': f'æ‰¾ä¸åˆ°æ–‡ä»¶: {file_id}'
                }
        else:
            # è¯»å–è§£æåçš„å†…å®¹
            content = await read_content_from_file(str(parsed_file_path))
            if not content:
                return {
                    'success': False,
                    'content': '',
                    'error': f'æ— æ³•è¯»å–è§£ææ–‡ä»¶: {parsed_file_path}'
                }
        
        # ç»Ÿè®¡å›¾ç‰‡æ•°é‡ï¼ˆåŒ¹é… [å›¾ç‰‡ æ•°å­—] æ ¼å¼ï¼‰
        image_count = len(re.findall(r'\[å›¾ç‰‡\s*\d*\]', content))
        
        # è·å–å­—ç¬¦æ•°
        char_count = file_metadata.get('char_count', len(content))
        
        return {
            'success': True,
            'content': content,
            'file_name': file_name,
            'file_type': file_type,
            'file_size': file_size,
            'doc_metadata': file_metadata,
            'image_count': image_count,
            'char_count': char_count
        }
            
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
        return {
            'success': False,
            'content': '',
            'error': f'ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}'
        }


async def scan_content_with_langextract(content: str, file_name: str = "æœªçŸ¥æ–‡ä»¶") -> Dict[str, Any]:
    """ä½¿ç”¨LangExtractæ‰«æå†…å®¹ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
    try:
        # ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼ˆå› ä¸ºlangextractç›®å‰æ˜¯åŒæ­¥çš„ï¼‰
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None, 
            langextract_scanner.scan_text,
            content,
            file_name
        )
        
        # ç›´æ¥è¿”å›Scannerçš„ç»“æœ
        return result
            
    except Exception as e:
        logger.error(f"LangExtractæ‰«æå¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'document_name': file_name
        }


async def scan_single_file(file_id: str) -> Dict[str, Any]:
    """æ‰«æå•ä¸ªæ–‡ä»¶å¹¶è¿”å›ç»“æœå­—å…¸"""
    file_data = await get_file_content_from_filesystem(file_id)
    
    if not file_data['success']:
        return {
            'file_id': file_id,
            'success': False,
            'error': file_data['error']
        }
    
    # ä½¿ç”¨LangExtractæ‰«æ
    scan_result = await scan_content_with_langextract(
        file_data['content'], 
        file_data['file_name']
    )
    
    if not scan_result['success']:
        return {
            'file_id': file_id,
            'file_name': file_data['file_name'],
            'success': False,
            'error': scan_result.get('error', 'æœªçŸ¥é”™è¯¯')
        }
    
    # åˆå¹¶æ–‡ä»¶ä¿¡æ¯å’Œæ‰«æç»“æœ
    return {
        **scan_result,  # ç›´æ¥ä½¿ç”¨Scannerè¿”å›çš„æ‰€æœ‰å­—æ®µ
        'file_id': file_id,
        'file_name': file_data['file_name'],  # æ·»åŠ file_nameä»¥ä¿æŒå…¼å®¹æ€§
        'file_type': file_data['file_type'],
        'file_size': file_data['file_size'],
        'image_count': file_data.get('image_count', 0),
        'char_count': file_data.get('char_count', 0)
    }


@mcp.tool()
async def scan_document_v2(file_ids: List[str]) -> str:
    """
    æ‰«ææ–‡æ¡£ä¸­çš„æ•æ„Ÿä¿¡æ¯
    
    Args:
        file_ids: æ–‡ä»¶IDåˆ—è¡¨ï¼ˆæ–‡ä»¶ç³»ç»Ÿä¸­çš„file_idåˆ—è¡¨ï¼‰
    
    Returns:
        æ‰«æç»“æœæŠ¥å‘Šï¼ˆåŒ…å«å¯è§†åŒ–æŠ¥å‘Šé“¾æ¥ï¼‰
    """
    try:
        logger.info(f"å¼€å§‹æ‰«æ {len(file_ids)} ä¸ªæ–‡ä»¶")
        
        if not file_ids:
            return "é”™è¯¯: æœªæä¾›æ–‡ä»¶ID"
        
        # ç»Ÿä¸€ä½¿ç”¨æ‰¹é‡å¤„ç†é€»è¾‘ï¼ˆæ— è®ºæ˜¯1ä¸ªè¿˜æ˜¯å¤šä¸ªæ–‡ä»¶ï¼‰
        output = ""
        
        # æ˜¾ç¤ºæ‰«ææŠ¥å‘Šå¤´éƒ¨
        output = f"æ‰«ææŠ¥å‘Š\n"
        output += f"{'='*50}\n"
        output += f"æ‰«ææ–‡ä»¶æ•°: {len(file_ids)}\n"
        output += f"æ‰«ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        output += f"æ‰«æå¼•æ“: LangExtract ({LANGEXTRACT_PROVIDER})\n"
        output += f"ä½¿ç”¨æ¨¡å‹: {LANGEXTRACT_MODEL}\n"
        output += f"{'='*50}\n\n"
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘åº¦
        semaphore = Semaphore(FILE_CONCURRENCY)
        
        async def scan_with_semaphore(file_id: str):
            async with semaphore:
                return await scan_single_file(file_id)
        
        # å¹¶å‘æ‰«ææ‰€æœ‰æ–‡ä»¶
        scan_tasks = [scan_with_semaphore(file_id) for file_id in file_ids]
        scan_results = await asyncio.gather(*scan_tasks, return_exceptions=True)
        
        # å¤„ç†æ‰«æç»“æœ
        total_sensitive_count = 0
        files_with_sensitive = 0
        langextract_results = []  # ç”¨äºå¯è§†åŒ–
        
        for idx, scan_data in enumerate(scan_results, 1):
            # å¤„ç†å¼‚å¸¸æƒ…å†µ
            if isinstance(scan_data, Exception):
                output += f"\nå†…å®¹æº{idx}: {file_ids[idx-1]}\n"
                output += f"æ‰«æå¤±è´¥: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ - {str(scan_data)}\n"
                output += "="*50 + "\n"
                continue
                
            if not scan_data.get('success', False):
                output += f"\nå†…å®¹æº{idx}: {scan_data.get('file_id', file_ids[idx-1])}\n"
                output += f"æ‰«æå¤±è´¥: {scan_data.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
                output += "="*50 + "\n"
                continue
            
            # æ”¶é›†LangExtractç»“æœç”¨äºå¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨äº†å¯è§†åŒ–ï¼‰
            if scan_data.get('langextract_result'):
                langextract_results.append(scan_data['langextract_result'])
            
            # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
            file_size_kb = scan_data.get('file_size', 0) / 1024
            
            # åˆ¤æ–­è§£æçŠ¶æ€
            parse_status = "å†…å®¹å®Œæ•´"
            
            # è¾“å‡ºæ–‡ä»¶ç»“æœ
            output += f"\nå†…å®¹æº{idx}: {scan_data.get('document_name', scan_data.get('file_name', 'æœªçŸ¥æ–‡ä»¶'))}\n"
            output += f"1.æ–‡æ¡£ä¿¡æ¯ï¼š{file_size_kb:.1f}KBã€æ–‡å­—{scan_data.get('char_count', 0)}"
            if scan_data.get('image_count', 0) > 0:
                output += f"(åŒ…å«å›¾ç‰‡{scan_data['image_count']}å¼ çš„è§£æå†…å®¹)"
            output += "\n"
            output += f"2.æ–‡æ¡£è§£æçŠ¶æ€ï¼š{parse_status}\n"
            # æ˜¾ç¤ºæ–‡æ¡£æ‘˜è¦
            document_summary = scan_data.get('document_summary', '')
            if document_summary:
                output += f"3.æ–‡æ¡£æ‘˜è¦ï¼š{document_summary}\n"
            else:
                output += f"3.æ–‡æ¡£æ‘˜è¦ï¼šæ— æ‘˜è¦\n"
            output += f"4.æ•æ„Ÿä¿¡æ¯æ‰«æç»“æœï¼š"
            
            if scan_data.get('has_sensitive', False):
                files_with_sensitive += 1
                total_sensitive_count += scan_data.get('sensitive_count', 0)
                output += f"å‘ç°{scan_data.get('sensitive_count', 0)}ä¸ªæ•æ„Ÿä¿¡æ¯\n"
                # æœ€å¤šå±•ç¤º3ä¸ª
                sensitive_items = scan_data.get('sensitive_items', [])
                for i, item in enumerate(sensitive_items[:3], 1):
                    output += f"  {i}) {item.get('type', 'æœªçŸ¥ç±»å‹')}: {item.get('masked_value', '***')}\n"
                if len(sensitive_items) > 3:
                    output += f"  ...è¿˜æœ‰{len(sensitive_items)-3}ä¸ªæ•æ„Ÿä¿¡æ¯æœªå±•ç¤º\n"
            else:
                output += "æœªå‘ç°æ•æ„Ÿä¿¡æ¯\n"
            
            output += "="*50 + "\n"
        
        # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡
        output += f"\n{'='*50}\n"
        output += f"æ‰«ææ±‡æ€»:\n"
        output += f"   - æ‰«ææ–‡ä»¶æ€»æ•°: {len(file_ids)}\n"
        output += f"   - åŒ…å«æ•æ„Ÿä¿¡æ¯çš„æ–‡ä»¶: {files_with_sensitive}\n"
        output += f"   - æ•æ„Ÿä¿¡æ¯æ€»æ•°: {total_sensitive_count}\n"
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = {
            "scan_time": datetime.now().isoformat(),
            "total_files": len(file_ids),
            "total_sensitive": total_sensitive_count,
            "files_with_sensitive": files_with_sensitive,
            "engine": "LangExtract",
            "model": LANGEXTRACT_MODEL,
            "items": [],
            "statistics": {}
        }
        
        # æ”¶é›†æ‰€æœ‰æ•æ„Ÿä¿¡æ¯é¡¹
        for scan_data in scan_results:
            if isinstance(scan_data, dict) and scan_data.get('success') and scan_data.get('has_sensitive'):
                file_name = scan_data.get('document_name', scan_data.get('file_name', 'æœªçŸ¥æ–‡ä»¶'))
                
                # ç›´æ¥ä½¿ç”¨Scannerè¿”å›çš„sensitive_items
                for item in scan_data.get('sensitive_items', []):
                    report_item = {
                        "type": item.get('type', 'æœªçŸ¥ç±»å‹'),
                        "masked_value": item.get('masked_value', '***'),
                        "context": item.get('context', ''),
                        "file": file_name,
                        "file_id": scan_data.get('file_id', '')  # æ·»åŠ file_id
                    }
                    report_data["items"].append(report_item)
                
                # ç›´æ¥ä½¿ç”¨Scannerè¿”å›çš„sensitive_statsç»Ÿè®¡
                if scan_data.get('sensitive_stats'):
                    for item_type, count in scan_data['sensitive_stats'].items():
                        if item_type not in report_data["statistics"]:
                            report_data["statistics"][item_type] = 0
                        report_data["statistics"][item_type] += count
        
        # ä¿å­˜æŠ¥å‘Šæ•°æ®ä¸ºJSON
        if total_sensitive_count > 0:
            try:
                os.makedirs(VISUALIZATION_OUTPUT_DIR, exist_ok=True)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                report_filename = f"scan_report_{timestamp}.json"
                report_path = os.path.join(VISUALIZATION_OUTPUT_DIR, report_filename)
                
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(report_data, f, ensure_ascii=False, indent=2)
                
                # ç”ŸæˆæŠ¥å‘Šæ ‡è¯†ç¬¦
                report_id = f"[REPORT:SENSITIVE_SCAN:{report_filename}:æŸ¥çœ‹å®Œæ•´æ‰«ææŠ¥å‘Š]"
                output += f"\n{'='*50}\n"
                output += f"ğŸ“Š æ‰«ææŠ¥å‘Š:\n"
                output += f"   {report_id}\n"
                logger.info(f"æ‰«ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
                
                # ç”ŸæˆLangExtract HTMLå¯è§†åŒ–æŠ¥å‘Š
                if langextract_results:
                    try:
                        viz_filename = f"scan_viz_{timestamp}.html"
                        viz_path = os.path.join(VISUALIZATION_OUTPUT_DIR, viz_filename)
                        
                        loop = asyncio.get_event_loop()
                        html_path = await loop.run_in_executor(
                            None,
                            langextract_scanner.generate_visualization,
                            langextract_results,
                            viz_path
                        )
                        
                        if html_path:
                            logger.info(f"LangExtractå¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {html_path}")
                    except Exception as e:
                        logger.error(f"ç”ŸæˆLangExtractå¯è§†åŒ–æŠ¥å‘Šå¤±è´¥: {str(e)}")
            except Exception as e:
                logger.error(f"ä¿å­˜æŠ¥å‘Šæ•°æ®å¤±è´¥: {str(e)}")
                output += f"\nä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}\n"
        
        return output.rstrip()  # å»æ‰æœ«å°¾æ¢è¡Œ
        
    except Exception as e:
        logger.error(f"æ‰«ææ–‡æ¡£è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}", exc_info=True)
        return f"é”™è¯¯: æ‰«æè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ - {str(e)}"


if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡å™¨
    port = config.get('port', 3008)
    logger.info(f"Starting Sensitive Data Scanner MCP Server V2 on port {port}")
    
    # æ˜¾ç¤ºå¼•æ“é…ç½®ä¿¡æ¯
    logger.info(f"æ‰«æå¼•æ“: LangExtract")
    logger.info(f"æ¨¡å‹: {LANGEXTRACT_MODEL}")
    logger.info(f"æä¾›å•†: {LANGEXTRACT_PROVIDER}")
    if LANGEXTRACT_PROVIDER == 'custom':
        logger.info(f"APIåœ°å€: {LANGEXTRACT_BASE_URL}")
    logger.info(f"å¯è§†åŒ–: å·²å¯ç”¨ (è¾“å‡ºç›®å½•: {VISUALIZATION_OUTPUT_DIR})")
    logger.info(f"åˆ†å—å¤§å°: {CHUNK_SIZE} å­—ç¬¦")
    logger.info(f"æ–‡ä»¶å¹¶å‘åº¦: {FILE_CONCURRENCY}")
    
    mcp.run(transport="streamable-http", host="0.0.0.0", port=port)