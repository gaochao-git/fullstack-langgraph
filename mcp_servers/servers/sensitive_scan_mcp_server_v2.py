#!/usr/bin/env python3
"""
Sensitive Data Scanner MCP Server V2
æ•æ„Ÿæ•°æ®æ‰«æMCPæœåŠ¡å™¨ V2 ç‰ˆæœ¬
é›†æˆ LangExtract è¿›è¡Œæ›´ç²¾ç¡®çš„æ•æ„Ÿä¿¡æ¯æå–å’Œå¯è§†åŒ–
"""

import json
import logging
import os
import asyncio
import re
import aiofiles
from pathlib import Path
from asyncio import Semaphore
from typing import Dict, Any, Optional, List
from datetime import datetime
from fastmcp import FastMCP
from base_config import MCPServerConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºMCPæœåŠ¡å™¨å®ä¾‹
mcp = FastMCP("Sensitive Data Scanner Server V2")

# åŠ è½½é…ç½®
config = MCPServerConfig('sensitive_scan_server_v2')

# è·å–é…ç½®
USE_LANGEXTRACT = config.get('use_langextract', False)
LANGEXTRACT_PROVIDER = config.get('langextract_provider', 'gemini')  # gemini, openai, custom
LANGEXTRACT_MODEL = config.get('langextract_model', 'gemini-2.0-flash-exp')
LANGEXTRACT_API_KEY = config.get('langextract_api_key', '')
LANGEXTRACT_BASE_URL = config.get('langextract_base_url', '')  # ç”¨äºè‡ªå®šä¹‰APIåœ°å€
VISUALIZATION_OUTPUT_DIR = config.get('visualization_output_dir', '/tmp/scan_visualizations')

# è·å–åˆ†å—å¤§å°é…ç½®ï¼ˆé»˜è®¤10000å­—ç¬¦ï¼‰
CHUNK_SIZE = config.get('chunk_size', 10000)

# è·å–æ–‡ä»¶å¹¶å‘åº¦é…ç½®ï¼ˆé»˜è®¤3ä¸ªæ–‡ä»¶åŒæ—¶æ‰«æï¼‰
FILE_CONCURRENCY = config.get('file_concurrency', 3)

# è·å–æ–‡æ¡£å­˜å‚¨è·¯å¾„é…ç½®
DOCUMENT_STORAGE_PATH = config.get('document_storage_path', '/tmp/documents/uploads')

logger.info(f"æ–‡æ¡£å­˜å‚¨è·¯å¾„é…ç½®: DOCUMENT_STORAGE_PATH = {DOCUMENT_STORAGE_PATH}")
logger.info(f"ä½¿ç”¨ LangExtract: {USE_LANGEXTRACT}")

# æ ¹æ®é…ç½®é€‰æ‹©æ‰«æå™¨
if USE_LANGEXTRACT:
    try:
        from langextract_sensitive_scanner import LangExtractSensitiveScanner
        
        # å‡†å¤‡APIå¯†é’¥
        api_key = LANGEXTRACT_API_KEY
        if not api_key:
            # æ ¹æ®æä¾›å•†ä»ç¯å¢ƒå˜é‡è¯»å–
            if LANGEXTRACT_PROVIDER == 'gemini':
                api_key = os.environ.get('GOOGLE_API_KEY')
            elif LANGEXTRACT_PROVIDER == 'openai' or LANGEXTRACT_PROVIDER == 'custom':
                api_key = os.environ.get('OPENAI_API_KEY')
        
        langextract_scanner = LangExtractSensitiveScanner(
            model_id=LANGEXTRACT_MODEL,
            api_key=api_key,
            provider=LANGEXTRACT_PROVIDER,
            base_url=LANGEXTRACT_BASE_URL if LANGEXTRACT_PROVIDER == 'custom' else None,
            enable_visualization=True  # å¯ç”¨å¯è§†åŒ–ä»¥æ”¯æŒåŸç”ŸæŠ¥å‘Šç”Ÿæˆ
        )
        logger.info(f"å·²å¯ç”¨ LangExtract æ‰«æå™¨ (æä¾›å•†: {LANGEXTRACT_PROVIDER}, æ¨¡å‹: {LANGEXTRACT_MODEL})")
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥ LangExtract: {str(e)}")
        logger.error("è¯·ç¡®ä¿ langextract_sensitive_scanner.py åœ¨åŒç›®å½•ä¸‹")
        # å¼ºåˆ¶ä½¿ç”¨ LangExtractï¼Œä¸å›é€€
        raise ImportError(f"å¿…é¡»ä½¿ç”¨ LangExtract ä½†å¯¼å…¥å¤±è´¥: {str(e)}")
    except Exception as e:
        logger.error(f"åˆå§‹åŒ– LangExtract æ‰«æå™¨å¤±è´¥: {str(e)}")
        # å¼ºåˆ¶ä½¿ç”¨ LangExtractï¼Œä¸å›é€€
        raise Exception(f"å¿…é¡»ä½¿ç”¨ LangExtract ä½†åˆå§‹åŒ–å¤±è´¥: {str(e)}")
else:
    langextract_scanner = None

# å¦‚æœä¸ä½¿ç”¨ LangExtractï¼Œåˆ™ä½¿ç”¨åŸæœ‰çš„ LangChain å®ç°
if not USE_LANGEXTRACT:
    # LangChain imports
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import SystemMessage, HumanMessage
    from langchain_core.output_parsers import JsonOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    
    # è·å–LLMé…ç½®
    LLM_API_BASE = config.get('llm_api_base') or os.environ.get('DEEPSEEK_BASE_URL', 'https://api.deepseek.com/v1')
    LLM_API_KEY = config.get('llm_api_key') or os.environ.get('DEEPSEEK_API_KEY', '')
    LLM_MODEL = config.get('llm_model') or os.environ.get('LLM_MODEL', 'deepseek-chat')
    
    # åˆå§‹åŒ–LangChain LLM
    llm = ChatOpenAI(
        model=LLM_MODEL,
        openai_api_base=LLM_API_BASE,
        openai_api_key=LLM_API_KEY,
        temperature=0.1,
        timeout=60.0,
        max_tokens=1000
    )
    
    # JSONè¾“å‡ºè§£æå™¨
    json_parser = JsonOutputParser()
    
    # é»˜è®¤æ‰«ææç¤ºè¯
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•æ„Ÿæ•°æ®æ‰«æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ‰«ææ–‡æœ¬ä¸­çš„æ•æ„Ÿä¿¡æ¯å¹¶ç”Ÿæˆè„±æ•åçš„å®‰å…¨æŠ¥å‘Šã€‚

éœ€è¦è¯†åˆ«çš„æ•æ„Ÿä¿¡æ¯ç±»å‹ï¼š
1. ä¸ªäººèº«ä»½ä¿¡æ¯ï¼šèº«ä»½è¯å·ã€æŠ¤ç…§å·ã€é©¾é©¶è¯å·
2. è”ç³»æ–¹å¼ï¼šæ‰‹æœºå·ã€åº§æœºå·ã€é‚®ç®±åœ°å€
3. é‡‘èä¿¡æ¯ï¼šé“¶è¡Œå¡å·ã€ä¿¡ç”¨å¡å·ã€è´¦å·ä¿¡æ¯
4. è´¦æˆ·å‡­æ®ï¼šç”¨æˆ·åå¯†ç ç»„åˆã€APIå¯†é’¥ã€Tokenã€è¯ä¹¦å¯†é’¥
5. ç½‘ç»œä¿¡æ¯ï¼šå†…ç½‘IPåœ°å€ã€æœåŠ¡å™¨åœ°å€ã€æ•°æ®åº“è¿æ¥ä¸²
6. åŒ»ç–—ä¿¡æ¯ï¼šç—…å†å·ã€åŒ»ä¿å·ã€è¯Šæ–­ä¿¡æ¯
7. å…¶ä»–æ•æ„Ÿï¼šç¤¾ä¿å·ã€è½¦ç‰Œå·ã€å®¶åº­ä½å€

é‡è¦æç¤ºï¼š
- å•ç‹¬çš„ç”¨æˆ·åï¼ˆå¦‚ï¼šadminã€rootã€gaochaoç­‰ï¼‰ä¸å±äºæ•æ„Ÿä¿¡æ¯
- åªæœ‰ç”¨æˆ·å+å¯†ç çš„ç»„åˆæ‰æ˜¯æ•æ„Ÿä¿¡æ¯
- å…¬å¼€çš„åŸŸåï¼ˆå¦‚ï¼šbaidu.comï¼‰ä¸å±äºæ•æ„Ÿä¿¡æ¯
- éœ€è¦é‡ç‚¹å…³æ³¨ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­ä¿¡æ¯æ˜¯å¦çœŸçš„æ•æ„Ÿ

è„±æ•è§„åˆ™ï¼š
- æ‰‹æœºå·ï¼šåªæ˜¾ç¤ºå‰3ä½å’Œå4ä½ï¼ˆå¦‚ï¼š138****5678ï¼‰
- èº«ä»½è¯å·ï¼šåªæ˜¾ç¤ºå‰6ä½å’Œå4ä½ï¼ˆå¦‚ï¼š110101****1234ï¼‰
- é“¶è¡Œå¡å·ï¼šåªæ˜¾ç¤ºå‰4ä½å’Œå4ä½ï¼ˆå¦‚ï¼š6222****4321ï¼‰
- é‚®ç®±ï¼š@å‰é¢éƒ¨åˆ†éšè—ä¸€åŠï¼ˆå¦‚ï¼šte**@163.comï¼‰
- IPåœ°å€ï¼šéšè—ä¸­é—´ä¸¤æ®µï¼ˆå¦‚ï¼š192.***.***234ï¼‰
- å¯†ç /å¯†é’¥ï¼šå…¨éƒ¨æ›¿æ¢ä¸ºæ˜Ÿå·
- å…¶ä»–æ•æ„Ÿä¿¡æ¯ï¼šä¿ç•™é¦–å°¾ï¼Œä¸­é—´ç”¨æ˜Ÿå·æ›¿æ¢

è¾“å‡ºè¦æ±‚ï¼š
ä½ å¿…é¡»ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
    "has_sensitive": true/false,  // æ˜¯å¦åŒ…å«æ•æ„Ÿä¿¡æ¯
    "sensitive_count": 0,         // æ•æ„Ÿä¿¡æ¯æ•°é‡
    "sensitive_items": [          // æ•æ„Ÿä¿¡æ¯åˆ—è¡¨
        {
            "type": "èº«ä»½è¯å·",
            "masked_value": "110101****1234",
            "context": "å‡ºç°çš„ä¸Šä¸‹æ–‡"
        }
    ],
    "summary": "æ–‡æ¡£æ‘˜è¦"         // 50å­—ä»¥å†…çš„æ–‡æ¡£å†…å®¹æ‘˜è¦
}

æ³¨æ„ï¼šç»å¯¹ä¸è¦åœ¨è¾“å‡ºä¸­åŒ…å«æ•æ„Ÿä¿¡æ¯çš„åŸå§‹å€¼ï¼"""


async def read_content_from_file(file_path: str) -> str:
    """ä»æ–‡ä»¶è¯»å–å†…å®¹"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'cp1252', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            async with aiofiles.open(file_path, 'r', encoding=encoding) as f:
                content = await f.read()
                logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç ä»æ–‡ä»¶è¯»å–å†…å®¹: {file_path}")
                return content
        except FileNotFoundError:
            logger.error(f"è§£ææ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return ""
        except UnicodeDecodeError:
            continue
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                logger.error(f"è§£ææ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return ""
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é”™è¯¯å¤„ç†ç­–ç•¥
    try:
        async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = await f.read()
            logger.warning(f"ä½¿ç”¨ utf-8 ç¼–ç ï¼ˆå¿½ç•¥é”™è¯¯ï¼‰è¯»å–æ–‡ä»¶: {file_path}")
            return content
    except FileNotFoundError:
        logger.error(f"è§£ææ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return ""
    except Exception as e:
        logger.error(f"è¯»å–è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return ""


async def get_file_content_from_filesystem(file_id: str) -> Dict[str, Any]:
    """ä»æ–‡ä»¶ç³»ç»Ÿè·å–æ–‡ä»¶å†…å®¹"""
    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        base_path = Path(DOCUMENT_STORAGE_PATH)
        
        # æŸ¥æ‰¾åŸå§‹æ–‡ä»¶å’Œè§£æåçš„æ–‡ä»¶
        original_file_pattern = f"{file_id}.*"
        parsed_file_path = base_path / f"{file_id}.parse.txt"
        
        # è·å–æ–‡ä»¶å…ƒæ•°æ®
        file_metadata = {}
        file_name = f"document_{file_id}"
        file_type = "unknown"
        file_size = 0
        
        # æŸ¥æ‰¾åŸå§‹æ–‡ä»¶è·å–æ–‡ä»¶ä¿¡æ¯
        if not file_name.startswith("document_"):
            for file_path in base_path.glob(original_file_pattern):
                if not str(file_path).endswith('.parse.txt'):
                    file_name = file_path.name
                    file_type = file_path.suffix[1:] if file_path.suffix else 'unknown'
                    file_size = file_path.stat().st_size
                    break
        
        # æ£€æŸ¥è§£æåçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not parsed_file_path.exists():
            # å°è¯•ç›´æ¥è¯»å–åŸå§‹æ–‡æœ¬æ–‡ä»¶
            for file_path in base_path.glob(original_file_pattern):
                if file_path.suffix in ['.txt', '.md']:
                    content = await read_content_from_file(str(file_path))
                    if content:
                        break
            else:
                logger.error(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {file_id}")
                return {
                    'success': False,
                    'content': '',
                    'error': f'æ‰¾ä¸åˆ°æ–‡ä»¶: {file_id}'
                }
        else:
            # è¯»å–è§£æåçš„å†…å®¹
            content = await read_content_from_file(str(parsed_file_path))
            if not content:
                return {
                    'success': False,
                    'content': '',
                    'error': f'æ— æ³•è¯»å–è§£ææ–‡ä»¶: {parsed_file_path}'
                }
        
        # ç»Ÿè®¡å›¾ç‰‡æ•°é‡ï¼ˆåŒ¹é… [å›¾ç‰‡ æ•°å­—] æ ¼å¼ï¼‰
        image_count = len(re.findall(r'\[å›¾ç‰‡\s*\d*\]', content))
        
        # è·å–å­—ç¬¦æ•°
        char_count = file_metadata.get('char_count', len(content))
        
        return {
            'success': True,
            'content': content,
            'file_name': file_name,
            'file_type': file_type,
            'file_size': file_size,
            'doc_metadata': file_metadata,
            'image_count': image_count,
            'char_count': char_count
        }
            
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
        return {
            'success': False,
            'content': '',
            'error': f'ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}'
        }


async def scan_content_with_llm(content: str, file_name: str = "æœªçŸ¥æ–‡ä»¶") -> Dict[str, Any]:
    """ä½¿ç”¨LLMæ‰«æå†…å®¹ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼ˆåŸæœ‰LangChainå®ç°ï¼‰"""
    try:
        max_chunk_size = CHUNK_SIZE
        
        # å¦‚æœå†…å®¹è¾ƒçŸ­ï¼Œç›´æ¥æ‰«æ
        if len(content) <= max_chunk_size:
            messages = [
                SystemMessage(content=SYSTEM_PROMPT),
                HumanMessage(content=f"è¯·æ‰«æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼š\n\næ–‡ä»¶åï¼š{file_name}\n\nå†…å®¹ï¼š\n{content}")
            ]
            
            response = await llm.ainvoke(messages)
            result = json_parser.parse(response.content)
            
            return {
                'success': True,
                'result': result
            }
        
        # å¯¹å¤§æ–‡ä»¶è¿›è¡Œåˆ†ç‰‡å¤„ç†
        chunks = []
        for i in range(0, len(content), max_chunk_size):
            chunks.append(content[i:i + max_chunk_size])
        
        logger.info(f"å¤§æ–‡ä»¶ {file_name} è¢«åˆ†æˆ {len(chunks)} ä¸ªåˆ†ç‰‡ï¼ˆæ¯ç‰‡æœ€å¤§ {max_chunk_size} å­—ç¬¦ï¼‰")
        
        # ä¸²è¡Œæ‰«ææ‰€æœ‰åˆ†ç‰‡ï¼Œä¿æŒæ–‡æœ¬é¡ºåº
        responses = []
        for idx, chunk in enumerate(chunks):
            messages = [
                SystemMessage(content=SYSTEM_PROMPT),
                HumanMessage(content=f"è¯·æ‰«æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼š\n\næ–‡ä»¶åï¼š{file_name}ï¼ˆç¬¬{idx+1}/{len(chunks)}éƒ¨åˆ†ï¼‰\n\nå†…å®¹ï¼š\n{chunk}")
            ]
            response = await llm.ainvoke(messages)
            responses.append(response)
            logger.debug(f"å®Œæˆæ‰«æ {file_name} çš„ç¬¬ {idx+1}/{len(chunks)} éƒ¨åˆ†")
        
        # åˆå¹¶åˆ†ç‰‡ç»“æœ
        merged_result = {
            'has_sensitive': False,
            'sensitive_count': 0,
            'sensitive_items': [],
            'summary': f'å¤§æ–‡ä»¶åˆ†{len(chunks)}éƒ¨åˆ†æ‰«æ'
        }
        
        all_summaries = []
        for idx, response in enumerate(responses):
            try:
                chunk_result = json_parser.parse(response.content)
                
                if chunk_result.get('has_sensitive'):
                    merged_result['has_sensitive'] = True
                    merged_result['sensitive_count'] += chunk_result.get('sensitive_count', 0)
                    
                    # æ·»åŠ æ•æ„Ÿé¡¹ï¼Œæ ‡æ³¨æ¥æºåˆ†ç‰‡
                    for item in chunk_result.get('sensitive_items', []):
                        item['chunk'] = idx + 1
                        merged_result['sensitive_items'].append(item)
                
                if chunk_result.get('summary'):
                    all_summaries.append(f"ç¬¬{idx+1}éƒ¨åˆ†: {chunk_result['summary']}")
                    
            except Exception as e:
                logger.error(f"è§£æç¬¬{idx+1}éƒ¨åˆ†ç»“æœå¤±è´¥: {e}")
        
        # é™åˆ¶æ•æ„Ÿä¿¡æ¯é¡¹æ•°é‡
        if len(merged_result['sensitive_items']) > 10:
            merged_result['sensitive_items'] = merged_result['sensitive_items'][:10]
            merged_result['summary'] += f"ï¼ˆä»…æ˜¾ç¤ºå‰10ä¸ªæ•æ„Ÿä¿¡æ¯ï¼‰"
        
        # åˆå¹¶æ‘˜è¦
        if all_summaries:
            merged_result['summary'] = " | ".join(all_summaries[:3])
            if len(all_summaries) > 3:
                merged_result['summary'] += f" ç­‰{len(all_summaries)}éƒ¨åˆ†"
        
        return {
            'success': True,
            'result': merged_result
        }
        
    except Exception as e:
        logger.error(f"LLMæ‰«æå¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': f'æ‰«æå¤±è´¥: {str(e)}'
        }


async def scan_content_with_langextract(content: str, file_name: str = "æœªçŸ¥æ–‡ä»¶") -> Dict[str, Any]:
    """ä½¿ç”¨LangExtractæ‰«æå†…å®¹ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
    try:
        # ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼ˆå› ä¸ºlangextractç›®å‰æ˜¯åŒæ­¥çš„ï¼‰
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None, 
            langextract_scanner.scan_text,
            content,
            file_name
        )
        
        if result["success"]:
            # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼
            llm_format_result = {
                'has_sensitive': result['has_sensitive'],
                'sensitive_count': result['sensitive_count'],
                'sensitive_items': [
                    {
                        'type': item['type'],
                        'masked_value': item['masked_value'],
                        'context': item['context']
                    }
                    for item in result.get('sensitive_items', [])
                ],
                'summary': f"LangExtractæ‰«æï¼Œå‘ç°{result['sensitive_count']}ä¸ªæ•æ„Ÿä¿¡æ¯"
            }
            
            return {
                'success': True,
                'result': llm_format_result,
                'langextract_result': result  # ä¿ç•™åŸå§‹ç»“æœç”¨äºå¯è§†åŒ–
            }
        else:
            return {
                'success': False,
                'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
            }
            
    except Exception as e:
        logger.error(f"LangExtractæ‰«æå¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': f'æ‰«æå¤±è´¥: {str(e)}'
        }


async def scan_single_file(file_id: str) -> Dict[str, Any]:
    """æ‰«æå•ä¸ªæ–‡ä»¶å¹¶è¿”å›ç»“æœå­—å…¸"""
    file_data = await get_file_content_from_filesystem(file_id)
    
    if not file_data['success']:
        return {
            'file_id': file_id,
            'success': False,
            'error': file_data['error']
        }
    
    # æ ¹æ®é…ç½®é€‰æ‹©æ‰«ææ–¹æ³•
    if USE_LANGEXTRACT and langextract_scanner:
        scan_result = await scan_content_with_langextract(
            file_data['content'], 
            file_data['file_name']
        )
    else:
        scan_result = await scan_content_with_llm(
            file_data['content'], 
            file_data['file_name']
        )
    
    if not scan_result['success']:
        return {
            'file_id': file_id,
            'file_name': file_data['file_name'],
            'success': False,
            'error': scan_result.get('error', 'æœªçŸ¥é”™è¯¯')
        }
    
    return {
        'file_id': file_id,
        'file_name': file_data['file_name'],
        'file_type': file_data['file_type'],
        'file_size': file_data['file_size'],
        'image_count': file_data.get('image_count', 0),
        'char_count': file_data.get('char_count', 0),
        'success': True,
        'result': scan_result['result'],
        'langextract_result': scan_result.get('langextract_result')  # å¦‚æœä½¿ç”¨LangExtract
    }


@mcp.tool()
async def scan_document_v2(file_ids: List[str], enable_visualization: bool = True) -> str:
    """
    æ‰«ææ–‡æ¡£ä¸­çš„æ•æ„Ÿä¿¡æ¯
    
    Args:
        file_ids: æ–‡ä»¶IDåˆ—è¡¨ï¼ˆæ–‡ä»¶ç³»ç»Ÿä¸­çš„file_idåˆ—è¡¨ï¼‰
        enable_visualization: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šï¼ˆä»…åœ¨ä½¿ç”¨LangExtractæ—¶æœ‰æ•ˆï¼Œé»˜è®¤ä¸ºTrueï¼‰
    
    Returns:
        æ‰«æç»“æœæŠ¥å‘Š
    """
    try:
        logger.info(f"å¼€å§‹æ‰«æ {len(file_ids)} ä¸ªæ–‡ä»¶ï¼Œå¯è§†åŒ–: {enable_visualization}")
        
        if not file_ids:
            return "é”™è¯¯: æœªæä¾›æ–‡ä»¶ID"
        
        # ç»Ÿä¸€ä½¿ç”¨æ‰¹é‡å¤„ç†é€»è¾‘ï¼ˆæ— è®ºæ˜¯1ä¸ªè¿˜æ˜¯å¤šä¸ªæ–‡ä»¶ï¼‰
        output = ""
        
        # è·å–å¼•æ“ä¿¡æ¯
        engine_info = get_scan_engine_info()
        
        # æ˜¾ç¤ºæ‰«ææŠ¥å‘Šå¤´éƒ¨
        output = f"æ‰«ææŠ¥å‘Š\n"
        output += f"{'='*50}\n"
        output += f"æ‰«ææ–‡ä»¶æ•°: {len(file_ids)}\n"
        output += f"æ‰«ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        output += f"æ‰«æå¼•æ“: {engine_info['engine']}"
        if engine_info['langextract_enabled']:
            output += f" ({engine_info['langextract_provider']})"
        output += "\n"
        output += f"ä½¿ç”¨æ¨¡å‹: {engine_info.get('langextract_model') or engine_info.get('langchain_model')}\n"
        output += f"{'='*50}\n\n"
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘åº¦
        semaphore = Semaphore(FILE_CONCURRENCY)
        
        async def scan_with_semaphore(file_id: str):
            async with semaphore:
                return await scan_single_file(file_id)
        
        # å¹¶å‘æ‰«ææ‰€æœ‰æ–‡ä»¶
        scan_tasks = [scan_with_semaphore(file_id) for file_id in file_ids]
        scan_results = await asyncio.gather(*scan_tasks, return_exceptions=True)
        
        # å¤„ç†æ‰«æç»“æœ
        total_sensitive_count = 0
        files_with_sensitive = 0
        langextract_results = []  # ç”¨äºå¯è§†åŒ–
        
        for idx, scan_data in enumerate(scan_results, 1):
            # å¤„ç†å¼‚å¸¸æƒ…å†µ
            if isinstance(scan_data, Exception):
                output += f"\nå†…å®¹æº{idx}: {file_ids[idx-1]}\n"
                output += f"æ‰«æå¤±è´¥: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ - {str(scan_data)}\n"
                output += "="*50 + "\n"
                continue
                
            if not scan_data.get('success', False):
                output += f"\nå†…å®¹æº{idx}: {scan_data.get('file_id', file_ids[idx-1])}\n"
                output += f"æ‰«æå¤±è´¥: {scan_data.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
                output += "="*50 + "\n"
                continue
            
            result = scan_data.get('result', {})
            
            # æ”¶é›†LangExtractç»“æœç”¨äºå¯è§†åŒ–
            if scan_data.get('langextract_result'):
                langextract_results.append(scan_data['langextract_result'])
            
            # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
            file_size_kb = scan_data.get('file_size', 0) / 1024
            
            # åˆ¤æ–­è§£æçŠ¶æ€
            parse_status = "å†…å®¹å®Œæ•´"
            if result.get('summary', '').find('è§£æå¤±è´¥') >= 0:
                parse_status = "å†…å®¹è§£æå¼‚å¸¸"
            
            # è¾“å‡ºæ–‡ä»¶ç»“æœ
            output += f"\nå†…å®¹æº{idx}: {scan_data.get('file_name', 'æœªçŸ¥æ–‡ä»¶')}\n"
            output += f"1.æ–‡æ¡£ä¿¡æ¯ï¼š{file_size_kb:.1f}KBã€æ–‡å­—{scan_data.get('char_count', 0)}"
            if scan_data.get('image_count', 0) > 0:
                output += f"(åŒ…å«å›¾ç‰‡{scan_data['image_count']}å¼ çš„è§£æå†…å®¹)"
            output += "\n"
            output += f"2.æ–‡æ¡£è§£æçŠ¶æ€ï¼š{parse_status}\n"
            output += f"3.æ–‡æ¡£æ‘˜è¦ï¼š{result.get('summary', 'æ— æ‘˜è¦')[:100]}\n"
            output += f"4.æ•æ„Ÿä¿¡æ¯æ‰«æç»“æœï¼š"
            
            if result.get('has_sensitive', False):
                files_with_sensitive += 1
                total_sensitive_count += result.get('sensitive_count', 0)
                output += f"å‘ç°{result.get('sensitive_count', 0)}ä¸ªæ•æ„Ÿä¿¡æ¯\n"
                # æœ€å¤šå±•ç¤º3ä¸ª
                sensitive_items = result.get('sensitive_items', [])
                for i, item in enumerate(sensitive_items[:3], 1):
                    output += f"  {i}) {item.get('type', 'æœªçŸ¥ç±»å‹')}: {item.get('masked_value', '***')}\n"
                if len(sensitive_items) > 3:
                    output += f"  ...è¿˜æœ‰{len(sensitive_items)-3}ä¸ªæ•æ„Ÿä¿¡æ¯æœªå±•ç¤º\n"
            else:
                output += "æœªå‘ç°æ•æ„Ÿä¿¡æ¯\n"
            
            output += "="*50 + "\n"
        
        # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡
        output += f"\n{'='*50}\n"
        output += f"æ‰«ææ±‡æ€»:\n"
        output += f"   - æ‰«ææ–‡ä»¶æ€»æ•°: {len(file_ids)}\n"
        output += f"   - åŒ…å«æ•æ„Ÿä¿¡æ¯çš„æ–‡ä»¶: {files_with_sensitive}\n"
        output += f"   - æ•æ„Ÿä¿¡æ¯æ€»æ•°: {total_sensitive_count}\n"
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = {
            "scan_time": datetime.now().isoformat(),
            "total_files": len(file_ids),
            "total_sensitive": total_sensitive_count,
            "files_with_sensitive": files_with_sensitive,
            "engine": engine_info['engine'],
            "model": engine_info.get('langextract_model') or engine_info.get('langchain_model'),
            "items": [],
            "statistics": {}
        }
        
        # æ”¶é›†æ‰€æœ‰æ•æ„Ÿä¿¡æ¯é¡¹
        for scan_data in scan_results:
            if isinstance(scan_data, dict) and scan_data.get('success'):
                result = scan_data.get('result', {})
                file_name = scan_data.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                
                for item in result.get('sensitive_items', []):
                    report_item = {
                        "type": item.get('type', 'æœªçŸ¥ç±»å‹'),
                        "masked_value": item.get('masked_value', '***'),
                        "context": item.get('context', ''),
                        "file": file_name
                    }
                    report_data["items"].append(report_item)
                    
                    # ç»Ÿè®¡
                    item_type = item.get('type', 'æœªçŸ¥ç±»å‹')
                    if item_type not in report_data["statistics"]:
                        report_data["statistics"][item_type] = 0
                    report_data["statistics"][item_type] += 1
        
        # ä¿å­˜æŠ¥å‘Šæ•°æ®ä¸ºJSON
        if total_sensitive_count > 0:
            try:
                os.makedirs(VISUALIZATION_OUTPUT_DIR, exist_ok=True)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                report_filename = f"scan_report_{timestamp}.json"
                report_path = os.path.join(VISUALIZATION_OUTPUT_DIR, report_filename)
                
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(report_data, f, ensure_ascii=False, indent=2)
                
                # ç”ŸæˆæŠ¥å‘Šæ ‡è¯†ç¬¦
                report_id = f"[REPORT:SENSITIVE_SCAN:{report_filename}:æŸ¥çœ‹å®Œæ•´æ‰«ææŠ¥å‘Š]"
                output += f"\n{'='*50}\n"
                output += f"ğŸ“Š æ‰«ææŠ¥å‘Š:\n"
                output += f"   {report_id}\n"
                logger.info(f"æ‰«ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
                
                # å¦‚æœä½¿ç”¨LangExtractï¼Œä»ç„¶å¯ä»¥ç”ŸæˆHTMLå¯è§†åŒ–
                if enable_visualization and USE_LANGEXTRACT and langextract_results:
                    try:
                        viz_filename = f"scan_viz_{timestamp}.html"
                        viz_path = os.path.join(VISUALIZATION_OUTPUT_DIR, viz_filename)
                        
                        loop = asyncio.get_event_loop()
                        html_path = await loop.run_in_executor(
                            None,
                            langextract_scanner.generate_visualization,
                            langextract_results,
                            viz_path
                        )
                        
                        if html_path:
                            logger.info(f"LangExtractå¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {html_path}")
                    except Exception as e:
                        logger.error(f"ç”ŸæˆLangExtractå¯è§†åŒ–æŠ¥å‘Šå¤±è´¥: {str(e)}")
            except Exception as e:
                logger.error(f"ä¿å­˜æŠ¥å‘Šæ•°æ®å¤±è´¥: {str(e)}")
                output += f"\nä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}\n"
        
        return output.rstrip()  # å»æ‰æœ«å°¾æ¢è¡Œ
        
    except Exception as e:
        logger.error(f"æ‰«ææ–‡æ¡£è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}", exc_info=True)
        return f"é”™è¯¯: æ‰«æè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ - {str(e)}"


def get_scan_engine_info() -> Dict[str, Any]:
    """
    è·å–å½“å‰æ‰«æå¼•æ“ä¿¡æ¯ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
    
    Returns:
        æ‰«æå¼•æ“é…ç½®ä¿¡æ¯å­—å…¸
    """
    return {
        "engine": "LangExtract" if USE_LANGEXTRACT else "LangChain",
        "langextract_enabled": USE_LANGEXTRACT,
        "langextract_provider": LANGEXTRACT_PROVIDER if USE_LANGEXTRACT else None,
        "langextract_model": LANGEXTRACT_MODEL if USE_LANGEXTRACT else None,
        "langextract_base_url": LANGEXTRACT_BASE_URL if USE_LANGEXTRACT and LANGEXTRACT_PROVIDER == 'custom' else None,
        "langchain_model": LLM_MODEL if not USE_LANGEXTRACT else None,
        "chunk_size": CHUNK_SIZE,
        "file_concurrency": FILE_CONCURRENCY,
        "visualization_enabled": USE_LANGEXTRACT,
        "visualization_output_dir": VISUALIZATION_OUTPUT_DIR if USE_LANGEXTRACT else None
    }


if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡å™¨
    port = config.get('port', 3008)
    logger.info(f"Starting Sensitive Data Scanner MCP Server V2 on port {port}")
    
    # æ˜¾ç¤ºå¼•æ“é…ç½®ä¿¡æ¯
    engine_info = get_scan_engine_info()
    logger.info(f"æ‰«æå¼•æ“: {engine_info['engine']}")
    logger.info(f"æ¨¡å‹: {engine_info.get('langextract_model') or engine_info.get('langchain_model')}")
    if engine_info['langextract_enabled']:
        logger.info(f"æä¾›å•†: {engine_info['langextract_provider']}")
        logger.info(f"å¯è§†åŒ–: å·²å¯ç”¨ (è¾“å‡ºç›®å½•: {engine_info['visualization_output_dir']})")
    logger.info(f"åˆ†å—å¤§å°: {engine_info['chunk_size']} å­—ç¬¦")
    logger.info(f"æ–‡ä»¶å¹¶å‘åº¦: {engine_info['file_concurrency']}")
    
    mcp.run(transport="streamable-http", host="0.0.0.0", port=port)