# MCP 敏感数据扫描服务配置示例
# 展示如何配置不同的模型提供商

# 示例1：使用 Google Gemini
sensitive_scan_server_gemini:
  enabled: true
  port: 3008
  script: "servers/sensitive_scan_mcp_server_v2.py"
  display_name: "敏感数据扫描服务V2(Gemini)"
  config:
    document_storage_path: "/tmp/documents/uploads"
    use_langextract: true
    langextract_provider: "gemini"  # 使用 Google Gemini
    langextract_model: "gemini-2.0-flash-exp"  # 或 gemini-1.5-pro
    langextract_api_key: ""  # 留空则从环境变量 GOOGLE_API_KEY 读取
    visualization_output_dir: "/tmp/scan_visualizations"

# 示例2：使用 OpenAI
sensitive_scan_server_openai:
  enabled: false
  port: 3008
  script: "servers/sensitive_scan_mcp_server_v2.py"
  display_name: "敏感数据扫描服务V2(OpenAI)"
  config:
    document_storage_path: "/tmp/documents/uploads"
    use_langextract: true
    langextract_provider: "openai"  # 使用 OpenAI
    langextract_model: "gpt-4o-mini"  # 或 gpt-4o, gpt-3.5-turbo
    langextract_api_key: ""  # 留空则从环境变量 OPENAI_API_KEY 读取
    visualization_output_dir: "/tmp/scan_visualizations"

# 示例3：使用 SiliconFlow (OpenAI 兼容接口)
sensitive_scan_server_siliconflow:
  enabled: false
  port: 3008
  script: "servers/sensitive_scan_mcp_server_v2.py"
  display_name: "敏感数据扫描服务V2(SiliconFlow)"
  config:
    document_storage_path: "/tmp/documents/uploads"
    use_langextract: true
    langextract_provider: "custom"  # 使用自定义 OpenAI 兼容服务
    langextract_model: "Qwen/QwQ-32B"  # SiliconFlow 上的模型
    langextract_api_key: "sk-your-siliconflow-api-key"
    langextract_base_url: "https://api.siliconflow.cn/v1"  # SiliconFlow API 地址
    visualization_output_dir: "/tmp/scan_visualizations"

# 示例4：使用通义千问 (通过 DashScope OpenAI 兼容接口)
sensitive_scan_server_dashscope:
  enabled: false
  port: 3008
  script: "servers/sensitive_scan_mcp_server_v2.py"
  display_name: "敏感数据扫描服务V2(通义千问)"
  config:
    document_storage_path: "/tmp/documents/uploads"
    use_langextract: true
    langextract_provider: "custom"
    langextract_model: "qwen-plus"
    langextract_api_key: "sk-your-dashscope-api-key"
    langextract_base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    visualization_output_dir: "/tmp/scan_visualizations"

# 示例5：使用 DeepSeek (OpenAI 兼容接口)
sensitive_scan_server_deepseek:
  enabled: false
  port: 3008
  script: "servers/sensitive_scan_mcp_server_v2.py"
  display_name: "敏感数据扫描服务V2(DeepSeek)"
  config:
    document_storage_path: "/tmp/documents/uploads"
    use_langextract: true
    langextract_provider: "custom"
    langextract_model: "deepseek-chat"
    langextract_api_key: "sk-your-deepseek-api-key"
    langextract_base_url: "https://api.deepseek.com/v1"
    visualization_output_dir: "/tmp/scan_visualizations"

# 示例6：使用 Ollama 本地模型
sensitive_scan_server_ollama:
  enabled: false
  port: 3008
  script: "servers/sensitive_scan_mcp_server_v2.py"
  display_name: "敏感数据扫描服务V2(Ollama本地)"
  config:
    document_storage_path: "/tmp/documents/uploads"
    use_langextract: true
    langextract_provider: "ollama"  # 使用 Ollama
    langextract_model: "llama2:7b"  # 本地模型名称
    visualization_output_dir: "/tmp/scan_visualizations"
    # Ollama 不需要 API key，默认连接 localhost:11434

# 示例7：使用原 LangChain 实现（不使用 LangExtract）
sensitive_scan_server_langchain:
  enabled: false
  port: 3008
  script: "servers/sensitive_scan_mcp_server_v2.py"
  display_name: "敏感数据扫描服务V2(LangChain)"
  config:
    document_storage_path: "/tmp/documents/uploads"
    use_langextract: false  # 不使用 LangExtract
    # LangChain 配置
    llm_api_base: "https://api.siliconflow.cn/v1"
    llm_api_key: "sk-your-api-key"
    llm_model: "Qwen/Qwen3-70B-Instruct"
    chunk_size: 10000
    file_concurrency: 3