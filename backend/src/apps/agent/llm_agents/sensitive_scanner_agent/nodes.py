"""æ•æ„Ÿæ•°æ®æ‰«ææ™ºèƒ½ä½“èŠ‚ç‚¹å®ç°"""

import json
import uuid
from typing import Dict, Any, List
from datetime import datetime
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig

from .state import ScannerState, ChunkState
from .tools import get_file_content, generate_scan_report
from .configuration import INIT_AGENT_CONFIG, AGENT_DETAIL_CONFIG
from src.shared.core.logging import get_logger

logger = get_logger(__name__)


async def initialize_scan(state: ScannerState, config: RunnableConfig) -> Dict[str, Any]:
    """åˆå§‹åŒ–æ‰«æä»»åŠ¡ï¼Œä»å·¥å…·æ¶ˆæ¯ä¸­æå–æ–‡ä»¶å†…å®¹"""
    import json
    from langchain_core.messages import ToolMessage, AIMessage
    
    # è·å–file_ids
    file_ids = state.get("file_ids", [])
    
    # ä»æ¶ˆæ¯ä¸­æå–æ–‡ä»¶å†…å®¹
    file_contents = {}
    messages = state.get("messages", [])
    
    # éå†æ¶ˆæ¯æŸ¥æ‰¾å·¥å…·è¿”å›çš„æ–‡ä»¶å†…å®¹
    for msg in messages:
        if isinstance(msg, ToolMessage):
            try:
                tool_result = json.loads(msg.content) if isinstance(msg.content, str) else msg.content
                if isinstance(tool_result, dict) and tool_result.get("success") and "file_id" in tool_result:
                    file_id = tool_result["file_id"]
                    file_contents[file_id] = {
                        "content": tool_result.get("content", ""),
                        "file_name": tool_result.get("file_name", ""),
                        "file_size": tool_result.get("file_size", 0)
                    }
                    logger.info(f"ä»å·¥å…·æ¶ˆæ¯ä¸­æå–æ–‡ä»¶ {file_id} çš„å†…å®¹")
            except Exception as e:
                logger.warning(f"è§£æå·¥å…·æ¶ˆæ¯å¤±è´¥: {e}")
    
    # ç”Ÿæˆæ‰«æå¼€å§‹æ¶ˆæ¯
    if file_contents:
        scan_start_msg = f"ğŸ” å¼€å§‹æ‰«æ {len(file_contents)} ä¸ªæ–‡ä»¶...\n\n"
        for idx, (file_id, file_info) in enumerate(file_contents.items()):
            scan_start_msg += f"âœ… æ–‡ä»¶ {idx + 1}: {file_info['file_name']} (ID: {file_id})\n"
        
        messages.append(AIMessage(content=scan_start_msg))
        logger.info(f"æˆåŠŸè·å– {len(file_contents)} ä¸ªæ–‡ä»¶çš„å†…å®¹ï¼Œå‡†å¤‡å¼€å§‹æ‰«æ")
    else:
        logger.warning("æœªèƒ½ä»æ¶ˆæ¯ä¸­æå–ä»»ä½•æ–‡ä»¶å†…å®¹")
        error_msg = "âš ï¸ æœªèƒ½è·å–ä»»ä½•æ–‡ä»¶å†…å®¹ï¼Œæ‰«æç»ˆæ­¢ã€‚"
        messages.append(AIMessage(content=error_msg))
        
        return {
            **state,
            "messages": messages,
            "errors": state.get("errors", []) + ["æœªèƒ½è·å–ä»»ä½•æ–‡ä»¶å†…å®¹"]
        }
    
    # æ›´æ–°çŠ¶æ€
    return {
        **state,
        "file_ids": list(file_contents.keys()),
        "file_contents": file_contents,
        "messages": messages,
        "errors": state.get("errors", []),
        "chunk_size": state.get("chunk_size", AGENT_DETAIL_CONFIG.get("chunk_size", 200)),
        "max_parallel_chunks": state.get("max_parallel_chunks", AGENT_DETAIL_CONFIG.get("max_parallel_chunks", 5)),
        "sensitive_types": state.get("sensitive_types", ["èº«ä»½è¯", "æ‰‹æœºå·", "é“¶è¡Œå¡", "é‚®ç®±", "å¯†ç ", "APIå¯†é’¥"])
    }


async def create_chunks(state: ScannerState, config: RunnableConfig) -> Dict[str, Any]:
    """å°†æ–‡æ¡£å†…å®¹åˆ†ç‰‡ï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç†"""
    from langchain_core.messages import AIMessage
    
    file_contents = state.get("file_contents", {})
    chunk_size = state.get("chunk_size", 200)  # æ”¹ä¸º200å­—ç¬¦æµ‹è¯•
    
    chunks = []
    chunk_info_msg = "ğŸ“„ å¼€å§‹å¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç‰‡å¤„ç†...\n\n"
    
    for idx, (file_id, file_info) in enumerate(file_contents.items()):
        content = file_info.get("content", "")
        file_name = file_info.get("file_name", "")
        
        chunk_info_msg += f"ğŸ“ æ–‡ä»¶ {idx + 1}: {file_name}\n"
        chunk_info_msg += f"   - æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦\n"
        
        # å¦‚æœå†…å®¹è¾ƒå°ï¼Œä¸éœ€è¦åˆ†ç‰‡
        if len(content) <= chunk_size:
            chunks.append({
                "chunk_id": f"{file_id}_chunk_0",
                "file_id": file_id,
                "file_name": file_name,
                "content": content,
                "chunk_index": 0,
                "total_chunks": 1
            })
        else:
            # æŒ‰è¡Œåˆ†å‰²ä»¥é¿å…åœ¨å•è¯ä¸­é—´åˆ‡æ–­
            lines = content.split('\n')
            current_chunk = []
            current_size = 0
            chunk_index = 0
            
            for line in lines:
                line_size = len(line) + 1  # +1 for newline
                
                if current_size + line_size > chunk_size and current_chunk:
                    # åˆ›å»ºæ–°åˆ†ç‰‡
                    chunk_content = '\n'.join(current_chunk)
                    chunks.append({
                        "chunk_id": f"{file_id}_chunk_{chunk_index}",
                        "file_id": file_id,
                        "file_name": file_name,
                        "content": chunk_content,
                        "chunk_index": chunk_index,
                        "total_chunks": -1  # æš‚æ—¶æœªçŸ¥æ€»æ•°
                    })
                    chunk_index += 1
                    current_chunk = [line]
                    current_size = line_size
                else:
                    current_chunk.append(line)
                    current_size += line_size
            
            # å¤„ç†æœ€åä¸€ä¸ªåˆ†ç‰‡
            if current_chunk:
                chunk_content = '\n'.join(current_chunk)
                chunks.append({
                    "chunk_id": f"{file_id}_chunk_{chunk_index}",
                    "file_id": file_id,
                    "file_name": file_name,
                    "content": chunk_content,
                    "chunk_index": chunk_index,
                    "total_chunks": chunk_index + 1
                })
                
            # æ›´æ–°æ€»åˆ†ç‰‡æ•°
            total_chunks = chunk_index + 1
            for chunk in chunks:
                if chunk["file_id"] == file_id:
                    chunk["total_chunks"] = total_chunks
                    
            chunk_info_msg += f"   - åˆ†ç‰‡æ•°é‡: {total_chunks} ä¸ª\n"
        
        chunk_info_msg += "\n"
    
    chunk_info_msg += f"ğŸ“Š æ€»è®¡åˆ›å»ºäº† {len(chunks)} ä¸ªåˆ†ç‰‡ï¼Œå‡†å¤‡è¿›è¡Œå¹¶è¡Œæ‰«æ...\n"
    logger.info(f"åˆ›å»ºäº† {len(chunks)} ä¸ªåˆ†ç‰‡")
    
    # æ·»åŠ åˆ†ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
    messages = state.get("messages", [])
    messages.append(AIMessage(content=chunk_info_msg))
    
    return {
        **state,
        "chunks": chunks,
        "messages": messages
    }


async def scan_chunk_with_llm(chunk: Dict[str, Any], llm, config: RunnableConfig) -> Dict[str, Any]:
    """ä½¿ç”¨LLMæ‰«æå•ä¸ªåˆ†ç‰‡"""
    try:
        from langchain_core.messages import HumanMessage
        import json
        
        # ç›´æ¥ç”Ÿæˆåˆ†ææç¤ºè¯ï¼ˆä¸ä½¿ç”¨å·¥å…·ï¼‰
        text = chunk["content"]
        chunk_index = chunk["chunk_index"]
        
        prompt = f"""ä½œä¸ºä¸“ä¸šçš„æ•æ„Ÿæ•°æ®æ‰«æä¸“å®¶ï¼Œè¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µï¼ˆè¿™æ˜¯ç¬¬{chunk_index + 1}ä¸ªç‰‡æ®µï¼‰ï¼Œè¯†åˆ«å…¶ä¸­æ‰€æœ‰çš„æ•æ„Ÿä¿¡æ¯ã€‚

===== å¾…åˆ†ææ–‡æœ¬å¼€å§‹ =====
{text}
===== å¾…åˆ†ææ–‡æœ¬ç»“æŸ =====

è¯·ç³»ç»Ÿæ€§åœ°è¯†åˆ«ä»¥ä¸‹ç±»å‹çš„æ•æ„Ÿä¿¡æ¯ï¼š

ã€ä¸ªäººèº«ä»½ä¿¡æ¯ã€‘
- èº«ä»½è¯å·ï¼š18ä½æ•°å­—ï¼Œæ ¼å¼å¦‚ 110101199001011234
- æ‰‹æœºå·ç ï¼š11ä½ï¼Œå¦‚ 13812345678ã€15912345678
- é“¶è¡Œå¡å·ï¼š16-19ä½è¿ç»­æ•°å­—
- ç¤¾ä¿å¡å·ã€æŠ¤ç…§å·ã€é©¾é©¶è¯å·ç­‰

ã€è´¦æˆ·å‡­è¯ä¿¡æ¯ã€‘
- é‚®ç®±åœ°å€ï¼šå¦‚ user@example.com
- ç”¨æˆ·åå¯†ç ï¼špassword=xxxã€pwd:xxxã€å¯†ç ï¼šxxx
- ç™»å½•å‡­è¯ã€ä¼šè¯IDç­‰

ã€æŠ€æœ¯æ•æ„Ÿä¿¡æ¯ã€‘
- APIå¯†é’¥ï¼šapi_key=xxxã€apikey:xxxã€access_keyã€secret_key
- è®¿é—®ä»¤ç‰Œï¼štoken=xxxã€bearer xxxã€auth_token
- æ•°æ®åº“è¿æ¥ï¼šmysql://user:pass@hostã€mongodb://xxx
- ç§é’¥è¯ä¹¦ï¼šBEGIN PRIVATE KEYã€BEGIN RSA PRIVATE KEY

ã€ç½‘ç»œä¿¡æ¯ã€‘
- IPåœ°å€ï¼šå¦‚ 192.168.1.1ã€10.0.0.1
- å†…ç½‘åœ°å€ã€æœåŠ¡å™¨åœ°å€
- URLä¸­åŒ…å«çš„æ•æ„Ÿå‚æ•°

ã€å…¶ä»–æ•æ„Ÿæ•°æ®ã€‘
- ä»»ä½•çœ‹èµ·æ¥åƒå¯†é’¥ã€å¯†ç ã€è¯ä¹¦çš„å­—ç¬¦ä¸²
- Base64ç¼–ç çš„å¯èƒ½æ•æ„Ÿå†…å®¹
- å…¶ä»–ä½ è®¤ä¸ºæ•æ„Ÿçš„ä¿¡æ¯

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›å¤ï¼Œæ¯ä¸€é¡¹éƒ½å¿…é¡»å•ç‹¬ä¸€è¡Œï¼Œä¸è¦é—æ¼ä»»ä½•æ ‡è®°ï¼š

[æ‰«æç»“æœå¼€å§‹]
å‘ç°æ•æ„Ÿæ•°æ®: æ˜¯/å¦

[æ•æ„Ÿæ•°æ®è¯¦æƒ…]
èº«ä»½è¯å·: Xä¸ªï¼Œä½äºç¬¬Yè¡Œã€ç¬¬Zè¡Œ
æ‰‹æœºå·: Xä¸ªï¼Œä½äºç¬¬Yè¡Œ
é“¶è¡Œå¡å·: Xä¸ªï¼Œä½äºç¬¬Yè¡Œ
é‚®ç®±åœ°å€: Xä¸ªï¼Œä½äºç¬¬Yè¡Œ
APIå¯†é’¥: Xä¸ªï¼Œä½äºç¬¬Yè¡Œ
æ•°æ®åº“è¿æ¥: Xä¸ªï¼Œä½äºç¬¬Yè¡Œ
IPåœ°å€: Xä¸ªï¼Œä½äºç¬¬Yè¡Œ
ï¼ˆæ ¹æ®å®é™…å‘ç°çš„ç±»å‹åˆ—å‡ºï¼Œå¦‚æœæŸç±»å‹æœªå‘ç°åˆ™å†™"0ä¸ª"ï¼‰

[ç»Ÿè®¡ä¿¡æ¯]
æ•æ„Ÿæ•°æ®æ€»æ•°: Xä¸ª
é£é™©ç­‰çº§: é«˜/ä¸­/ä½/æ— 

[æ‰«ææ‘˜è¦]
æœ¬ç‰‡æ®µå‘ç°Xä¸ªèº«ä»½è¯å·ã€Yä¸ªæ‰‹æœºå·...ï¼ˆç®€è¦æ€»ç»“ï¼‰
[æ‰«æç»“æœç»“æŸ]"""
        
        # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        messages = [HumanMessage(content=prompt)]
        response = await llm.ainvoke(messages, config=config)
        
        # è§£æLLMè¿”å›çš„ç»“æ„åŒ–æ–‡æœ¬
        try:
            content = response.content
            import re
            
            # åˆå§‹åŒ–ç»“æœ
            scan_result = {
                "found_sensitive_data": False,
                "details": {},
                "total_count": 0,
                "risk_level": "æ— ",
                "summary": ""
            }
            
            # æå–æ˜¯å¦å‘ç°æ•æ„Ÿæ•°æ®
            found_match = re.search(r'å‘ç°æ•æ„Ÿæ•°æ®:\s*(æ˜¯|å¦)', content)
            if found_match:
                scan_result["found_sensitive_data"] = found_match.group(1) == "æ˜¯"
            
            # æå–å„ç±»æ•æ„Ÿæ•°æ®
            sensitive_types = [
                "èº«ä»½è¯å·", "æ‰‹æœºå·", "é“¶è¡Œå¡å·", "é‚®ç®±åœ°å€", 
                "APIå¯†é’¥", "æ•°æ®åº“è¿æ¥", "IPåœ°å€", "å¯†ç ä¿¡æ¯",
                "è®¿é—®ä»¤ç‰Œ", "ç§é’¥è¯ä¹¦"
            ]
            
            for data_type in sensitive_types:
                # åŒ¹é…æ ¼å¼ï¼šç±»å‹: Xä¸ªï¼Œä½äºç¬¬Yè¡Œã€ç¬¬Zè¡Œ
                pattern = rf'{data_type}:\s*(\d+)ä¸ª[ï¼Œ,]?(.*)(?=\n|$)'
                match = re.search(pattern, content)
                if match:
                    count = int(match.group(1))
                    if count > 0:
                        locations_str = match.group(2)
                        # æå–ä½ç½®ä¿¡æ¯
                        locations = re.findall(r'ç¬¬(\d+)è¡Œ', locations_str)
                        locations = [f"ç¬¬{loc}è¡Œ" for loc in locations]
                        
                        scan_result["details"][data_type] = {
                            "count": count,
                            "locations": locations,
                            "risk": "é«˜" if data_type in ["èº«ä»½è¯å·", "é“¶è¡Œå¡å·", "APIå¯†é’¥", "ç§é’¥è¯ä¹¦"] else "ä¸­"
                        }
            
            # æå–æ€»æ•°
            total_match = re.search(r'æ•æ„Ÿæ•°æ®æ€»æ•°:\s*(\d+)ä¸ª', content)
            if total_match:
                scan_result["total_count"] = int(total_match.group(1))
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°æ€»æ•°ï¼Œä»detailsè®¡ç®—
                scan_result["total_count"] = sum(
                    info["count"] for info in scan_result["details"].values()
                )
            
            # æå–é£é™©ç­‰çº§
            risk_match = re.search(r'é£é™©ç­‰çº§:\s*(é«˜|ä¸­|ä½|æ— )', content)
            if risk_match:
                scan_result["risk_level"] = risk_match.group(1)
            
            # æå–æ‘˜è¦
            summary_match = re.search(r'\[æ‰«ææ‘˜è¦\]\n(.+?)(?=\[æ‰«æç»“æœç»“æŸ\]|$)', content, re.DOTALL)
            if summary_match:
                scan_result["summary"] = summary_match.group(1).strip()
            
        except Exception as e:
            logger.warning(f"è§£æLLMè¿”å›å†…å®¹å¤±è´¥: {e}")
            scan_result = {
                "found_sensitive_data": False,
                "details": {},
                "total_count": 0,
                "risk_level": "æ— ",
                "summary": "è§£æç»“æœå¤±è´¥"
            }
        
        return {
            "chunk_id": chunk["chunk_id"],
            "file_id": chunk["file_id"],
            "file_name": chunk["file_name"],
            "chunk_index": chunk["chunk_index"],
            "total_chunks": chunk["total_chunks"],
            "scan_result": scan_result,
            "success": True
        }
    except Exception as e:
        logger.error(f"LLMæ‰«æåˆ†ç‰‡ {chunk['chunk_id']} æ—¶å‡ºé”™: {str(e)}")
        return {
            "chunk_id": chunk["chunk_id"],
            "file_id": chunk["file_id"],
            "file_name": chunk["file_name"],
            "chunk_index": chunk["chunk_index"],
            "total_chunks": chunk["total_chunks"],
            "scan_result": None,
            "success": False,
            "error": str(e)
        }


async def parallel_scan(state: ScannerState, config: RunnableConfig) -> Dict[str, Any]:
    """ä¸²è¡Œæ‰«ææ‰€æœ‰åˆ†ç‰‡ - ä½¿ç”¨LLM"""
    from langchain_core.messages import AIMessage
    
    chunks = state.get("chunks", [])
    
    # è·å–LLMé…ç½®
    from .llm import get_llm_config
    from .configuration import INIT_AGENT_CONFIG
    from langchain_openai import ChatOpenAI
    
    agent_id = INIT_AGENT_CONFIG["agent_id"]
    selected_model = config.get("configurable", {}).get("selected_model") if config else None
    llm_config = get_llm_config(agent_id, selected_model)
    llm = ChatOpenAI(**llm_config)
    
    # è¾“å‡ºæ‰«æå¼€å§‹ä¿¡æ¯
    scan_progress_msg = f"ğŸ” å¼€å§‹ä¸²è¡Œæ‰«æ {len(chunks)} ä¸ªåˆ†ç‰‡..."
    scan_progress_msg += f"\n   - ä½¿ç”¨æ¨¡å‹: {llm_config.get('model', 'æœªçŸ¥')}"
    
    messages = state.get("messages", [])
    messages.append(AIMessage(content=scan_progress_msg))
    
    chunk_results = []
    
    # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºè¿›åº¦
    file_chunks_map = {}
    for chunk in chunks:
        file_name = chunk["file_name"]
        if file_name not in file_chunks_map:
            file_chunks_map[file_name] = []
        file_chunks_map[file_name].append(chunk)
    
    # ä¸²è¡Œå¤„ç†æ‰€æœ‰åˆ†ç‰‡
    current_file = None
    for i, chunk in enumerate(chunks):
        file_name = chunk["file_name"]
        chunk_index = chunk["chunk_index"]
        total_chunks = chunk["total_chunks"]
        
        # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œè¾“å‡ºæ–‡ä»¶ä¿¡æ¯
        if file_name != current_file:
            current_file = file_name
            file_msg = f"\nğŸ“„ æ­£åœ¨æ‰«ææ–‡ä»¶: {file_name} ({total_chunks} ä¸ªåˆ†ç‰‡)"
            messages.append(AIMessage(content=file_msg))
        
        # æ‰«æå•ä¸ªåˆ†ç‰‡
        chunk_result = await scan_chunk_with_llm(chunk, llm, config)
        chunk_results.append(chunk_result)
        
        # è¾“å‡ºè¿›åº¦ï¼ˆæ¯10ä¸ªåˆ†ç‰‡æˆ–æœ€åä¸€ä¸ªåˆ†ç‰‡æ—¶ï¼‰
        if (i + 1) % 10 == 0 or i == len(chunks) - 1:
            progress_msg = f"   è¿›åº¦: {i + 1}/{len(chunks)} åˆ†ç‰‡å·²å®Œæˆ"
            messages.append(AIMessage(content=progress_msg))
    
    # è¾“å‡ºæ‰«æå®Œæˆä¿¡æ¯
    complete_msg = f"\nâœ… å®Œæˆæ‰€æœ‰ {len(chunk_results)} ä¸ªåˆ†ç‰‡çš„æ‰«æï¼Œæ­£åœ¨æ±‡æ€»ç»“æœ...\n"
    messages.append(AIMessage(content=complete_msg))
    
    logger.info(f"å®Œæˆ {len(chunk_results)} ä¸ªåˆ†ç‰‡çš„LLMæ‰«æ")
    
    return {
        **state,
        "chunk_results": chunk_results,
        "messages": messages
    }


async def aggregate_results(state: ScannerState, config: RunnableConfig) -> Dict[str, Any]:
    """èšåˆæ‰«æç»“æœï¼ˆReduceé˜¶æ®µï¼‰"""
    chunk_results = state.get("chunk_results", [])
    file_contents = state.get("file_contents", {})
    
    # æŒ‰æ–‡ä»¶åˆ†ç»„ç»“æœ
    file_chunks = {}
    for chunk_result in chunk_results:
        if not chunk_result.get("success"):
            continue
            
        file_id = chunk_result["file_id"]
        if file_id not in file_chunks:
            file_chunks[file_id] = {
                "file_name": chunk_result["file_name"],
                "chunks": [],
                "total_chunks": chunk_result["total_chunks"]
            }
        
        file_chunks[file_id]["chunks"].append(chunk_result.get("scan_result", {}))
    
    # å¯¹æ¯ä¸ªæ–‡ä»¶åˆå¹¶å…¶åˆ†ç‰‡ç»“æœ
    file_results = {}
    total_sensitive_count = 0
    
    for file_id, file_data in file_chunks.items():
        # æ‰‹åŠ¨åˆå¹¶è¯¥æ–‡ä»¶çš„æ‰€æœ‰åˆ†ç‰‡ç»“æœ
        merged = {
            "found_sensitive_data": False,
            "details": {},
            "total_count": 0,
            "risk_level": "æ— ",
            "summaries": []
        }
        
        # åˆå¹¶å„åˆ†ç‰‡çš„ç»“æœ
        for idx, chunk_result in enumerate(file_data["chunks"]):
            if not chunk_result or not isinstance(chunk_result, dict):
                continue
                
            if chunk_result.get("found_sensitive_data", False):
                merged["found_sensitive_data"] = True
                
            # åˆå¹¶è¯¦ç»†ä¿¡æ¯
            details = chunk_result.get("details", {})
            for data_type, info in details.items():
                if data_type not in merged["details"]:
                    merged["details"][data_type] = {
                        "count": 0,
                        "locations": [],
                        "risk": "ä½"
                    }
                
                merged["details"][data_type]["count"] += info.get("count", 0)
                
                # æ·»åŠ åˆ†ç‰‡æ ‡è¯†åˆ°ä½ç½®ä¿¡æ¯
                locations = info.get("locations", [])
                for loc in locations:
                    merged["details"][data_type]["locations"].append(f"åˆ†ç‰‡{idx+1}-{loc}")
                
                # æ›´æ–°é£é™©ç­‰çº§ï¼ˆå–æœ€é«˜ï¼‰
                risk_levels = ["ä½", "ä¸­", "é«˜"]
                current_risk = info.get("risk", "ä½")
                if risk_levels.index(current_risk) > risk_levels.index(merged["details"][data_type]["risk"]):
                    merged["details"][data_type]["risk"] = current_risk
            
            # æ”¶é›†æ‘˜è¦
            if summary := chunk_result.get("summary"):
                merged["summaries"].append(f"åˆ†ç‰‡{idx+1}: {summary}")
            
            merged["total_count"] += chunk_result.get("total_count", 0)
        
        # è¯„ä¼°æ€»ä½“é£é™©ç­‰çº§
        if merged["total_count"] == 0:
            merged["risk_level"] = "æ— "
        elif merged["total_count"] < 10:
            merged["risk_level"] = "ä½"
        elif merged["total_count"] < 50:
            merged["risk_level"] = "ä¸­"
        else:
            merged["risk_level"] = "é«˜"
        
        file_results[file_id] = {
            "file_name": file_data["file_name"],
            "chunks_scanned": len(file_data["chunks"]),
            "total_chunks": file_data["total_chunks"],
            "total_count": merged.get("total_count", 0),
            "details": merged.get("details", {}),
            "risk_level": merged.get("risk_level", "æ— "),
            "summaries": merged.get("summaries", [])
        }
        
        total_sensitive_count += merged.get("total_count", 0)
    
    # è®¡ç®—æ€»ä½“é£é™©ç­‰çº§
    if total_sensitive_count == 0:
        overall_risk = "æ— "
    elif total_sensitive_count < 10:
        overall_risk = "ä½"
    elif total_sensitive_count < 50:
        overall_risk = "ä¸­"
    else:
        overall_risk = "é«˜"
    
    scan_results = {
        "scan_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "files_scanned": len(file_results),
        "total_sensitive_items": total_sensitive_count,
        "overall_risk_level": overall_risk,
        "file_results": file_results
    }
    
    logger.info(f"èšåˆå®Œæˆï¼šæ‰«æäº† {len(file_results)} ä¸ªæ–‡ä»¶ï¼Œå‘ç° {total_sensitive_count} ä¸ªæ•æ„Ÿé¡¹")
    
    return {
        **state,
        "scan_results": scan_results
    }


async def generate_report(state: ScannerState, config: RunnableConfig) -> Dict[str, Any]:
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    scan_results = state.get("scan_results", {})
    errors = state.get("errors", [])
    
    # ç”ŸæˆæŠ¥å‘Šå†…å®¹
    report_lines = [
        "# æ•æ„Ÿæ•°æ®æ‰¹é‡æ‰«ææŠ¥å‘Š",
        "",
        f"## æ‰«ææ¦‚è§ˆ",
        f"- æ‰«ææ—¶é—´: {scan_results.get('scan_time', 'æœªçŸ¥')}",
        f"- æ‰«ææ–‡ä»¶æ•°: {scan_results.get('files_scanned', 0)}",
        f"- æ•æ„Ÿæ•°æ®æ€»æ•°: {scan_results.get('total_sensitive_items', 0)}",
        f"- æ€»ä½“é£é™©ç­‰çº§: **{scan_results.get('overall_risk_level', 'æœªçŸ¥')}**"
    ]
    
    # æ·»åŠ å„æ–‡ä»¶çš„æ‰«æç»“æœ
    file_results = scan_results.get("file_results", {})
    if file_results:
        report_lines.extend(["", "## æ–‡ä»¶æ‰«æè¯¦æƒ…"])
        
        for file_id, result in file_results.items():
            report_lines.extend([
                "",
                f"### ğŸ“„ {result.get('file_name', file_id)}",
                f"- æ‰«æåˆ†ç‰‡: {result.get('chunks_scanned', 0)}/{result.get('total_chunks', 0)}",
                f"- æ•æ„Ÿæ•°æ®: {result.get('total_count', 0)} é¡¹"
            ])
            
            details = result.get("details", {})
            if details:
                report_lines.append("- è¯¦ç»†åˆ†ç±»:")
                for data_type, info in details.items():
                    count = info.get("count", 0)
                    if count > 0:
                        report_lines.append(f"  - {data_type}: {count} ä¸ª")
                        positions = info.get("positions", [])
                        if positions:
                            for pos in positions[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªä½ç½®
                                report_lines.append(f"    - {pos}")
                            if len(positions) > 3:
                                report_lines.append(f"    - ...è¿˜æœ‰ {len(positions) - 3} å¤„")
    
    # æ·»åŠ é”™è¯¯ä¿¡æ¯
    if errors:
        report_lines.extend([
            "",
            "## âš ï¸ å¤„ç†é”™è¯¯",
        ])
        for error in errors:
            report_lines.append(f"- {error}")
    
    # æ·»åŠ å»ºè®®
    report_lines.extend([
        "",
        "## ğŸ’¡ å®‰å…¨å»ºè®®",
    ])
    
    risk_level = scan_results.get("overall_risk_level", "æœªçŸ¥")
    if risk_level == "é«˜":
        report_lines.extend([
            "1. **ç«‹å³è¡ŒåŠ¨**: æ–‡æ¡£åŒ…å«å¤§é‡æ•æ„Ÿä¿¡æ¯ï¼Œéœ€è¦ç«‹å³é‡‡å–ä¿æŠ¤æªæ–½",
            "2. **æ•°æ®è„±æ•**: å¯¹æ•æ„Ÿæ•°æ®è¿›è¡Œè„±æ•å¤„ç†æˆ–ä½¿ç”¨åŠ å¯†å­˜å‚¨",
            "3. **è®¿é—®æ§åˆ¶**: ä¸¥æ ¼é™åˆ¶æ–‡æ¡£çš„è®¿é—®æƒé™ï¼Œå®æ–½æœ€å°æƒé™åŸåˆ™",
            "4. **å®¡è®¡è¿½è¸ª**: å»ºç«‹å®Œæ•´çš„æ•°æ®è®¿é—®å®¡è®¡æœºåˆ¶",
            "5. **åˆè§„æ£€æŸ¥**: ç¡®ä¿ç¬¦åˆGDPRã€ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ç­‰ç›¸å…³æ³•è§„"
        ])
    elif risk_level == "ä¸­":
        report_lines.extend([
            "1. **é£é™©è¯„ä¼°**: è¯„ä¼°æ•æ„Ÿä¿¡æ¯çš„å¿…è¦æ€§å’Œé£é™©",
            "2. **æ•°æ®åˆ†ç±»**: å¯¹ä¸åŒæ•æ„Ÿçº§åˆ«çš„æ•°æ®è¿›è¡Œåˆ†ç±»ç®¡ç†",
            "3. **å®šæœŸå®¡æŸ¥**: å®šæœŸå®¡æŸ¥å’Œæ›´æ–°æ•°æ®ä¿æŠ¤ç­–ç•¥",
            "4. **å‘˜å·¥åŸ¹è®­**: åŠ å¼ºæ•°æ®å®‰å…¨æ„è¯†åŸ¹è®­"
        ])
    elif risk_level == "ä½":
        report_lines.extend([
            "1. **ä¿æŒè­¦æƒ•**: ç»§ç»­ä¿æŒè‰¯å¥½çš„æ•°æ®å®‰å…¨ä¹ æƒ¯",
            "2. **å®šæœŸæ‰«æ**: å»ºè®®å®šæœŸè¿›è¡Œæ•æ„Ÿæ•°æ®æ‰«æ",
            "3. **é¢„é˜²ä¸ºä¸»**: åœ¨æ•°æ®äº§ç”Ÿæºå¤´å°±åšå¥½ä¿æŠ¤"
        ])
    else:
        report_lines.extend([
            "1. **å®‰å…¨è‰¯å¥½**: æœªå‘ç°æ•æ„Ÿæ•°æ®ï¼Œæ–‡æ¡£å®‰å…¨æ€§è‰¯å¥½",
            "2. **æŒç»­ç›‘æ§**: å»ºè®®å®šæœŸè¿›è¡Œå®‰å…¨æ‰«æ"
        ])
    
    final_report = "\n".join(report_lines)
    
    # åˆ›å»ºAIå“åº”æ¶ˆæ¯
    response_message = AIMessage(content=final_report)
    
    return {
        **state,
        "final_report": final_report,
        "messages": state.get("messages", []) + [response_message]
    }


# ä½¿ç”¨æ ‡å‡†çš„å·¥å…·èŠ‚ç‚¹
from langgraph.prebuilt import ToolNode
from .tools import analyze_sensitive_data_prompt, merge_scan_results, generate_scan_report

# åˆ›å»ºå·¥å…·èŠ‚ç‚¹å®ä¾‹
tool_node = ToolNode([get_file_content, analyze_sensitive_data_prompt, merge_scan_results, generate_scan_report])