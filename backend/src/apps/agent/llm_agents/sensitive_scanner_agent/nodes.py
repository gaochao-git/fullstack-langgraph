"""æ•æ„Ÿæ•°æ®æ‰«æèŠ‚ç‚¹å®ç°"""
from typing import Dict, Any
from langchain_core.messages import AIMessage
from src.shared.db.config import get_async_db_context
from src.apps.agent.service.document_service import document_service
from src.shared.core.logging import get_logger
from .state import OverallState
from .llm import get_llm

logger = get_logger(__name__)


async def fetch_files(state: OverallState) -> Dict[str, Any]:
    """è·å–æ–‡ä»¶å†…å®¹å’Œç”¨æˆ·è¾“å…¥æ–‡æœ¬"""
    file_contents = {}
    errors = []
    user_input_text = ""
    
    # 1. æå–ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
    for msg in state["messages"]:
        # æ‰¾åˆ°ç”¨æˆ·çš„æ¶ˆæ¯ï¼ˆhuman messageï¼‰
        if hasattr(msg, "type") and msg.type == "human":
            user_input_text = msg.content
            logger.info(f"æå–åˆ°ç”¨æˆ·è¾“å…¥æ–‡æœ¬: {user_input_text[:100]}...")
            break
    
    # 2. ä»æ¶ˆæ¯ä¸­æå– file_ids
    file_ids = []
    for msg in state["messages"]:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ human message ä¸”åŒ…å« file_ids
        if hasattr(msg, "additional_kwargs") and msg.additional_kwargs:
            if "file_ids" in msg.additional_kwargs:
                file_ids.extend(msg.additional_kwargs["file_ids"])
    
    # å¦‚æœstateä¸­ä¹Ÿæœ‰file_idsï¼Œåˆå¹¶
    if state.get("file_ids"):
        file_ids.extend(state["file_ids"])
    
    # å»é‡
    file_ids = list(set(file_ids))
    
    # 3. åˆ¤æ–­æ‰«æå†…å®¹
    has_user_text = bool(user_input_text and user_input_text.strip())
    has_files = bool(file_ids)
    
    if not has_user_text and not has_files:
        return {
            "user_input_text": "",
            "file_contents": {},
            "errors": ["æœªæ‰¾åˆ°éœ€è¦æ‰«æçš„å†…å®¹"],
            "messages": state["messages"] + [AIMessage(content="æœªæ‰¾åˆ°éœ€è¦æ‰«æçš„å†…å®¹ï¼Œè¯·æä¾›æ–‡æœ¬æˆ–ä¸Šä¼ æ–‡ä»¶")]
        }
    
    # 4. è·å–æ–‡ä»¶å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if has_files:
        logger.info(f"å‡†å¤‡è·å–æ–‡ä»¶å†…å®¹ï¼Œfile_ids: {file_ids}")
        
        async with get_async_db_context() as db:
            for file_id in file_ids:
                try:
                    doc_info = await document_service.get_document_content(db, file_id)
                    if doc_info:
                        file_contents[file_id] = {
                            "content": doc_info.get("content", ""),
                            "file_name": doc_info.get("file_name", ""),
                            "file_size": doc_info.get("file_size", 0)
                        }
                    else:
                        errors.append(f"æ–‡ä»¶ {file_id} ä¸å­˜åœ¨")
                except Exception as e:
                    logger.error(f"è·å–æ–‡ä»¶å†…å®¹å¤±è´¥ {file_id}: {e}")
                    errors.append(f"è·å–æ–‡ä»¶ {file_id} å¤±è´¥: {str(e)}")
    
    # 5. ç”ŸæˆçŠ¶æ€æ¶ˆæ¯
    status_parts = []
    if has_user_text:
        status_parts.append("ç”¨æˆ·è¾“å…¥æ–‡æœ¬")
    if len(file_contents) > 0:
        status_parts.append(f"{len(file_contents)} ä¸ªæ–‡ä»¶")
    
    status_message = f"å‡†å¤‡æ‰«æ: {' å’Œ '.join(status_parts)}"
    
    return {
        "user_input_text": user_input_text,
        "file_contents": file_contents,
        "errors": state.get("errors", []) + errors,
        "messages": state["messages"] + [AIMessage(content=status_message)]
    }


async def scan_files(state: OverallState) -> Dict[str, Any]:
    """ä¸²è¡Œæ‰«æç”¨æˆ·è¾“å…¥å’Œæ–‡ä»¶ä¸­çš„æ•æ„Ÿæ•°æ®ï¼Œæ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹è°ƒç”¨LLM"""
    
    # å‡†å¤‡æ‰€æœ‰éœ€è¦æ‰«æçš„å†…å®¹æº
    scan_sources = []
    
    # 1. ç”¨æˆ·è¾“å…¥æ–‡æœ¬
    user_text = state.get("user_input_text", "")
    if user_text and user_text.strip():
        scan_sources.append({
            "source_name": "ç”¨æˆ·è¾“å…¥æ–‡æœ¬",
            "content": user_text,
            "source_type": "user_input"
        })
    
    # 2. æ–‡ä»¶å†…å®¹ï¼ˆæ¯ä¸ªæ–‡ä»¶ä½œä¸ºç‹¬ç«‹çš„æ‰«ææºï¼‰
    for file_id, file_info in state.get("file_contents", {}).items():
        if file_info.get("content"):
            scan_sources.append({
                "source_name": f"æ–‡ä»¶ï¼š{file_info.get('file_name', file_id)}",
                "content": file_info["content"],  # ä¸å†æˆªæ–­
                "source_type": "file",
                "file_id": file_id,
                "file_size": file_info.get("file_size", len(file_info["content"]))
            })
    
    if not scan_sources:
        return {
            "messages": state["messages"] + [AIMessage(content="æœªæ‰¾åˆ°éœ€è¦æ‰«æçš„å†…å®¹")]
        }
    
    # æ”¶é›†æ‰€æœ‰æ‰«æç»“æœ
    all_scan_results = []
    
    # ä¸²è¡Œæ‰«ææ¯ä¸ªå†…å®¹æº
    for i, source in enumerate(scan_sources, 1):
        # ä¸ºæ¯ä¸ªæºåˆ›å»ºæ–°çš„LLMå®ä¾‹
        llm = get_llm()
        
        logger.info(f"æ‰«æè¿›åº¦ [{i}/{len(scan_sources)}] - {source['source_name']}")
        
        try:
            # ä¸ºæ¯ä¸ªæºæ„å»ºä¸“é—¨çš„æç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•æ„Ÿæ•°æ®æ‰«æå·¥å…·ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ‰«ææ–‡æœ¬ä¸­çš„æ•æ„Ÿä¿¡æ¯å¹¶ç”Ÿæˆè„±æ•åçš„å®‰å…¨æŠ¥å‘Šã€‚

å¾…æ‰«æå†…å®¹æ¥æºï¼š{source['source_name']}
å†…å®¹é•¿åº¦ï¼š{len(source['content'])} å­—ç¬¦

å¾…æ‰«æå†…å®¹ï¼š
{source['content']}

ä½ çš„ä»»åŠ¡ï¼šæ‰«æä¸Šè¿°å†…å®¹ä¸­çš„æ‰€æœ‰æ•æ„Ÿæ•°æ®ï¼Œå¹¶ç”Ÿæˆè„±æ•æŠ¥å‘Šã€‚

ç‰¹åˆ«æ³¨æ„ï¼š
- å¦‚æœå†…å®¹çœ‹èµ·æ¥åƒæ˜¯æ–‡ä»¶è§£æå¤±è´¥çš„é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚"è§£æå¤±è´¥"ã€"æ— æ³•è¯»å–"ã€"æ–‡ä»¶æŸå"ç­‰ï¼‰ï¼Œè¯·è¾“å‡ºï¼š
  â€¢ æ–‡ä»¶å†…å®¹å¼‚å¸¸ï¼š[ç®€è¦è¯´æ˜å¼‚å¸¸æƒ…å†µ]
- å¦‚æœå†…å®¹æ˜¯ä¹±ç æˆ–æ— æ³•ç†è§£çš„æ ¼å¼ï¼Œè¯·è¾“å‡ºï¼š
  â€¢ æ–‡ä»¶å†…å®¹å¼‚å¸¸ï¼šæ–‡ä»¶å¯èƒ½å·²æŸåæˆ–æ ¼å¼ä¸æ”¯æŒ

é‡è¦æç¤ºï¼š
- å•ç‹¬çš„ç”¨æˆ·åï¼ˆå¦‚ï¼šgaochaoã€adminç­‰ï¼‰ä¸å±äºæ•æ„Ÿä¿¡æ¯ï¼Œä¸éœ€è¦è„±æ•
- ç”¨æˆ·å+å¯†ç çš„ç»„åˆæ‰æ˜¯æ•æ„Ÿä¿¡æ¯ï¼ˆéœ€è¦å¯¹å¯†ç è„±æ•ï¼‰
- é‡ç‚¹å…³æ³¨ï¼šèº«ä»½è¯å·ã€æ‰‹æœºå·ã€é“¶è¡Œå¡å·ã€å¯†ç ã€é‚®ç®±ã€IPåœ°å€ã€APIå¯†é’¥ç­‰

é‡è¦è¦æ±‚ï¼š
- ç»å¯¹ä¸è¦åœ¨æŠ¥å‘Šä¸­æ˜¾ç¤ºæ•æ„Ÿä¿¡æ¯çš„åŸå§‹å€¼
- å¯¹æ‰€æœ‰æ•æ„Ÿä¿¡æ¯è¿›è¡Œè„±æ•å¤„ç†ï¼Œè§„åˆ™å¦‚ä¸‹ï¼š
  * é»˜è®¤ï¼šå°†æ•æ„Ÿä¿¡æ¯çš„ååŠéƒ¨åˆ†ç”¨*æ›¿æ¢ï¼ˆå¦‚ï¼š13812345678 â†’ 138****5678ï¼‰
  * èº«ä»½è¯å·ï¼šåªæ˜¾ç¤ºå‰6ä½å’Œå4ä½ï¼ˆå¦‚ï¼š110101****1234ï¼‰
  * é“¶è¡Œå¡å·ï¼šåªæ˜¾ç¤ºå‰4ä½å’Œå4ä½ï¼ˆå¦‚ï¼š6222****4321ï¼‰
  * é‚®ç®±ï¼š@å‰é¢éƒ¨åˆ†éšè—ä¸€åŠï¼ˆå¦‚ï¼štest@163.com â†’ te**@163.comï¼‰
  * å¯†ç ï¼šæ˜¾ç¤ºå‰2-3ä½å­—ç¬¦ï¼Œå…¶ä½™ç”¨*æ›¿æ¢ï¼ˆå¦‚ï¼š12345678 â†’ 123*****ï¼‰
  * çŸ­å¯†ç ï¼ˆå°‘äº6ä½ï¼‰ï¼šæ˜¾ç¤ºç¬¬ä¸€ä½ï¼ˆå¦‚ï¼š12345 â†’ 1****ï¼‰

è¾“å‡ºè¦æ±‚ï¼š
è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆæ¯è¡Œç´§å‡‘ï¼Œä¸è¦ç©ºè¡Œï¼‰ï¼š

1. æ–‡æ¡£æ‘˜è¦ï¼š[ç®€è¦æè¿°æ–‡æ¡£å†…å®¹ï¼Œä¸è¶…è¿‡50å­—]
2. æ•æ„Ÿä¿¡æ¯æ‰«æç»“æœï¼šâ€¢ å‘ç°[ç±»å‹]ï¼š[è„±æ•åçš„å€¼] (å…³è”ä¿¡æ¯) æˆ– â€¢ æœªå‘ç°æ•æ„Ÿä¿¡æ¯

é‡è¦ï¼š
- åªéœ€è¦è¾“å‡ºè¿™2è¡Œ
- æ¯è¡Œå†…å®¹ç´§å‡‘ï¼Œä¸è¦æ¢è¡Œ
- å¤šä¸ªæ•æ„Ÿä¿¡æ¯ç”¨ â€¢ åˆ†éš”ï¼Œéƒ½åœ¨ç¬¬2è¡Œå†…"""
            
            # è°ƒç”¨LLMæ‰«æ
            result = await llm.ainvoke(prompt)
            
            # åˆ¤æ–­æ‰«æç»“æœç±»å‹
            scan_content = result.content
            is_error = "æ–‡ä»¶å†…å®¹å¼‚å¸¸" in scan_content
            has_sensitive = "æœªå‘ç°æ•æ„Ÿä¿¡æ¯" not in scan_content and not is_error
            
            # è®¡ç®—æ–‡å­—æ•°é‡ï¼ˆåŒ…æ‹¬å…¨éƒ¨å†…å®¹ï¼‰
            word_count = len(source['content'])
            
            all_scan_results.append({
                "source": source['source_name'],
                "scan_result": scan_content,
                "has_sensitive": has_sensitive,
                "is_content_error": is_error,
                "file_size": source.get('file_size', 0),  # ä¿å­˜æ–‡ä»¶å¤§å°
                "word_count": word_count  # ä¿å­˜æ–‡å­—æ•°é‡
            })
            
        except Exception as e:
            logger.error(f"æ‰«æ {source['source_name']} æ—¶å‡ºé”™: {e}")
            all_scan_results.append({
                "source": source['source_name'],
                "scan_result": f"æ‰«æå¤±è´¥: {str(e)}",
                "has_sensitive": False,
                "error": True,
                "file_size": source.get('file_size', 0),  # ä¿å­˜æ–‡ä»¶å¤§å°
                "word_count": 0  # é”™è¯¯æ—¶æ–‡å­—æ•°é‡ä¸º0
            })
    
    # æ„å»ºæœ€ç»ˆçš„ç»¼åˆæŠ¥å‘Š
    sensitive_count = sum(1 for r in all_scan_results if r.get("has_sensitive", False))
    error_count = sum(1 for r in all_scan_results if r.get("error", False))
    content_error_count = sum(1 for r in all_scan_results if r.get("is_content_error", False))
    
    # æ„å»ºæŠ¥å‘Šå†…å®¹
    report_parts = []
    
    # æ‰«ææ¦‚è§ˆ
    report_parts.append("ã€æ‰«ææ¦‚è§ˆã€‘")
    report_parts.append(f"ğŸ“Š æ‰«æèŒƒå›´ï¼š{len(scan_sources)} ä¸ªå†…å®¹æº")
    report_parts.append("")
    
    # æ‰«æè¯¦æƒ…
    report_parts.append("ã€æ‰«æè¯¦æƒ…ã€‘")
    for i, result in enumerate(all_scan_results, 1):
        # æ„å»ºå†…å®¹æºæ ‡é¢˜
        source_type = "ç”¨æˆ·è¾“å…¥æ–‡æœ¬" if i == 1 and "ç”¨æˆ·è¾“å…¥" in result['source'] else f"ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶{result['source'].replace('æ–‡ä»¶ï¼š', '')}"
        report_parts.append(f"\nå†…å®¹æº{i}:{source_type}")
        
        # 1. æ–‡ä»¶çŠ¶æ€
        if result.get("error"):
            status = "å†…å®¹è§£æå¼‚å¸¸"
        elif result.get("is_content_error"):
            status = "å†…å®¹è§£æå¼‚å¸¸"
        # å¯ä»¥æ ¹æ®å†…å®¹é•¿åº¦åˆ¤æ–­æ˜¯å¦æˆªæ–­ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        else:
            status = "å†…å®¹å·²è§£æ"
        report_parts.append(f"1. æ–‡ä»¶çŠ¶æ€ï¼š{status}")
        
        # ä»LLMè¿”å›çš„å†…å®¹ä¸­æå–ä¿¡æ¯
        scan_content = result['scan_result']
        
        # è§£æLLMçš„è¾“å‡ºï¼ˆç°åœ¨åªæœ‰2è¡Œï¼‰å¹¶æŒ‰é¡ºåºç»„è£…æŠ¥å‘Š
        lines = scan_content.strip().split('\n')
        
        # ä¸´æ—¶å­˜å‚¨å„éƒ¨åˆ†å†…å®¹
        doc_summary = ""
        scan_result = ""
        
        for line in lines:
            if line.startswith('1. '):
                # æ–‡æ¡£æ‘˜è¦
                doc_summary = '2. ' + line[3:]
            elif line.startswith('2. '):
                # æ•æ„Ÿä¿¡æ¯æ‰«æç»“æœ
                scan_result = '4. ' + line[3:]
        
        # æŒ‰æ­£ç¡®çš„é¡ºåºæ·»åŠ åˆ°æŠ¥å‘Šä¸­
        if doc_summary:
            report_parts.append(doc_summary)
        
        # æ·»åŠ æ–‡æ¡£ä¿¡æ¯è¡Œï¼ˆç¬¬3è¡Œï¼‰
        file_size = result.get('file_size', 0)
        word_count = result.get('word_count', 0)
        report_parts.append(f"3. æ–‡æ¡£ä¿¡æ¯ï¼šæ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚ â€¢ æ–‡å­—æ•°é‡: {word_count}å­—")
        
        # æ·»åŠ æ•æ„Ÿä¿¡æ¯æ‰«æç»“æœï¼ˆç¬¬4è¡Œï¼‰
        if scan_result:
            report_parts.append(scan_result)
    
    # æ‰«ææ€»ç»“
    report_parts.append("\nã€æ‰«ææ€»ç»“ã€‘")
    
    # è®¡ç®—å¤„ç†å¼‚å¸¸æ•°ï¼ˆåŒ…æ‹¬è§£æå¼‚å¸¸å’Œæ‰«æå¤±è´¥ï¼‰
    total_errors = error_count + content_error_count
    
    report_parts.append(f"â€¢ æ€»è®¡æ‰«æï¼š{len(scan_sources)} ä¸ªå†…å®¹æº")
    report_parts.append(f"â€¢ å¤„ç†å¼‚å¸¸ï¼š{total_errors} ä¸ªå†…å®¹æº")
    report_parts.append(f"â€¢ å‘ç°æ•æ„Ÿä¿¡æ¯ï¼š{sensitive_count} ä¸ªå†…å®¹æº")
    
    final_report = "\n".join(report_parts)
    
    # åªè¿”å›ä¸€ä¸ªæœ€ç»ˆçš„ç»¼åˆæŠ¥å‘Šæ¶ˆæ¯
    return {
        "messages": state["messages"] + [AIMessage(content=final_report)]
    }