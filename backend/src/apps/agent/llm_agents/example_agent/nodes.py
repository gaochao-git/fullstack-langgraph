"""
Example Agent èŠ‚ç‚¹å®šä¹‰
å±•ç¤ºå¦‚ä½•å®šä¹‰è‡ªå®šä¹‰å·¥ä½œæµèŠ‚ç‚¹
"""

from typing import Dict, Any, Literal
from langchain_core.messages import AIMessage, ToolMessage, HumanMessage
from langchain_openai import ChatOpenAI
from .state import ExampleAgentState
from .tools import word_counter, text_analyzer
from .configuration import INIT_AGENT_CONFIG  # ä» configuration.py å¯¼å…¥
from .llm import get_llm_config
from src.shared.core.logging import get_logger

logger = get_logger(__name__)


async def analyze_task_node(state: ExampleAgentState) -> Dict[str, Any]:
    """åˆ†æä»»åŠ¡ç±»å‹çš„èŠ‚ç‚¹
    
    æ ¹æ®ç”¨æˆ·æ¶ˆæ¯åˆ¤æ–­éœ€è¦æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹
    """
    messages = state.get("messages", [])
    if not messages:
        return {"task_type": "unknown", "error": "æ²¡æœ‰æ¶ˆæ¯"}
    
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_user_message = None
    for msg in reversed(messages):
        if hasattr(msg, 'type') and msg.type == "human":
            last_user_message = msg.content
            break
    
    if not last_user_message:
        return {"task_type": "unknown", "error": "æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯"}
    
    # ç®€å•çš„ä»»åŠ¡åˆ†ç±»ï¼šå¤æ‚ä»»åŠ¡éœ€è¦ LLMï¼Œå…¶ä»–ç”¨é€šç”¨å¤„ç†
    content_lower = last_user_message.lower()
    if "å¤æ‚" in content_lower or "è¯¦ç»†åˆ†æ" in content_lower or len(last_user_message) > 200:
        task_type = "complex"
    else:
        task_type = "simple"
    
    logger.debug(f"è¯†åˆ«ä»»åŠ¡ç±»å‹: {task_type}")
    
    return {
        "task_type": task_type,
        "workflow_steps": [f"ä»»åŠ¡è¯†åˆ«: {task_type}"]
    }


async def process_task_node(state: ExampleAgentState) -> Dict[str, Any]:
    """å¤„ç†ç®€å•ä»»åŠ¡çš„èŠ‚ç‚¹
    
    ä½¿ç”¨å†…ç½®å·¥å…·å¤„ç†ä»»åŠ¡
    """
    messages = state.get("messages", [])
    processing_results = state.get("processing_results", {})
    workflow_steps = state.get("workflow_steps", [])
    
    # è·å–è¦å¤„ç†çš„æ–‡æœ¬
    text_to_process = ""
    for msg in reversed(messages):
        if hasattr(msg, 'type') and msg.type == "human":
            text_to_process = msg.content
            break
    
    # æ‰§è¡Œå¤„ç†ï¼ˆè¿™é‡Œæ¼”ç¤ºä½¿ç”¨å·¥å…·ï¼‰
    try:
        # å­—æ•°ç»Ÿè®¡
        word_result = word_counter.invoke({"text": text_to_process})
        processing_results["word_count"] = word_result
        
        # æ–‡æœ¬åˆ†æ
        analysis_result = text_analyzer.invoke({"text": text_to_process})
        processing_results["analysis"] = analysis_result
        
        workflow_steps.append("ä½¿ç”¨å·¥å…·å¤„ç†ä»»åŠ¡")
        
    except Exception as e:
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè®¾ç½®é”™è¯¯çŠ¶æ€
        return {
            "error": str(e),
            "workflow_steps": workflow_steps + ["å¤„ç†å¤±è´¥"]
        }
    
    return {
        "processing_results": processing_results,
        "workflow_steps": workflow_steps
    }


async def format_response_node(state: ExampleAgentState) -> Dict[str, Any]:
    """æ ¼å¼åŒ–å“åº”çš„èŠ‚ç‚¹
    
    å°†å¤„ç†ç»“æœæ ¼å¼åŒ–ä¸ºç”¨æˆ·å‹å¥½çš„æ¶ˆæ¯
    """
    processing_results = state.get("processing_results", {})
    workflow_steps = state.get("workflow_steps", [])
    
    # æ„å»ºå“åº”æ¶ˆæ¯
    response_parts = []
    
    # å¦‚æœæœ‰å¤„ç†ç»“æœï¼Œæ ¼å¼åŒ–è¾“å‡º
    if processing_results:
        if "word_count" in processing_results:
            response_parts.append(f"ğŸ“Š {processing_results['word_count']}")
        
        if "analysis" in processing_results:
            response_parts.append(f"\nğŸ” {processing_results['analysis']}")
            
        if "llm_response" in processing_results:
            response_parts.append(f"\nğŸ’¡ {processing_results['llm_response']}")
    else:
        response_parts.append("âœ… å¤„ç†å®Œæˆ")
    
    # æ·»åŠ å·¥ä½œæµæ­¥éª¤ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰
    if logger.isEnabledFor(10):  # DEBUG level
        response_parts.append(f"\n\nğŸ”§ å·¥ä½œæµæ­¥éª¤ï¼š")
        for step in workflow_steps:
            response_parts.append(f"\n  - {step}")
    
    workflow_steps.append("æ ¼å¼åŒ–å“åº”")
    
    # åˆ›å»º AI æ¶ˆæ¯
    response_message = AIMessage(content="".join(response_parts))
    
    return {
        "messages": [response_message],
        "workflow_steps": workflow_steps
    }


def should_continue_after_task(state: ExampleAgentState) -> Literal["process_task", "llm_process"]:
    """ä»»åŠ¡åˆ†æåçš„è·¯ç”±å†³ç­–
    
    ä½¿ç”¨ Literal ç±»å‹æ³¨è§£æä¾›æ›´å¥½çš„ç±»å‹å®‰å…¨æ€§ï¼ˆLangGraph 0.6.6 æ¨èï¼‰
    
    Returns:
        ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åç§°
    """
    task_type = state.get("task_type", "unknown")
    
    # ç®€åŒ–çš„è·¯ç”±é€»è¾‘ï¼šå¤æ‚ä»»åŠ¡ç”¨ LLMï¼Œå…¶ä»–ç”¨é€šç”¨å¤„ç†
    if task_type == "complex":
        return "llm_process"
    else:
        return "process_task"


def should_retry_or_continue(state: ExampleAgentState) -> Literal["retry", "error_handler", "format_response"]:
    """å¤„ç†åå†³å®šæ˜¯å¦é‡è¯•æˆ–ç»§ç»­
    
    ä½¿ç”¨ Literal ç±»å‹æ³¨è§£æä¾›æ›´å¥½çš„ç±»å‹å®‰å…¨æ€§ï¼ˆLangGraph 0.6.6 æ¨èï¼‰
    
    Returns:
        ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åç§°
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    error = state.get("error")
    if error:
        retry_count = state.get("retry_count", 0)
        if retry_count < 3:
            return "retry"
        else:
            return "error_handler"
    
    # æ­£å¸¸æµç¨‹ï¼Œæ ¼å¼åŒ–å“åº”
    return "format_response"


async def error_handler_node(state: ExampleAgentState) -> Dict[str, Any]:
    """é”™è¯¯å¤„ç†èŠ‚ç‚¹"""
    error = state.get("error", "æœªçŸ¥é”™è¯¯")
    workflow_steps = state.get("workflow_steps", [])
    
    error_message = f"âŒ å¤„ç†å¤±è´¥: {error}"
    workflow_steps.append(f"é”™è¯¯å¤„ç†: {error}")
    
    return {
        "messages": [AIMessage(content=error_message)],
        "workflow_steps": workflow_steps
    }


async def llm_process_node(state: ExampleAgentState) -> Dict[str, Any]:
    """ä½¿ç”¨ LLM å¤„ç†å¤æ‚ä»»åŠ¡
    
    è¿™ä¸ªèŠ‚ç‚¹å±•ç¤ºäº†å¦‚ä½•åœ¨èŠ‚ç‚¹å†…éƒ¨åˆ›å»ºå’Œä½¿ç”¨ LLM
    """
    messages = state.get("messages", [])
    workflow_steps = state.get("workflow_steps", [])
    
    # è·å– LLM é…ç½®ï¼ˆä½¿ç”¨ agent_utils çš„å…¬å…±æ–¹æ³•ï¼‰
    llm_config = get_llm_config(INIT_AGENT_CONFIG["agent_id"])
    llm = ChatOpenAI(**llm_config)
    
    # æ„å»º LLM æç¤º
    prompt = "è¯·å¤„ç†ä»¥ä¸‹å¤æ‚ä»»åŠ¡ï¼š"
    for msg in messages:
        if hasattr(msg, 'content'):
            prompt += f"\n{msg.content}"
    
    try:
        # è°ƒç”¨ LLM
        response = await llm.ainvoke([HumanMessage(content=prompt)])
        
        workflow_steps.append("LLM å¤„ç†å¤æ‚ä»»åŠ¡")
        
        return {
            "messages": [response],
            "processing_results": {"llm_response": response.content},
            "workflow_steps": workflow_steps
        }
    except Exception as e:
        return {
            "error": f"LLM è°ƒç”¨å¤±è´¥: {str(e)}",
            "workflow_steps": workflow_steps + ["LLM è°ƒç”¨å¤±è´¥"]
        }