"""
Agent è¿è¡Œå¤„ç†æ¨¡å—
å¤„ç† LangGraph Agent çš„æµå¼å’Œéæµå¼è¿è¡Œè¯·æ±‚
"""
import json
from typing import Dict, Any, List
from src.shared.core.logging import get_logger
from src.shared.core.exceptions import BusinessException
from src.shared.schemas.response import ResponseCode, success_response
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
from sqlalchemy import select
from src.shared.db.config import get_async_db_context
from .document_service import document_service
from ..utils import serialize_value
from .user_threads_db import (check_user_thread_exists,create_user_thread_mapping)
from ..llm_agents.agent_registry import AgentRegistry
from .agent_service import agent_service
from ..models import AgentDocumentSession
from ..llm_agents.hooks import create_token_usage_hook
logger = get_logger(__name__)

# å®šä¹‰è¿è¡Œè¯·æ±‚ä½“
class RunCreate(BaseModel):
    agent_id: str  # æ™ºèƒ½ä½“IDï¼ˆå¿…éœ€ï¼‰
    user_name: str  # ç”¨æˆ·åï¼ˆå¿…éœ€ï¼‰
    query: str  # æŸ¥è¯¢å†…å®¹ï¼ˆå¿…éœ€ï¼‰
    chat_mode: str = "streaming"  # èŠå¤©æ¨¡å¼ï¼šstreaming æˆ– blocking
    file_ids: Optional[List[str]] = None  # æ–‡ä»¶IDåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    config: Optional[Dict[str, Any]] = None  # é…ç½®ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    

async def prepare_run(thread_id: str, request_body: RunCreate, request=None) -> tuple[str, dict, str]:
    """æ›´æ–°æ™ºèƒ½ä½“ä½¿ç”¨ç»Ÿè®¡ï¼Œç¡®ä¿ç”¨æˆ·çº¿ç¨‹æ˜ å°„"""
    # æ›´æ–°æ™ºèƒ½ä½“ä½¿ç”¨ç»Ÿè®¡
    agent_id = request_body.agent_id
    try:
        async with get_async_db_context() as async_db:
            await agent_service.increment_run_count(async_db, agent_id)
    except Exception as e:
        # ç»Ÿè®¡æ›´æ–°å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        logger.error(f"æ›´æ–°æ™ºèƒ½ä½“ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
    
    # ä½¿ç”¨è¯·æ±‚ä¸­çš„ user_name
    user_name = request_body.user_name
    try:
        await ensure_user_thread_mapping(user_name, thread_id, request_body)
    except Exception as e:
         # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ 
        logger.error(f"å¤„ç†ç”¨æˆ·çº¿ç¨‹å…³è”æ—¶å‡ºé”™: {e}", exc_info=True)


async def ensure_user_thread_mapping(user_name, thread_id, request_body):
    """
    ç¡®ä¿ç”¨æˆ·å’Œçº¿ç¨‹çš„å½’å±å·²å†™å…¥user_threadsè¡¨ï¼Œå¦‚ä¸å­˜åœ¨åˆ™è‡ªåŠ¨å†™å…¥ã€‚
    è‡ªåŠ¨æå–thread_titleï¼ˆå–queryå†…å®¹å‰20å­—ï¼‰ã€‚
    """
    exists = await check_user_thread_exists(user_name, thread_id)
    if not exists:
        # ä½¿ç”¨ query ä½œä¸ºæ ‡é¢˜
        thread_title = request_body.query[:20] + "..." if len(request_body.query) > 20 else request_body.query
        
        # ä»request_bodyä¸­è·å–agent_idï¼Œå†…éƒ¨ä½œä¸ºagent_idä½¿ç”¨
        agent_id = request_body.agent_id
        
        logger.info(f"åˆ›å»ºç”¨æˆ·çº¿ç¨‹æ˜ å°„: user_name={user_name}, thread_id={thread_id}, thread_title={thread_title}, agent_id={agent_id}")
        await create_user_thread_mapping(user_name, thread_id, thread_title, agent_id)

def prepare_config(request_body, thread_id):
    """å‡†å¤‡é…ç½® - å…¬å…±æ–¹æ³•"""
    config = request_body.config or {}
    
    # æå– stream_mode å’Œå…¶ä»–é…ç½®
    stream_mode = config.get("stream_mode", ["updates", "messages", "values"])
    selected_model = config.get("selected_model")
    
    # æ„å»ºæ–°çš„é…ç½®ç»“æ„
    new_config = {
        "configurable": {
            "thread_id": thread_id,
            "user_name": request_body.user_name,
            "agent_id": request_body.agent_id
        },
        "recursion_limit": config.get("recursion_limit", 100)
    }
    
    # æ·»åŠ æ¨¡å‹é€‰æ‹©
    if selected_model:
        new_config["configurable"]["selected_model"] = selected_model
        
    return new_config, stream_mode


async def prepare_graph_input(request_body, config, thread_id):
    """å‡†å¤‡å›¾è¾“å…¥ - æ„å»ºæ¶ˆæ¯æ ¼å¼å¹¶å¤„ç†æ–‡æ¡£ä¸Šä¸‹æ–‡"""
    # æ„å»ºæ¶ˆæ¯æ ¼å¼çš„è¾“å…¥
    messages = [{"type": "human","content": request_body.query}]
    
    # å¦‚æœæœ‰æ–‡æ¡£ï¼Œè·å–æ–‡æ¡£ä¿¡æ¯å¹¶æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
    docs_info = []
    if request_body.file_ids:
        logger.info(f"æ£€æµ‹åˆ°å…³è”æ–‡æ¡£ {len(request_body.file_ids)} ä¸ª: {request_body.file_ids}")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•è·å–æ–‡æ¡£å…ƒä¿¡æ¯ï¼ˆä¸åŒ…å«å†…å®¹ï¼‰
        async with get_async_db_context() as db:
            docs_info = await document_service.get_documents_info_async(db, request_body.file_ids)
        
        if docs_info:
            logger.info(f"æˆåŠŸè·å– {len(docs_info)} ä¸ªæ–‡æ¡£çš„å…ƒä¿¡æ¯")
            # æ„å»ºfilesæ•°ç»„ï¼ŒåŒ…å«æ–‡ä»¶çš„å®Œæ•´ä¿¡æ¯
            files = []
            for doc in docs_info:
                files.append({
                    "file_id": doc['file_id'],
                    "file_name": doc['file_name'],
                    "file_size": doc['file_size']
                })
            
            # å°†filesä¿¡æ¯æ·»åŠ åˆ°æ¶ˆæ¯ä¸­ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è½¬ä¸ºadditional_kwargs
            messages[0]["files"] = files
            logger.debug(f"å·²å°†æ–‡ä»¶ä¿¡æ¯æ·»åŠ åˆ°æ¶ˆæ¯çš„ files å­—æ®µ")
        else:
            logger.warning(f"æœªèƒ½è·å–åˆ°ä»»ä½•æ–‡æ¡£ä¿¡æ¯")
    
    graph_input = {"messages": messages}
    
    # å¦‚æœæœ‰å…³è”çš„æ–‡æ¡£ï¼Œå°†æ–‡æ¡£å…ƒä¿¡æ¯æ·»åŠ åˆ°æ¶ˆæ¯ä¸Šä¸‹æ–‡ä¸­
    if docs_info:
            # æ„å»ºæ–‡æ¡£ä¿¡æ¯çš„æç¤º
            files_summary = "\n".join([f"- {doc['file_name']} (ID: {doc['file_id']}, å¤§å°: {doc['file_size']} bytes)" for doc in docs_info])
            
            # åœ¨ç”¨æˆ·æ¶ˆæ¯å‰æ’å…¥æ–‡æ¡£å…ƒä¿¡æ¯ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯
            doc_message = {
                "type": "system",
                "content": f"""ç”¨æˆ·ä¸Šä¼ äº†ä»¥ä¸‹æ–‡æ¡£ä¾›å‚è€ƒ{files_summary}"""
            }
            graph_input["messages"].insert(0, doc_message)
            logger.info(f"å·²æ·»åŠ æ–‡æ¡£å…ƒä¿¡æ¯ï¼Œå…± {len(docs_info)} ä¸ªæ–‡æ¡£")
            
            # ä¿å­˜ä¼šè¯å’Œæ–‡æ¡£çš„å…³è”
            agent_id = config.get("configurable", {}).get("agent_id", "diagnostic_agent")
            user_name = config.get("configurable", {}).get("user_name", "system")
            await save_thread_file_associations(thread_id, request_body.file_ids, agent_id, user_name)
    
    return graph_input


async def process_stream_chunk(chunk, event_id):
    """å¤„ç†å•ä¸ªæµå¼æ•°æ®å—"""    
    # Handle tuple format from LangGraph streaming
    if isinstance(chunk, tuple) and len(chunk) >= 2:
        if len(chunk) == 2:
            # æ ‡å‡†æ ¼å¼: (event_type, data)
            event_type, data = chunk
        elif len(chunk) == 3:
            # å­å›¾æ ¼å¼: (namespace, event_type, data)
            namespace, event_type, data = chunk
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•è·å–æœ€åä¸¤ä¸ªå…ƒç´ 
            logger.error(f"æœªçŸ¥çš„chunkæ ¼å¼ï¼Œé•¿åº¦={len(chunk)}")
            raise BadRequestException("æœªçŸ¥çš„chunkæ ¼å¼")
        
        serialized_data = serialize_value(data)
        
        # Check for interrupts
        has_interrupt = False
        if event_type == "updates" and isinstance(data, dict) and "__interrupt__" in data:
            logger.info(f"Interrupt detected: {data}")
            interrupt_data = data["__interrupt__"]
            
            # æ£€æŸ¥ interrupt_data æ˜¯å¦ä¸ºç©º
            if not interrupt_data or len(interrupt_data) == 0:
                logger.warning(f"æ£€æµ‹åˆ°ç©ºçš„ä¸­æ–­æ•°æ®: {interrupt_data}")
            
            has_interrupt = True
        
        return f"id: {event_id}\nevent: {event_type}\ndata: {json.dumps(serialized_data, ensure_ascii=False)}\n\n", has_interrupt
    else:
        # Handle dict format (fallback)
        serializable_chunk = {}
        for key, value in chunk.items(): serializable_chunk[key] = serialize_value(value)
        event_type = list(serializable_chunk.keys())[0] if serializable_chunk else "data"
        return f"id: {event_id}\nevent: {event_type}\ndata: {json.dumps(serializable_chunk[event_type], ensure_ascii=False)}\n\n", False


async def execute_graph_request(request_body: RunCreate, thread_id: str, request=None, is_streaming: bool = True):
    """æ‰§è¡Œå›¾è¯·æ±‚çš„é€šç”¨å‡½æ•° - æ”¯æŒæµå¼å’Œéæµå¼"""
    # å‡†å¤‡è¿è¡Œï¼ˆæ›´æ–°ç»Ÿè®¡ã€ç¡®ä¿çº¿ç¨‹æ˜ å°„ï¼‰
    await prepare_run(thread_id, request_body, request)
    
    # åˆ›å»ºè¿è¡Œæ—¥å¿—
    run_log_id = None
    try:
        from .run_log_service import run_log_service
        from src.shared.db.config import get_async_db_context
        
        # è·å–å®¢æˆ·ç«¯ä¿¡æ¯
        ip_address = None
        user_agent = None
        if request:
            ip_address = request.client.host if hasattr(request, 'client') else None
            user_agent = request.headers.get('user-agent', None)
        
        async with get_async_db_context() as db:
            run_log = await run_log_service.create_run_log(
                db=db,
                agent_id=request_body.agent_id,
                thread_id=thread_id,
                user_name=request_body.user_name,
                ip_address=ip_address,
                user_agent=user_agent
            )
            run_log_id = run_log.id
            logger.info(f"åˆ›å»ºè¿è¡Œæ—¥å¿—: {run_log_id}")
    except Exception as e:
        # æ—¥å¿—åˆ›å»ºå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        logger.error(f"åˆ›å»ºè¿è¡Œæ—¥å¿—å¤±è´¥: {e}", exc_info=True)
    
    # å‡†å¤‡é…ç½®ï¼ˆè¿”å› config, stream_modeï¼‰
    config, stream_modes = prepare_config(request_body, thread_id)
    
    # å‡†å¤‡è¾“å…¥ï¼ˆåŒ…æ‹¬å¤„ç†æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼‰
    graph_input = await prepare_graph_input(request_body, config, thread_id)
    
    # åˆ›å»ºå›¾
    graph = await AgentRegistry.create_agent(request_body.agent_id, config)
    logger.info(f"[Agentåˆ›å»º] åŠ¨æ€åˆ›å»ºæ™ºèƒ½ä½“graph: {graph}")
    
    if is_streaming:
        # æµå¼å¤„ç†
        logger.info(f"Starting stream with modes: {stream_modes}")
        
        event_id = 0
        has_interrupt = False
        collected_messages = []  # æ”¶é›†æ‰€æœ‰æ¶ˆæ¯ç”¨äºè®¡ç®—token
        
        # è·å–LLMé…ç½®ä»¥è®¡ç®—tokené™åˆ¶
        llm_config = None
        try:
            from ..llm_agents.agent_utils import get_llm_config_from_db
            llm_config = await get_llm_config_from_db(request_body.agent_id)
        except Exception as e:
            logger.warning(f"è·å–LLMé…ç½®å¤±è´¥: {e}")
        
        # åˆ›å»ºtokenä½¿ç”¨ç›‘æ§å™¨
        token_usage_hook = create_token_usage_hook(llm_config)
        
        async for chunk in graph.astream(graph_input, config=config, stream_mode=stream_modes, subgraphs=True):
            try:
                event_id += 1
                sse_data, chunk_has_interrupt = await process_stream_chunk(chunk, event_id)
                yield sse_data
                if chunk_has_interrupt: has_interrupt = True
                
                # æ”¶é›†æ¶ˆæ¯ç”¨äºtokenç»Ÿè®¡
                if isinstance(chunk, tuple) and len(chunk) >= 2:
                    event_type, data = chunk[-2:]  # è·å–æœ€åä¸¤ä¸ªå…ƒç´ 
                    if event_type == "values" and isinstance(data, dict) and "messages" in data:
                        collected_messages = data["messages"]
                
            except Exception as e:
                logger.error(f"Serialization error: {e}, chunk type: {type(chunk)}, chunk: {chunk}", exc_info=True)
                event_id += 1
                yield f"id: {event_id}\nevent: error\ndata: {json.dumps({'error': str(e), 'chunk_type': str(type(chunk)), 'chunk': str(chunk)}, ensure_ascii=False)}\n\n"
        
        # åœ¨æµç»“æŸæ—¶å‘é€tokenä½¿ç”¨æƒ…å†µ
        if collected_messages:
            total_tokens = token_usage_hook.count_messages_tokens(collected_messages)
            max_tokens = token_usage_hook.max_context_length
            usage_ratio = total_tokens / max_tokens
            
            event_id += 1
            token_usage_event = {
                "thread_id": thread_id,
                "token_usage": {
                    "used": total_tokens,
                    "total": max_tokens,
                    "percentage": round(usage_ratio * 100, 1),
                    "remaining": max_tokens - total_tokens
                }
            }
            yield f"id: {event_id}\nevent: token_usage\ndata: {json.dumps(token_usage_event, ensure_ascii=False)}\n\n"
            logger.info(f"ğŸ“Š å‘é€tokenä½¿ç”¨æƒ…å†µ: {total_tokens}/{max_tokens} ({usage_ratio*100:.1f}%)")
        
        # End event - only send if no interrupt occurred
        if not has_interrupt:
            event_id += 1
            yield f"id: {event_id}\nevent: end\ndata: {json.dumps({'status': 'completed'}, ensure_ascii=False)}\n\n"
        else:
            logger.info("Skipping end event due to interrupt - waiting for user approval")
    else:
        # éæµå¼å¤„ç†
        result = await graph.ainvoke(graph_input, config=config, stream_mode=stream_modes)
        
        # å¤„ç†ç»“æœ
        final_response = {
            "thread_id": thread_id,
            "status": "completed",
            "result": result
        }
        
        # å¦‚æœç»“æœä¸­æœ‰messagesï¼Œæå–æœ€åä¸€æ¡AIæ¶ˆæ¯
        if isinstance(result, dict) and "messages" in result:
            messages = result["messages"]
            # æ‰¾åˆ°æœ€åä¸€æ¡AIæ¶ˆæ¯
            for message in reversed(messages):
                # æ£€æŸ¥ role æˆ– type å­—æ®µ
                is_ai_message = False
                if hasattr(message, "role") and message.role == "assistant":
                    is_ai_message = True
                elif hasattr(message, "type") and message.type == "ai":
                    is_ai_message = True
                
                if is_ai_message:
                    final_response["last_message"] = {
                        "content": getattr(message, "content", str(message)),
                        "type": "ai"  # ç»Ÿä¸€ä½¿ç”¨ typeï¼Œä¸ LangGraph æ¶ˆæ¯æ ¼å¼ä¿æŒä¸€è‡´
                    }
                    break
        
        yield final_response


async def stream_run_standard(thread_id: str, request_body: RunCreate, request=None):
    """Standard LangGraph streaming endpoint - æ”¯æŒåŠ¨æ€æ™ºèƒ½ä½“æ£€æŸ¥"""
    # æš‚æ—¶ç¦ç”¨å¿ƒè·³ä»¥é¿å…MCP CloseResourceError
    # TODO: éœ€è¦æ›´å¥½çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†å¿ƒè·³ä¸MCPå·¥å…·çš„å…¼å®¹æ€§
    async def generate():
        """æ¢å¤åŸå§‹çš„æµå¼å¤„ç†ï¼ˆæš‚æ—¶ç¦ç”¨å¿ƒè·³ï¼‰"""
        try:
            # ä½¿ç”¨é€šç”¨æ‰§è¡Œå‡½æ•°ï¼Œæµå¼æ¨¡å¼
            async for item in execute_graph_request(request_body, thread_id, request, is_streaming=True):
                yield item
            
            # æ›´æ–°è¿è¡Œæ—¥å¿—ä¸ºæˆåŠŸ
            try:
                from .run_log_service import run_log_service
                from src.shared.db.config import get_async_db_context
                from src.shared.db.models import now_shanghai
                
                async with get_async_db_context() as db:
                    await run_log_service.update_run_log(
                        db=db,
                        thread_id=thread_id,
                        run_status='success',
                        end_time=now_shanghai()
                    )
            except Exception as log_e:
                logger.error(f"æ›´æ–°è¿è¡Œæ—¥å¿—å¤±è´¥: {log_e}")
                
        except Exception as e:
            logger.error(f"æµå¼å¤„ç†å¼‚å¸¸: {e}", exc_info=True)
            
            # æ›´æ–°è¿è¡Œæ—¥å¿—ä¸ºå¤±è´¥
            try:
                from .run_log_service import run_log_service
                from src.shared.db.config import get_async_db_context
                from src.shared.db.models import now_shanghai
                
                async with get_async_db_context() as db:
                    await run_log_service.update_run_log(
                        db=db,
                        thread_id=thread_id,
                        run_status='failed',
                        end_time=now_shanghai(),
                        error_message=str(e)
                    )
            except Exception as log_e:
                logger.error(f"æ›´æ–°è¿è¡Œæ—¥å¿—å¤±è´¥: {log_e}")
            
            yield f"event: error\n"
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Content-Type": "text/event-stream"
        }
    )


async def invoke_run_standard(thread_id: str, request_body: RunCreate, request=None):
    """Standard LangGraph non-streaming endpoint - éæµå¼è°ƒç”¨"""    
    try:
        # ä½¿ç”¨é€šç”¨æ‰§è¡Œå‡½æ•°ï¼Œéæµå¼æ¨¡å¼
        async for final_response in execute_graph_request(request_body, thread_id, request, is_streaming=False):
            # éæµå¼æ¨¡å¼åªä¼šyieldä¸€æ¬¡ç»“æœ
            # æ›´æ–°è¿è¡Œæ—¥å¿—ä¸ºæˆåŠŸ
            try:
                from .run_log_service import run_log_service
                from src.shared.db.config import get_async_db_context
                from src.shared.db.models import now_shanghai
                
                async with get_async_db_context() as db:
                    await run_log_service.update_run_log(
                        db=db,
                        thread_id=thread_id,
                        run_status='success',
                        end_time=now_shanghai()
                    )
            except Exception as log_e:
                logger.error(f"æ›´æ–°è¿è¡Œæ—¥å¿—å¤±è´¥: {log_e}")
                
            return success_response(final_response)
    except Exception as e:
        logger.error(f"éæµå¼è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
        
        # æ›´æ–°è¿è¡Œæ—¥å¿—ä¸ºå¤±è´¥
        try:
            from .run_log_service import run_log_service
            from src.shared.db.config import get_async_db_context
            from src.shared.db.models import now_shanghai
            
            async with get_async_db_context() as db:
                await run_log_service.update_run_log(
                    db=db,
                    thread_id=thread_id,
                    run_status='failed',
                    end_time=now_shanghai(),
                    error_message=str(e)
                )
        except Exception as log_e:
            logger.error(f"æ›´æ–°è¿è¡Œæ—¥å¿—å¤±è´¥: {log_e}")
            
        raise BusinessException(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}", ResponseCode.INTERNAL_ERROR)


async def completion_handler(thread_id: str, request_body: RunCreate, request=None):
    """ç»Ÿä¸€çš„è¡¥å…¨å¤„ç†å‡½æ•° - æ”¯æŒæµå¼å’Œéæµå¼"""
    if request_body.chat_mode == "streaming":
        # æµå¼å¤„ç†
        return await stream_run_standard(thread_id, request_body, request)
    elif request_body.chat_mode == "blocking":
        # éæµå¼å¤„ç†
        return await invoke_run_standard(thread_id, request_body, request)
    else:
        raise BusinessException(f"chat_modeå¿…é¡»æ˜¯ 'streaming' æˆ– 'blocking'", ResponseCode.BAD_REQUEST)


async def save_thread_file_associations(thread_id: str, file_ids: List[str], agent_id: str, user_name: str) -> None:
    """
    ä¿å­˜ä¼šè¯å’Œæ–‡æ¡£çš„å…³è”å…³ç³»
    
    Args:
        thread_id: ä¼šè¯çº¿ç¨‹ID
        file_ids: æ–‡ä»¶IDåˆ—è¡¨
        agent_id: æ™ºèƒ½ä½“ID
        user_name: ç”¨æˆ·å
    """    
    try:
        async with get_async_db_context() as db:
            for file_id in file_ids:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å…³è”
                result = await db.execute(
                    select(AgentDocumentSession).where(
                        AgentDocumentSession.thread_id == thread_id,
                        AgentDocumentSession.file_id == file_id
                    )
                )
                existing = result.scalar_one_or_none()
                
                if not existing:
                    # åˆ›å»ºæ–°çš„å…³è”
                    session = AgentDocumentSession(
                        thread_id=thread_id,
                        file_id=file_id,
                        agent_id=agent_id,
                        create_by=user_name
                    )
                    db.add(session)
            
            await db.commit()
            logger.info(f"âœ… ä¿å­˜ä¼šè¯æ–‡æ¡£å…³è”æˆåŠŸ: thread_id={thread_id}, files={file_ids}")
    except Exception as e:
        logger.error(f"ä¿å­˜ä¼šè¯æ–‡æ¡£å…³è”å¤±è´¥: {e}", exc_info=True)
        # ä¸å½±å“ä¸»æµç¨‹


async def get_thread_file_ids(thread_id: str) -> List[str]:
    """
    è·å–ä¼šè¯å…³è”çš„æ–‡ä»¶IDåˆ—è¡¨
    
    Args:
        thread_id: ä¼šè¯çº¿ç¨‹ID
        
    Returns:
        æ–‡ä»¶IDåˆ—è¡¨
    """
    
    
    try:
        async with get_async_db_context() as db:
            result = await db.execute(
                select(AgentDocumentSession.file_id)
                .where(AgentDocumentSession.thread_id == thread_id)
                .order_by(AgentDocumentSession.create_time)
            )
            
            file_ids = [row[0] for row in result.fetchall()]
            logger.info(f"âœ… è·å–ä¼šè¯æ–‡æ¡£å…³è”æˆåŠŸ: thread_id={thread_id}, files={file_ids}")
            return file_ids
    except Exception as e:
        logger.error(f"è·å–ä¼šè¯æ–‡æ¡£å…³è”å¤±è´¥: {e}", exc_info=True)
        return []