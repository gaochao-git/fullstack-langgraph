"""
Agent è¿è¡Œå¤„ç†æ¨¡å—
å¤„ç† LangGraph Agent çš„æµå¼å’Œéæµå¼è¿è¡Œè¯·æ±‚
"""
import json
from typing import Dict, Any, List
from src.shared.core.logging import get_logger
from src.shared.core.exceptions import BusinessException
from src.shared.schemas.response import ResponseCode, success_response
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
from sqlalchemy import select
from src.shared.db.config import get_async_db_context
from ..checkpoint_factory import create_checkpointer
from .document_service import document_service
from src.shared.db.config import get_sync_db
from ..utils import (prepare_graph_config, serialize_value)
from .user_threads_db import (check_user_thread_exists,create_user_thread_mapping)
from ..llm_agents.agent_registry import AgentRegistry
from .agent_config_service import AgentConfigService
from .agent_service import agent_service
from ..models import AgentDocumentSession
logger = get_logger(__name__)

# å®šä¹‰è¿è¡Œè¯·æ±‚ä½“
class RunCreate(BaseModel):
    assistant_id: Optional[str] = None  # å‰ç«¯è°ƒç”¨æ—¶ä¼ é€’ï¼ŒAPIè°ƒç”¨æ—¶å¯ä»è®¤è¯ä¿¡æ¯è·å–
    input: Optional[Dict[str, Any]] = None
    config: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None
    stream_mode: Optional[List[str]] = ["values"]
    interrupt_before: Optional[List[str]] = None
    interrupt_after: Optional[List[str]] = None
    on_disconnect: Optional[str] = None
    command: Optional[Dict[str, Any]] = None
    checkpoint: Optional[Dict[str, Any]] = None
    user_name: Optional[str] = None  # ç”¨æˆ·åï¼Œç”¨äºçº¿ç¨‹å…³è”
    file_ids: Optional[List[str]] = None  # å…³è”çš„æ–‡ä»¶IDåˆ—è¡¨
    

async def validate_and_prepare_run(thread_id: str, request_body: RunCreate, request=None) -> tuple[str, dict, str]:
    """éªŒè¯å’Œå‡†å¤‡è¿è¡Œå‚æ•° - å…¬å…±æ–¹æ³•ï¼Œæå–stream_run_standardå’Œinvoke_run_standardçš„é‡å¤é€»è¾‘"""
    
    # è·å–è®¤è¯ä¿¡æ¯
    current_user = None
    auth_type = None
    if request and hasattr(request.state, 'current_user'):
        current_user = request.state.current_user
        auth_type = request.state.auth_type
    
    # è·å– agent_idï¼Œä¼˜å…ˆä»è¯·æ±‚ä½“è·å–ï¼Œå…¶æ¬¡ä»è®¤è¯ä¿¡æ¯è·å–
    agent_id = request_body.assistant_id
    
    # å¦‚æœæ˜¯ agent_key è®¤è¯ä¸”æ²¡æœ‰æä¾› assistant_idï¼Œä»è®¤è¯ä¿¡æ¯ä¸­è·å–
    if not agent_id and current_user and auth_type == 'agent_key':
        agent_id = current_user.get('agent_id')
        logger.info(f"ä»agent_keyè®¤è¯ä¿¡æ¯ä¸­è·å–åˆ°agent_id: {agent_id}")
    
    if not agent_id: raise BusinessException("å¿…é¡»æä¾›æ™ºèƒ½ä½“ID", ResponseCode.BAD_REQUEST)
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¯¥æ™ºèƒ½ä½“
    db_gen = get_sync_db()
    db = next(db_gen)
    try:
        agent_config = AgentConfigService.get_agent_config(agent_id, db)
        if not agent_config: raise BusinessException(f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}", ResponseCode.NOT_FOUND)
        
        # å¦‚æœæ˜¯agent_keyè®¤è¯ï¼ŒéªŒè¯agent_idæ˜¯å¦åŒ¹é…ï¼ˆåªåœ¨è¯·æ±‚ä½“ä¸­æä¾›äº†agent_idæ—¶éªŒè¯ï¼‰
        if auth_type == "agent_key" and request_body.assistant_id:
            agent_id_from_auth = current_user.get('agent_id')
            if agent_id_from_auth and agent_id_from_auth != agent_id:
                logger.warning(f"Agent ID mismatch: {agent_id_from_auth} != {agent_id}")
                raise BusinessException("æ™ºèƒ½ä½“IDä¸åŒ¹é…", ResponseCode.FORBIDDEN)
            logger.info(f"æ™ºèƒ½ä½“ {agent_id} agent_keyè®¤è¯éªŒè¯æˆåŠŸ")
        elif auth_type == "jwt":
            logger.info(f"æ™ºèƒ½ä½“ {agent_id} JWTè®¤è¯éªŒè¯æˆåŠŸ")
        elif auth_type:
            logger.info(f"æ™ºèƒ½ä½“ {agent_id} ä½¿ç”¨ {auth_type} è®¤è¯")
        else:
            logger.warning(f"æ™ºèƒ½ä½“ {agent_id} æœªé€šè¿‡è®¤è¯ä¸­é—´ä»¶")
            
    finally:
        db.close()
    
    # æ›´æ–°æ™ºèƒ½ä½“ä½¿ç”¨ç»Ÿè®¡
    try:
        async with get_async_db_context() as async_db:
            await agent_service.increment_run_count(async_db, agent_id)
            logger.info(f"âœ… å·²æ›´æ–°æ™ºèƒ½ä½“ {agent_id} çš„ä½¿ç”¨ç»Ÿè®¡")
    except Exception as e:
        logger.error(f"æ›´æ–°æ™ºèƒ½ä½“ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
        # ç»Ÿè®¡æ›´æ–°å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
    
    # è·å–ç”¨æˆ·åï¼Œä¼˜å…ˆä»è®¤è¯ä¿¡æ¯ä¸­è·å–
    user_name = None
    
    # 1. é¦–å…ˆå°è¯•ä»è®¤è¯ä¿¡æ¯ä¸­è·å–ï¼ˆagent_keyè®¤è¯ä¼šè‡ªåŠ¨å¸¦ç”¨æˆ·åï¼‰
    if current_user:
        user_name = current_user.get('username')
        logger.info(f"ä»è®¤è¯ä¿¡æ¯ä¸­è·å–åˆ°ç”¨æˆ·å: {user_name}")
    
    # 2. å¦‚æœè®¤è¯ä¿¡æ¯ä¸­æ²¡æœ‰ï¼Œå†ä»è¯·æ±‚å‚æ•°ä¸­è·å–
    if not user_name and request_body.config and request_body.config.get("configurable"):
        user_name = request_body.config["configurable"].get("user_name")
        logger.info(f"ä»è¯·æ±‚å‚æ•°ä¸­è·å–åˆ°ç”¨æˆ·å: {user_name}")
    
    # ç”¨æˆ·åæ˜¯å¿…é¡»çš„
    if not user_name:
        raise BusinessException("æ— æ³•è·å–ç”¨æˆ·å", ResponseCode.BAD_REQUEST)
    
    if user_name:
        logger.info(f"ğŸ” å¼€å§‹å¤„ç†ç”¨æˆ·çº¿ç¨‹å…³è”: {user_name} -> {thread_id}")
        try:
            await ensure_user_thread_mapping(user_name, thread_id, request_body)
        except Exception as e:
            logger.error(f"å¤„ç†ç”¨æˆ·çº¿ç¨‹å…³è”æ—¶å‡ºé”™: {e}", exc_info=True)
            # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
    else:
        logger.warning(f"âš ï¸ è¯·æ±‚ä¸­æ²¡æœ‰æä¾›ç”¨æˆ·åï¼Œè·³è¿‡ç”¨æˆ·çº¿ç¨‹å…³è”åˆ›å»º")
    
    return agent_id, agent_config, user_name


async def ensure_user_thread_mapping(user_name, thread_id, request_body):
    """
    ç¡®ä¿ç”¨æˆ·å’Œçº¿ç¨‹çš„å½’å±å·²å†™å…¥user_threadsè¡¨ï¼Œå¦‚ä¸å­˜åœ¨åˆ™è‡ªåŠ¨å†™å…¥ã€‚
    è‡ªåŠ¨æå–thread_titleï¼ˆå–æ¶ˆæ¯å†…å®¹å‰20å­—ï¼‰ã€‚
    """
    logger.info(f"[ensure_user_thread_mapping] called with user_name={user_name}, thread_id={thread_id}")
    exists = await check_user_thread_exists(user_name, thread_id)
    logger.info(f"[ensure_user_thread_mapping] exists={exists}")
    if not exists:
        thread_title = None
        if hasattr(request_body, 'input') and request_body.input and "messages" in request_body.input:
            messages = request_body.input["messages"]
            if messages and len(messages) > 0:
                last_msg = messages[-1]
                if isinstance(last_msg, dict) and "content" in last_msg:
                    content = str(last_msg["content"])
                    thread_title = content[:20] + "..." if len(content) > 20 else content
        
        # ä»request_bodyä¸­è·å–assistant_idï¼Œå†…éƒ¨ä½œä¸ºagent_idä½¿ç”¨
        agent_id = getattr(request_body, 'assistant_id', None)
        
        logger.info(f"[ensure_user_thread_mapping] creating mapping: user_name={user_name}, thread_id={thread_id}, thread_title={thread_title}, agent_id={agent_id}")
        await create_user_thread_mapping(user_name, thread_id, thread_title, agent_id)

async def process_stream_chunk(chunk, event_id, thread_id):
    """å¤„ç†å•ä¸ªæµå¼æ•°æ®å—"""    
    # Handle tuple format from LangGraph streaming
    if isinstance(chunk, tuple) and len(chunk) >= 2:
        if len(chunk) == 2:
            # æ ‡å‡†æ ¼å¼: (event_type, data)
            event_type, data = chunk
        elif len(chunk) == 3:
            # å­å›¾æ ¼å¼: (namespace, event_type, data)
            namespace, event_type, data = chunk
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•è·å–æœ€åä¸¤ä¸ªå…ƒç´ 
            event_type, data = chunk[-2:]
            logger.warning(f"âš ï¸ æœªçŸ¥çš„chunkæ ¼å¼ï¼Œé•¿åº¦={len(chunk)}, å°è¯•ä½¿ç”¨åä¸¤ä¸ªå…ƒç´ ")
        
        serialized_data = serialize_value(data)
        
        # Save messages to thread history from LangGraph state
        if event_type == "values" and isinstance(data, dict) and "messages" in data:
            # ä¸å†æ“ä½œthread_messages
            pass
        
        # Also save messages from updates events (when nodes return message updates)
        elif event_type == "updates" and isinstance(data, dict):
            for node_name, node_data in data.items():
                if isinstance(node_data, dict) and "messages" in node_data:
                    # ä¸å†æ“ä½œthread_messages
                    pass
                break  # Only process the first node with messages
        
        # Check for interrupts
        has_interrupt = False
        if event_type == "updates" and isinstance(data, dict) and "__interrupt__" in data:
            logger.info(f"Interrupt detected: {data}")
            interrupt_data = data["__interrupt__"]
            
            # æ£€æŸ¥ interrupt_data æ˜¯å¦ä¸ºç©º
            if interrupt_data and len(interrupt_data) > 0:
                # ä¸å†æ“ä½œthread_interrupts
                pass
            else:
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ç©ºçš„ä¸­æ–­æ•°æ®: {interrupt_data}")
                
                # å¤„ç†ç©ºçš„ä¸­æ–­æ•°æ®ï¼šåˆ›å»ºå·¥å…·å®¡æ‰¹è¯·æ±‚
                # è¿™é€šå¸¸å‘ç”Ÿåœ¨ create_react_agent ä½¿ç”¨ interrupt_before=["tools"] æ—¶
                # ä¸å†æ“ä½œthread_interrupts
                pass
            
            has_interrupt = True
        
        return f"id: {event_id}\nevent: {event_type}\ndata: {json.dumps(serialized_data, ensure_ascii=False)}\n\n", has_interrupt
    else:
        # Handle dict format (fallback)
        serializable_chunk = {}
        for key, value in chunk.items():
            serializable_chunk[key] = serialize_value(value)
        
        event_type = list(serializable_chunk.keys())[0] if serializable_chunk else "data"
        return f"id: {event_id}\nevent: {event_type}\ndata: {json.dumps(serializable_chunk[event_type], ensure_ascii=False)}\n\n", False

async def stream_with_graph_postgres(graph, request_body, thread_id):
    """PostgreSQLæ¨¡å¼ä¸“ç”¨çš„å›¾æµåª’ä½“å¤„ç†å‡½æ•°"""
    config, graph_input, stream_modes, checkpoint = prepare_graph_config(request_body, thread_id)
    
    # ä»æ¶ˆæ¯ä¸­è·å– file_ids
    file_ids = None
    
    if graph_input and "messages" in graph_input:
        messages = graph_input["messages"]
        if messages and len(messages) > 0:
            last_msg = messages[-1]
            if isinstance(last_msg, dict):
                # ä»æ¶ˆæ¯æœ¬èº«è·å– file_ids
                file_ids = last_msg.get("file_ids")
    
    # å¦‚æœæœ‰å…³è”çš„æ–‡æ¡£ï¼Œå°†æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°æ¶ˆæ¯ä¸Šä¸‹æ–‡ä¸­
    if file_ids:
        logger.info(f"æ£€æµ‹åˆ°å…³è”æ–‡æ¡£: {file_ids}, æ–‡æ¡£æ•°é‡: {len(file_ids) if isinstance(file_ids, list) else 'N/A'}")
        
        
        # è·å–æ–‡æ¡£ä¸Šä¸‹æ–‡
        doc_context = document_service.get_document_context(file_ids)
        if doc_context:
            # åœ¨ç”¨æˆ·æ¶ˆæ¯å‰æ’å…¥æ–‡æ¡£ä¸Šä¸‹æ–‡ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯
            doc_message = {
                "type": "system",
                "content": f"è¯·å‚è€ƒä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n{doc_context}"
            }
            graph_input["messages"].insert(0, doc_message)
            logger.info(f"å·²æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(doc_context)} å­—ç¬¦")
            
            # ä¿å­˜ä¼šè¯å’Œæ–‡æ¡£çš„å…³è”
            agent_id = config.get("configurable", {}).get("agent_id", "diagnostic_agent")
            user_name = config.get("configurable", {}).get("user_name", "system")
            await save_thread_file_associations(thread_id, file_ids, agent_id, user_name)
    
    
    logger.info(f"Starting stream with modes: {stream_modes}, checkpoint: {checkpoint}")
    
    event_id = 0
    has_interrupt = False
    
    async for chunk in graph.astream(graph_input, config=config, stream_mode=stream_modes, subgraphs=True):
        try:
            event_id += 1
            sse_data, chunk_has_interrupt = await process_stream_chunk(chunk, event_id, thread_id)
            yield sse_data
            if chunk_has_interrupt:
                has_interrupt = True
        except Exception as e:
            logger.error(f"Serialization error: {e}, chunk type: {type(chunk)}, chunk: {chunk}", exc_info=True)
            event_id += 1
            yield f"id: {event_id}\nevent: error\ndata: {json.dumps({'error': str(e), 'chunk_type': str(type(chunk)), 'chunk': str(chunk)}, ensure_ascii=False)}\n\n"
    
    # End event - only send if no interrupt occurred
    if not has_interrupt:
        event_id += 1
        yield f"id: {event_id}\nevent: end\ndata: {json.dumps({'status': 'completed'}, ensure_ascii=False)}\n\n"
    else:
        logger.info("Skipping end event due to interrupt - waiting for user approval")

async def handle_chat_streaming(request_body, thread_id):
    """å¤„ç†PostgreSQLæ¨¡å¼çš„æµå¼å“åº” - å®Œå…¨åŸºäºæ•°æ®åº“é…ç½®"""    
    # ä» assistant_id è·å–å†…éƒ¨ä½¿ç”¨çš„ agent_id
    agent_id = request_body.assistant_id    
    # æŒ‰ç…§å®˜æ–¹æ¨¡å¼ï¼šåœ¨async withå†…å®Œæˆæ•´ä¸ªè¯·æ±‚å‘¨æœŸ
    async with create_checkpointer() as checkpointer:
        # å°†agent_idæ·»åŠ åˆ°configä¸­
        if not request_body.config: request_body.config = {}
        if not request_body.config.get("configurable"): request_body.config["configurable"] = {}
        request_body.config["configurable"]["agent_id"] = agent_id
        
        # ä½¿ç”¨æ³¨å†Œä¸­å¿ƒåŠ¨æ€åˆ›å»º Agent
        graph = await AgentRegistry.create_agent(agent_id, request_body.config, checkpointer)
        logger.info(f"[Agentåˆ›å»º] åŠ¨æ€åˆ›å»ºæ™ºèƒ½ä½“graph: {graph}")
        
        # åœ¨åŒä¸€ä¸ªasync withå†…æ‰§è¡Œå®Œæ•´çš„æµå¼å¤„ç†
        async for item in stream_with_graph_postgres(graph, request_body, thread_id):
            yield item

async def stream_run_standard(thread_id: str, request_body: RunCreate, request=None):
    """Standard LangGraph streaming endpoint - æ”¯æŒåŠ¨æ€æ™ºèƒ½ä½“æ£€æŸ¥"""
    
    # ä½¿ç”¨å…¬å…±æ–¹æ³•éªŒè¯å’Œå‡†å¤‡è¿è¡Œå‚æ•°
    await validate_and_prepare_run(thread_id, request_body, request)

    async def generate():
        try:
            async for item in handle_chat_streaming(request_body, thread_id):
                yield item
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Error in streaming: {e}")
            logger.error(f"Full traceback: {error_details}")
            yield f"event: error\n"
            yield f"data: {json.dumps({'type': 'error', 'error': str(e), 'traceback': error_details}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Content-Type": "text/event-stream"
        }
    )


async def invoke_run_standard(thread_id: str, request_body: RunCreate, request=None):
    """Standard LangGraph non-streaming endpoint - éæµå¼è°ƒç”¨"""
    
    # ä½¿ç”¨å…¬å…±æ–¹æ³•éªŒè¯å’Œå‡†å¤‡è¿è¡Œå‚æ•°
    agent_id, agent_config, user_name = await validate_and_prepare_run(thread_id, request_body, request)
    
    # PostgreSQL æ¨¡å¼ä¸‹çš„éæµå¼å¤„ç†
    is_builtin = agent_config.get('is_builtin') == 'yes'
    return await handle_chat_invoke(thread_id, request_body, agent_id, is_builtin)


async def handle_chat_invoke(thread_id: str, request_body: RunCreate, agent_id: str, is_builtin: bool):
    """PostgreSQL æ¨¡å¼ä¸‹çš„éæµå¼å¤„ç†"""
    async with create_checkpointer() as checkpointer:
        # ä¸å†è°ƒç”¨ setup()ï¼Œè¡¨ç»“æ„å·²åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»º
        
        # å‡†å¤‡é…ç½®å’Œè¾“å…¥
        config, graph_input, _, checkpoint = prepare_graph_config(request_body, thread_id)
        
        # åœ¨é…ç½®ä¸­æ·»åŠ  agent_id
        config["configurable"]["agent_id"] = agent_id
        
        # ä½¿ç”¨æ³¨å†Œä¸­å¿ƒåŠ¨æ€åˆ›å»º Agent
        graph = await AgentRegistry.create_agent(agent_id, config, checkpointer)
        
        # ä»æ¶ˆæ¯ä¸­è·å– file_idsï¼ˆå¦‚æœæœ‰ï¼‰
        file_ids = None
        if graph_input and "messages" in graph_input:
            messages = graph_input["messages"]
            if messages and len(messages) > 0:
                last_msg = messages[-1]
                if isinstance(last_msg, dict):
                    # ä»æ¶ˆæ¯æœ¬èº«è·å– file_ids
                    file_ids = last_msg.get("file_ids")
        
        # å¦‚æœæœ‰æ–‡æ¡£ï¼Œæ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡
        if file_ids:
            logger.info(f"éæµå¼è°ƒç”¨æ£€æµ‹åˆ°å…³è”æ–‡æ¡£: {file_ids}, æ–‡æ¡£æ•°é‡: {len(file_ids) if isinstance(file_ids, list) else 'N/A'}")
            # è·å–æ–‡æ¡£ä¸Šä¸‹æ–‡
            doc_context = document_service.get_document_context(file_ids)
            if doc_context:
                # åœ¨ç”¨æˆ·æ¶ˆæ¯å‰æ’å…¥æ–‡æ¡£ä¸Šä¸‹æ–‡ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯
                doc_message = {
                    "type": "system",
                    "content": f"è¯·å‚è€ƒä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n{doc_context}"
                }
                graph_input["messages"].insert(0, doc_message)
                logger.info(f"å·²æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(doc_context)} å­—ç¬¦")
        
        # user_name ä» config.configurable è·å–
        user_name = None
        if request_body.config and request_body.config.get("configurable"):
            user_name = request_body.config["configurable"].get("user_name")
        
        if file_ids and user_name:
            await save_thread_file_associations(thread_id, file_ids, agent_id, user_name)
        
        # éæµå¼è°ƒç”¨
        try:
            result = await graph.ainvoke(graph_input, config=config)
            
            # å¤„ç†ç»“æœ
            final_response = {
                "thread_id": thread_id,
                "status": "completed",
                "result": result
            }
            
            # å¦‚æœç»“æœä¸­æœ‰messagesï¼Œæå–æœ€åä¸€æ¡AIæ¶ˆæ¯
            if isinstance(result, dict) and "messages" in result:
                messages = result["messages"]
                # æ‰¾åˆ°æœ€åä¸€æ¡AIæ¶ˆæ¯
                for message in reversed(messages):
                    # æ£€æŸ¥ role æˆ– type å­—æ®µ
                    is_ai_message = False
                    if hasattr(message, "role") and message.role == "assistant":
                        is_ai_message = True
                    elif hasattr(message, "type") and message.type == "ai":
                        is_ai_message = True
                    
                    if is_ai_message:
                        final_response["last_message"] = {
                            "content": getattr(message, "content", str(message)),
                            "type": "ai"  # ç»Ÿä¸€ä½¿ç”¨ typeï¼Œä¸ LangGraph æ¶ˆæ¯æ ¼å¼ä¿æŒä¸€è‡´
                        }
                        break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­
            state = await checkpointer.aget(config)
            if state:
                # state å¯èƒ½æ˜¯ä¸€ä¸ªå­—å…¸æˆ–å¯¹è±¡
                next_nodes = state.get("next") if isinstance(state, dict) else getattr(state, "next", None)
                if next_nodes:
                    final_response["status"] = "interrupted"
                    final_response["interrupted_at"] = list(next_nodes)
            
            return success_response(final_response)
            
        except Exception as e:
            logger.error(f"éæµå¼è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            raise BusinessException(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}", ResponseCode.INTERNAL_ERROR)


async def save_thread_file_associations(thread_id: str, file_ids: List[str], agent_id: str, user_name: str) -> None:
    """
    ä¿å­˜ä¼šè¯å’Œæ–‡æ¡£çš„å…³è”å…³ç³»
    
    Args:
        thread_id: ä¼šè¯çº¿ç¨‹ID
        file_ids: æ–‡ä»¶IDåˆ—è¡¨
        agent_id: æ™ºèƒ½ä½“ID
        user_name: ç”¨æˆ·å
    """    
    try:
        async with get_async_db_context() as db:
            for file_id in file_ids:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å…³è”
                result = await db.execute(
                    select(AgentDocumentSession).where(
                        AgentDocumentSession.thread_id == thread_id,
                        AgentDocumentSession.file_id == file_id
                    )
                )
                existing = result.scalar_one_or_none()
                
                if not existing:
                    # åˆ›å»ºæ–°çš„å…³è”
                    session = AgentDocumentSession(
                        thread_id=thread_id,
                        file_id=file_id,
                        agent_id=agent_id,
                        create_by=user_name
                    )
                    db.add(session)
            
            await db.commit()
            logger.info(f"âœ… ä¿å­˜ä¼šè¯æ–‡æ¡£å…³è”æˆåŠŸ: thread_id={thread_id}, files={file_ids}")
    except Exception as e:
        logger.error(f"ä¿å­˜ä¼šè¯æ–‡æ¡£å…³è”å¤±è´¥: {e}", exc_info=True)
        # ä¸å½±å“ä¸»æµç¨‹


async def get_thread_file_ids(thread_id: str) -> List[str]:
    """
    è·å–ä¼šè¯å…³è”çš„æ–‡ä»¶IDåˆ—è¡¨
    
    Args:
        thread_id: ä¼šè¯çº¿ç¨‹ID
        
    Returns:
        æ–‡ä»¶IDåˆ—è¡¨
    """
    
    
    try:
        async with get_async_db_context() as db:
            result = await db.execute(
                select(AgentDocumentSession.file_id)
                .where(AgentDocumentSession.thread_id == thread_id)
                .order_by(AgentDocumentSession.create_time)
            )
            
            file_ids = [row[0] for row in result.fetchall()]
            logger.info(f"âœ… è·å–ä¼šè¯æ–‡æ¡£å…³è”æˆåŠŸ: thread_id={thread_id}, files={file_ids}")
            return file_ids
    except Exception as e:
        logger.error(f"è·å–ä¼šè¯æ–‡æ¡£å…³è”å¤±è´¥: {e}", exc_info=True)
        return []