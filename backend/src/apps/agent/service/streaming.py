"""
æµå¼å¤„ç†ç›¸å…³æ¥å£å’Œå‡½æ•°
"""
import json
from typing import Dict, Any, List
from src.shared.core.logging import get_logger
from src.shared.core.exceptions import BusinessException
from src.shared.schemas.response import ResponseCode
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from src.shared.db.config import get_async_db_context

from ..utils import (
    prepare_graph_config, 
    serialize_value,
    CHECK_POINT_URI,
    recover_thread_from_postgres
)
from .user_threads_db import (
    check_user_thread_exists,
    create_user_thread_mapping,
    init_user_threads_db
)

logger = get_logger(__name__)

# åˆ é™¤æ‰€æœ‰threads_storeã€thread_messagesã€thread_interruptsç›¸å…³å…¨å±€å˜é‡å’Œç›¸å…³æ“ä½œ
# æ™ºèƒ½ä½“é…ç½®å®Œå…¨åŸºäºæ•°æ®åº“ï¼Œæ— éœ€é™æ€å…¨å±€å˜é‡

def init_refs(ASSISTANTS_param):
    """ä¿ç•™å‡½æ•°ç­¾åä»¥å…¼å®¹ç°æœ‰è°ƒç”¨ï¼Œä½†å®é™…ä¸åšä»»ä½•æ“ä½œ"""
    pass

async def ensure_user_thread_mapping(user_name, thread_id, request_body):
    """
    ç¡®ä¿ç”¨æˆ·å’Œçº¿ç¨‹çš„å½’å±å·²å†™å…¥user_threadsè¡¨ï¼Œå¦‚ä¸å­˜åœ¨åˆ™è‡ªåŠ¨å†™å…¥ã€‚
    è‡ªåŠ¨æå–thread_titleï¼ˆå–æ¶ˆæ¯å†…å®¹å‰20å­—ï¼‰ã€‚
    """
    import asyncio
    from .user_threads_db import check_user_thread_exists, create_user_thread_mapping
    logger.info(f"[ensure_user_thread_mapping] called with user_name={user_name}, thread_id={thread_id}")
    exists = await check_user_thread_exists(user_name, thread_id)
    logger.info(f"[ensure_user_thread_mapping] exists={exists}")
    if not exists:
        thread_title = None
        if hasattr(request_body, 'input') and request_body.input and "messages" in request_body.input:
            messages = request_body.input["messages"]
            if messages and len(messages) > 0:
                last_msg = messages[-1]
                if isinstance(last_msg, dict) and "content" in last_msg:
                    content = str(last_msg["content"])
                    thread_title = content[:20] + "..." if len(content) > 20 else content
        logger.info(f"[ensure_user_thread_mapping] creating mapping: user_name={user_name}, thread_id={thread_id}, thread_title={thread_title}")
        await create_user_thread_mapping(user_name, thread_id, thread_title)

class RunCreate(BaseModel):
    assistant_id: str
    input: Optional[Dict[str, Any]] = None
    config: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None
    stream_mode: Optional[List[str]] = ["values"]
    interrupt_before: Optional[List[str]] = None
    interrupt_after: Optional[List[str]] = None
    on_disconnect: Optional[str] = None
    command: Optional[Dict[str, Any]] = None
    checkpoint: Optional[Dict[str, Any]] = None
    user_name: Optional[str] = None  # ç”¨æˆ·åï¼Œç”¨äºçº¿ç¨‹å…³è”
    file_ids: Optional[List[str]] = None  # å…³è”çš„æ–‡ä»¶IDåˆ—è¡¨

async def process_stream_chunk(chunk, event_id, thread_id):
    """å¤„ç†å•ä¸ªæµå¼æ•°æ®å—"""    
    # Handle tuple format from LangGraph streaming
    if isinstance(chunk, tuple) and len(chunk) >= 2:
        if len(chunk) == 2:
            # æ ‡å‡†æ ¼å¼: (event_type, data)
            event_type, data = chunk
        elif len(chunk) == 3:
            # å­å›¾æ ¼å¼: (namespace, event_type, data)
            namespace, event_type, data = chunk
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•è·å–æœ€åä¸¤ä¸ªå…ƒç´ 
            event_type, data = chunk[-2:]
            logger.warning(f"âš ï¸ æœªçŸ¥çš„chunkæ ¼å¼ï¼Œé•¿åº¦={len(chunk)}, å°è¯•ä½¿ç”¨åä¸¤ä¸ªå…ƒç´ ")
        
        serialized_data = serialize_value(data)
        
        # Save messages to thread history from LangGraph state
        if event_type == "values" and isinstance(data, dict) and "messages" in data:
            # ä¸å†æ“ä½œthread_messages
            pass
        
        # Also save messages from updates events (when nodes return message updates)
        elif event_type == "updates" and isinstance(data, dict):
            for node_name, node_data in data.items():
                if isinstance(node_data, dict) and "messages" in node_data:
                    # ä¸å†æ“ä½œthread_messages
                    pass
                break  # Only process the first node with messages
        
        # Check for interrupts
        has_interrupt = False
        if event_type == "updates" and isinstance(data, dict) and "__interrupt__" in data:
            logger.info(f"Interrupt detected: {data}")
            interrupt_data = data["__interrupt__"]
            
            # æ£€æŸ¥ interrupt_data æ˜¯å¦ä¸ºç©º
            if interrupt_data and len(interrupt_data) > 0:
                # ä¸å†æ“ä½œthread_interrupts
                pass
            else:
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ç©ºçš„ä¸­æ–­æ•°æ®: {interrupt_data}")
                
                # å¤„ç†ç©ºçš„ä¸­æ–­æ•°æ®ï¼šåˆ›å»ºå·¥å…·å®¡æ‰¹è¯·æ±‚
                # è¿™é€šå¸¸å‘ç”Ÿåœ¨ create_react_agent ä½¿ç”¨ interrupt_before=["tools"] æ—¶
                # ä¸å†æ“ä½œthread_interrupts
                pass
            
            has_interrupt = True
        
        return f"id: {event_id}\nevent: {event_type}\ndata: {json.dumps(serialized_data, ensure_ascii=False)}\n\n", has_interrupt
    else:
        # Handle dict format (fallback)
        serializable_chunk = {}
        for key, value in chunk.items():
            serializable_chunk[key] = serialize_value(value)
        
        event_type = list(serializable_chunk.keys())[0] if serializable_chunk else "data"
        return f"id: {event_id}\nevent: {event_type}\ndata: {json.dumps(serializable_chunk[event_type], ensure_ascii=False)}\n\n", False

async def stream_with_graph_postgres(graph, request_body, thread_id):
    """PostgreSQLæ¨¡å¼ä¸“ç”¨çš„å›¾æµåª’ä½“å¤„ç†å‡½æ•°"""
    config, graph_input, stream_modes, checkpoint = prepare_graph_config(request_body, thread_id)
    
    # ä» configurable ä¸­è·å– file_ids
    file_ids = None
    if request_body.config and request_body.config.get("configurable"):
        file_ids = request_body.config["configurable"].get("file_ids")
    
    # å¦‚æœæœ‰å…³è”çš„æ–‡æ¡£ï¼Œå°†æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°æ¶ˆæ¯ä¸Šä¸‹æ–‡ä¸­
    if file_ids and graph_input and "messages" in graph_input:
        logger.info(f"ğŸ“„ æ£€æµ‹åˆ°å…³è”æ–‡æ¡£: {file_ids}")
        from .document_service import document_service
        
        # è·å–æ–‡æ¡£ä¸Šä¸‹æ–‡
        doc_context = document_service.get_document_context(file_ids)
        if doc_context:
            # åœ¨ç”¨æˆ·æ¶ˆæ¯å‰æ’å…¥æ–‡æ¡£ä¸Šä¸‹æ–‡ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯
            doc_message = {
                "type": "system",
                "content": f"è¯·å‚è€ƒä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n{doc_context}"
            }
            graph_input["messages"].insert(0, doc_message)
            logger.info(f"âœ… å·²æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(doc_context)} å­—ç¬¦")
            
            # ä¿å­˜ä¼šè¯å’Œæ–‡æ¡£çš„å…³è”
            agent_id = config.get("configurable", {}).get("agent_id", "diagnostic_agent")
            user_name = config.get("configurable", {}).get("user_name", "system")
            await save_thread_file_associations(thread_id, file_ids, agent_id, user_name)
    
    logger.info(f"Starting stream with modes: {stream_modes}, checkpoint: {checkpoint}")
    
    event_id = 0
    has_interrupt = False
    
    async for chunk in graph.astream(graph_input, config=config, stream_mode=stream_modes, subgraphs=True):
        try:
            event_id += 1
            sse_data, chunk_has_interrupt = await process_stream_chunk(chunk, event_id, thread_id)
            yield sse_data
            if chunk_has_interrupt:
                has_interrupt = True
        except Exception as e:
            logger.error(f"Serialization error: {e}, chunk type: {type(chunk)}, chunk: {chunk}", exc_info=True)
            event_id += 1
            yield f"id: {event_id}\nevent: error\ndata: {json.dumps({'error': str(e), 'chunk_type': str(type(chunk)), 'chunk': str(chunk)}, ensure_ascii=False)}\n\n"
    
    # End event - only send if no interrupt occurred
    if not has_interrupt:
        event_id += 1
        yield f"id: {event_id}\nevent: end\ndata: {json.dumps({'status': 'completed'}, ensure_ascii=False)}\n\n"
    else:
        logger.info("Skipping end event due to interrupt - waiting for user approval")

async def handle_postgres_streaming(request_body, thread_id):
    """å¤„ç†PostgreSQLæ¨¡å¼çš„æµå¼å“åº” - å®Œå…¨åŸºäºæ•°æ®åº“é…ç½®"""
    from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
    from .agent_config_service import AgentConfigService
    
    # LangGraph SDKä½¿ç”¨assistant_idï¼Œè½¬æ¢ä¸ºå†…éƒ¨çš„agent_id
    agent_id = request_body.assistant_id
    # ä»æ•°æ®åº“è·å–æ™ºèƒ½ä½“é…ç½®
    from src.shared.db.config import get_sync_db
    db_gen = get_sync_db()
    db = next(db_gen)
    try:
        agent_config = AgentConfigService.get_agent_config(agent_id, db)
    finally:
        db.close()
    
    if not agent_config:
        raise Exception(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ™ºèƒ½ä½“é…ç½®: {agent_id}")
    
    # æ ¹æ®æ•°æ®åº“ä¸­çš„is_builtinå­—æ®µåˆ¤æ–­ä½¿ç”¨å“ªä¸ªå›¾
    is_builtin = agent_config.get('is_builtin') == 'yes'
    # æŒ‰ç…§å®˜æ–¹æ¨¡å¼ï¼šåœ¨async withå†…å®Œæˆæ•´ä¸ªè¯·æ±‚å‘¨æœŸ
    async with AsyncPostgresSaver.from_conn_string(CHECK_POINT_URI) as checkpointer:
        await checkpointer.setup()
        
        if is_builtin:
            # å†…ç½®æ™ºèƒ½ä½“ä½¿ç”¨ä¸“ç”¨å›¾
            if agent_id == 'diagnostic_agent':
                from ..llm_agents.diagnostic_agent.graph import builder
                graph = builder.compile(checkpointer=checkpointer, name="diagnostic-agent")
            else:
                raise Exception(f"ä¸æ”¯æŒçš„å†…ç½®æ™ºèƒ½ä½“: {agent_id}")
        else:
            # è‡ªå®šä¹‰æ™ºèƒ½ä½“ä½¿ç”¨generic_agentå›¾
            from ..llm_agents.generic_agent.graph import builder
            graph = builder.compile(checkpointer=checkpointer, name=f"{agent_id}-agent")
        
        # å°†agent_idæ·»åŠ åˆ°configä¸­ï¼Œä¼ é€’ç»™graph
        if not request_body.config:
            request_body.config = {}
        if not request_body.config.get("configurable"):
            request_body.config["configurable"] = {}
        request_body.config["configurable"]["agent_id"] = agent_id
        
        # åœ¨åŒä¸€ä¸ªasync withå†…æ‰§è¡Œå®Œæ•´çš„æµå¼å¤„ç†
        async for item in stream_with_graph_postgres(graph, request_body, thread_id):
            yield item

async def stream_run_standard(thread_id: str, request_body: RunCreate):
    """Standard LangGraph streaming endpoint - æ”¯æŒåŠ¨æ€æ™ºèƒ½ä½“æ£€æŸ¥"""
    from .agent_config_service import AgentConfigService
    
    # LangGraph SDKä½¿ç”¨assistant_idï¼Œè½¬æ¢ä¸ºå†…éƒ¨çš„agent_id
    agent_id = request_body.assistant_id
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¯¥æ™ºèƒ½ä½“
    from src.shared.db.config import get_sync_db
    db_gen = get_sync_db()
    db = next(db_gen)
    try:
        agent_config = AgentConfigService.get_agent_config(agent_id, db)
    finally:
        db.close()
    
    if not agent_config:
        raise BusinessException(f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}", ResponseCode.NOT_FOUND)
    
    # ç§»é™¤éªŒè¯é€»è¾‘ï¼Œç›´æ¥æ ¹æ®æ•°æ®åº“é…ç½®å¤„ç†æ™ºèƒ½ä½“
    
    # åˆ›å»ºç”¨æˆ·çº¿ç¨‹å…³è”ï¼ˆå¦‚æœæä¾›äº†ç”¨æˆ·åä¸”å…³è”ä¸å­˜åœ¨ï¼‰
    # ç”¨æˆ·åå¯èƒ½åœ¨ request_body.user_name æˆ– request_body.input.user_name ä¸­
    user_name = None
    if request_body.user_name:
        user_name = request_body.user_name
    elif request_body.input and isinstance(request_body.input, dict) and "user_name" in request_body.input:
        user_name = request_body.input["user_name"]
    
    if user_name:
        logger.info(f"ğŸ” å¼€å§‹å¤„ç†ç”¨æˆ·çº¿ç¨‹å…³è”: {user_name} -> {thread_id}")
        try:
            await ensure_user_thread_mapping(user_name, thread_id, request_body)
        except Exception as e:
            logger.error(f"å¤„ç†ç”¨æˆ·çº¿ç¨‹å…³è”æ—¶å‡ºé”™: {e}", exc_info=True)
            # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
    else:
        logger.warning(f"âš ï¸ è¯·æ±‚ä¸­æ²¡æœ‰æä¾›ç”¨æˆ·åï¼Œè·³è¿‡ç”¨æˆ·çº¿ç¨‹å…³è”åˆ›å»º")

    async def generate():
        try:
            # åªä¿ç•™PostgreSQLå¤„ç†é€»è¾‘
            async for item in handle_postgres_streaming(request_body, thread_id):
                yield item
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Error in streaming: {e}")
            logger.error(f"Full traceback: {error_details}")
            yield f"event: error\n"
            yield f"data: {json.dumps({'type': 'error', 'error': str(e), 'traceback': error_details}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Content-Type": "text/event-stream"
        }
    )


async def save_thread_file_associations(thread_id: str, file_ids: List[str], agent_id: str, user_name: str) -> None:
    """
    ä¿å­˜ä¼šè¯å’Œæ–‡æ¡£çš„å…³è”å…³ç³»
    
    Args:
        thread_id: ä¼šè¯çº¿ç¨‹ID
        file_ids: æ–‡ä»¶IDåˆ—è¡¨
        agent_id: æ™ºèƒ½ä½“ID
        user_name: ç”¨æˆ·å
    """
    from ..models import AgentDocumentSession
    
    try:
        async with get_async_db_context() as db:
            for file_id in file_ids:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å…³è”
                result = await db.execute(
                    select(AgentDocumentSession).where(
                        AgentDocumentSession.thread_id == thread_id,
                        AgentDocumentSession.file_id == file_id
                    )
                )
                existing = result.scalar_one_or_none()
                
                if not existing:
                    # åˆ›å»ºæ–°çš„å…³è”
                    session = AgentDocumentSession(
                        thread_id=thread_id,
                        file_id=file_id,
                        agent_id=agent_id,
                        create_by=user_name
                    )
                    db.add(session)
            
            await db.commit()
            logger.info(f"âœ… ä¿å­˜ä¼šè¯æ–‡æ¡£å…³è”æˆåŠŸ: thread_id={thread_id}, files={file_ids}")
    except Exception as e:
        logger.error(f"ä¿å­˜ä¼šè¯æ–‡æ¡£å…³è”å¤±è´¥: {e}", exc_info=True)
        # ä¸å½±å“ä¸»æµç¨‹


async def get_thread_file_ids(thread_id: str) -> List[str]:
    """
    è·å–ä¼šè¯å…³è”çš„æ–‡ä»¶IDåˆ—è¡¨
    
    Args:
        thread_id: ä¼šè¯çº¿ç¨‹ID
        
    Returns:
        æ–‡ä»¶IDåˆ—è¡¨
    """
    from ..models import AgentDocumentSession
    
    try:
        async with get_async_db_context() as db:
            result = await db.execute(
                select(AgentDocumentSession.file_id)
                .where(AgentDocumentSession.thread_id == thread_id)
                .order_by(AgentDocumentSession.create_time)
            )
            
            file_ids = [row[0] for row in result.fetchall()]
            logger.info(f"âœ… è·å–ä¼šè¯æ–‡æ¡£å…³è”æˆåŠŸ: thread_id={thread_id}, files={file_ids}")
            return file_ids
    except Exception as e:
        logger.error(f"è·å–ä¼šè¯æ–‡æ¡£å…³è”å¤±è´¥: {e}", exc_info=True)
        return []