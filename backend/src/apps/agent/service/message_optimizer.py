"""
æ¶ˆæ¯ä¼˜åŒ–æœåŠ¡ - å¤„ç†å¤§æ–‡æ¡£å’Œé•¿å¯¹è¯çš„ä¼˜åŒ–ç­–ç•¥

åŠŸèƒ½ï¼š
1. MapReduceï¼šå•æ¬¡è¾“å…¥è¿‡é•¿æ—¶åˆ†å—å¤„ç†
2. æ¶ˆæ¯å‹ç¼©ï¼šå¤šè½®å¯¹è¯è¿‡é•¿æ—¶å‹ç¼©å†å²
3. å¯é…ç½®å¼€å…³ï¼šå®‰å…¨åœ°é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ
"""

import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import tiktoken
import json

from src.shared.core.logging import get_logger
from src.shared.core.config import settings

logger = get_logger(__name__)


@dataclass
class OptimizerConfig:
    """ä¼˜åŒ–å™¨é…ç½®"""
    enabled: bool = False  # æ€»å¼€å…³
    
    # MapReduceé…ç½®
    enable_map_reduce: bool = True
    single_message_threshold: int = 50000  # å•æ¡æ¶ˆæ¯è¶…è¿‡æ­¤tokenæ•°è§¦å‘MapReduce
    chunk_size: int = 20000  # æ¯ä¸ªchunkçš„å¤§å°
    chunk_overlap: int = 1000  # chunkä¹‹é—´çš„é‡å 
    map_reduce_all_roles: bool = False  # æ˜¯å¦å¯¹æ‰€æœ‰è§’è‰²çš„æ¶ˆæ¯è¿›è¡ŒMapReduceï¼ˆé»˜è®¤åªå¤„ç†useræ¶ˆæ¯ï¼‰
    
    # æ¶ˆæ¯å‹ç¼©é…ç½®
    enable_compression: bool = True
    total_context_threshold: float = 0.8  # è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡çš„80%è§¦å‘å‹ç¼©
    compression_target: float = 0.6  # å‹ç¼©åˆ°æœ€å¤§ä¸Šä¸‹æ–‡çš„60%
    
    # ä¿ç•™ç­–ç•¥
    keep_system_messages: bool = True  # å§‹ç»ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    keep_recent_messages: int = 10  # å§‹ç»ˆä¿ç•™æœ€è¿‘Næ¡æ¶ˆæ¯
    

class MessageOptimizer:
    """æ¶ˆæ¯ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: Optional[OptimizerConfig] = None):
        self.config = config or OptimizerConfig()
        self.encoder = tiktoken.get_encoding("cl100k_base")
        
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°"""
        try:
            return len(self.encoder.encode(text))
        except Exception:
            # ç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            english_chars = len(text) - chinese_chars
            return int(chinese_chars / 1.5 + english_chars / 4)
    
    def count_messages_tokens(self, messages: List[Dict[str, str]]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€»tokenæ•°"""
        total = 0
        for msg in messages:
            # è§’è‰²å’Œå†…å®¹çš„token
            total += self.count_tokens(msg.get("role", ""))
            total += self.count_tokens(msg.get("content", ""))
            # æ¶ˆæ¯æ ¼å¼çš„é¢å¤–å¼€é”€
            total += 4
        return total
    
    async def optimize_messages(
        self, 
        messages: List[Dict[str, str]], 
        max_context_length: int,
        llm_instance: Any = None
    ) -> Tuple[List[Dict[str, str]], Dict[str, Any]]:
        """
        ä¼˜åŒ–æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            (ä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨, ä¼˜åŒ–ä¿¡æ¯)
        """
        if not self.config.enabled:
            return messages, {"optimized": False, "reason": "optimizer_disabled"}
        
        optimization_info = {
            "optimized": False,
            "original_tokens": 0,
            "final_tokens": 0,
            "strategies_used": []
        }
        
        # è®¡ç®—åŸå§‹tokenæ•°
        total_tokens = self.count_messages_tokens(messages)
        optimization_info["original_tokens"] = total_tokens
        
        # 1. å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¶…é•¿çš„å•æ¡æ¶ˆæ¯éœ€è¦MapReduceï¼ˆä¸ç®¡æ€»é‡å¤šå°‘ï¼‰
        optimized_messages = messages.copy()
        if self.config.enable_map_reduce and llm_instance:
            optimized_messages = await self._apply_map_reduce_if_needed(
                optimized_messages, llm_instance, optimization_info
            )
        elif self.config.enable_map_reduce and not llm_instance:
            logger.warning("MapReduce å·²å¯ç”¨ä½† LLM å®ä¾‹ä¸å¯ç”¨ï¼Œè·³è¿‡ MapReduce å¤„ç†")
        
        # 2. æ£€æŸ¥æ€»é•¿åº¦æ˜¯å¦éœ€è¦å‹ç¼©
        current_tokens = self.count_messages_tokens(optimized_messages)
        if (self.config.enable_compression and 
            current_tokens > max_context_length * self.config.total_context_threshold):
            
            target_tokens = int(max_context_length * self.config.compression_target)
            optimized_messages = await self._compress_messages(
                optimized_messages, target_tokens, llm_instance, optimization_info
            )
        
        # è®°å½•æœ€ç»ˆç»“æœ
        optimization_info["optimized"] = len(optimization_info["strategies_used"]) > 0
        optimization_info["final_tokens"] = self.count_messages_tokens(optimized_messages)
        
        # å¦‚æœæ²¡æœ‰è¿›è¡Œä»»ä½•ä¼˜åŒ–ï¼Œè®°å½•åŸå› 
        if not optimization_info["optimized"]:
            if total_tokens < max_context_length * 0.5:
                optimization_info["reason"] = "within_safe_range"
            else:
                optimization_info["reason"] = "no_optimization_needed"
        
        if optimization_info["optimized"]:
            # è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            reduction = optimization_info["original_tokens"] - optimization_info["final_tokens"]
            reduction_rate = (reduction / optimization_info["original_tokens"]) * 100
            
            logger.info(
                f"ğŸ“Š æ¶ˆæ¯ä¼˜åŒ–å®Œæˆ:\n"
                f"  - åŸå§‹ tokens: {optimization_info['original_tokens']:,}\n"
                f"  - ä¼˜åŒ–å tokens: {optimization_info['final_tokens']:,}\n"
                f"  - å‡å°‘ tokens: {reduction:,} ({reduction_rate:.1f}%)\n"
                f"  - ä½¿ç”¨ç­–ç•¥: {', '.join(optimization_info['strategies_used'])}\n"
                f"  - ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡: {(optimization_info['final_tokens'] / max_context_length * 100):.1f}%"
            )
        
        return optimized_messages, optimization_info
    
    async def _apply_map_reduce_if_needed(
        self,
        messages: List[Dict[str, str]],
        llm_instance: Any,
        optimization_info: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """å¯¹è¶…é•¿æ¶ˆæ¯åº”ç”¨MapReduce"""
        result_messages = []
        
        for msg in messages:
            msg_tokens = self.count_tokens(msg.get("content", ""))
            
            # è·å–æ¶ˆæ¯è§’è‰²ï¼ˆå…¼å®¹ role æˆ– type å­—æ®µï¼‰
            msg_role = msg.get("role", msg.get("type", "")).lower()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºç”¨æˆ·æ¶ˆæ¯
            # role å­—æ®µ: user â†’ ç”¨æˆ·æ¶ˆæ¯
            # type å­—æ®µ: human â†’ ç”¨æˆ·æ¶ˆæ¯
            is_user_message = False
            if "role" in msg:
                is_user_message = msg.get("role", "").lower() == "user"
            elif "type" in msg:
                is_user_message = msg.get("type", "").lower() == "human"
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæ¶ˆæ¯ç»“æ„
            if msg_tokens > 100:  # åªå¯¹è¾ƒé•¿æ¶ˆæ¯è¾“å‡ºè°ƒè¯•ä¿¡æ¯
                logger.debug(
                    f"æ¶ˆæ¯åˆ†æ - tokens: {msg_tokens}, role/type: '{msg_role}', "
                    f"æ˜¯ç”¨æˆ·æ¶ˆæ¯: {is_user_message}, é˜ˆå€¼: {self.config.single_message_threshold}"
                )
            
            # åªå¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œç³»ç»Ÿæ¶ˆæ¯é€šå¸¸æ˜¯é‡è¦çš„æç¤ºè¯ä¸åº”è¢«å‹ç¼©
            if msg_tokens > self.config.single_message_threshold and is_user_message:
                # éœ€è¦MapReduceå¤„ç†
                logger.info(
                    f"ğŸ” æ£€æµ‹åˆ°è¶…é•¿æ¶ˆæ¯:\n"
                    f"  - æ¶ˆæ¯é•¿åº¦: {msg_tokens:,} tokens\n"
                    f"  - è§¦å‘é˜ˆå€¼: {self.config.single_message_threshold:,} tokens\n"
                    f"  - å¯åŠ¨ MapReduce å¤„ç†..."
                )
                
                try:
                    reduced_content = await self._map_reduce_content(
                        msg["content"], 
                        "è¯·æå–å¹¶æ€»ç»“å…³é”®ä¿¡æ¯ï¼Œä¿ç•™é‡è¦ç»†èŠ‚ã€‚",
                        llm_instance
                    )
                    
                    # è®¡ç®—ä¼˜åŒ–åçš„tokenæ•°
                    reduced_tokens = self.count_tokens(reduced_content)
                    reduction_rate = ((msg_tokens - reduced_tokens) / msg_tokens) * 100
                    
                    logger.info(
                        f"âœ… MapReduce å®Œæˆ:\n"
                        f"  - åŸå§‹é•¿åº¦: {msg_tokens:,} tokens\n"
                        f"  - ä¼˜åŒ–åé•¿åº¦: {reduced_tokens:,} tokens\n"
                        f"  - å‹ç¼©ç‡: {reduction_rate:.1f}%"
                    )
                    
                    # ä¿æŒåŸæ¶ˆæ¯çš„å­—æ®µç»“æ„ï¼ˆrole æˆ– typeï¼‰
                    optimized_msg = {}
                    if "role" in msg:
                        optimized_msg["role"] = msg["role"]
                    elif "type" in msg:
                        optimized_msg["type"] = msg["type"]
                    optimized_msg["content"] = f"[å·²ä¼˜åŒ–çš„é•¿æ–‡æœ¬]\n{reduced_content}"
                    
                    result_messages.append(optimized_msg)
                    
                    optimization_info["strategies_used"].append("map_reduce")
                    
                except Exception as e:
                    logger.error(f"MapReduceå¤„ç†å¤±è´¥: {e}")
                    result_messages.append(msg)  # å¤±è´¥æ—¶ä¿ç•™åŸæ¶ˆæ¯
            elif msg_tokens > self.config.single_message_threshold and not is_user_message:
                # è¶…é•¿ä½†ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯
                logger.info(
                    f"âš ï¸ æ£€æµ‹åˆ°è¶…é•¿éç”¨æˆ·æ¶ˆæ¯:\n"
                    f"  - æ¶ˆæ¯é•¿åº¦: {msg_tokens:,} tokens\n"
                    f"  - æ¶ˆæ¯ç±»å‹: {msg_role}\n"
                    f"  - è·³è¿‡ MapReduce å¤„ç†ï¼ˆä»…å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼‰"
                )
                result_messages.append(msg)
            else:
                result_messages.append(msg)
        
        return result_messages
    
    async def _map_reduce_content(
        self,
        content: str,
        instruction: str,
        llm_instance: Any
    ) -> str:
        """
        MapReduceå¤„ç†é•¿å†…å®¹
        
        Map: å°†å†…å®¹åˆ†å—ï¼Œæ¯å—ç‹¬ç«‹å¤„ç†
        Reduce: åˆå¹¶æ‰€æœ‰å—çš„å¤„ç†ç»“æœ
        """
        # åˆ†å—
        chunks = self._split_text(content)
        total_chars = len(content)
        avg_chunk_size = total_chars // len(chunks) if chunks else 0
        
        logger.info(
            f"ğŸ“„ MapReduce åˆ†å—ä¿¡æ¯:\n"
            f"  - æ€»å­—ç¬¦æ•°: {total_chars:,}\n"
            f"  - åˆ†å—æ•°é‡: {len(chunks)}\n"
            f"  - å¹³å‡å—å¤§å°: {avg_chunk_size:,} å­—ç¬¦\n"
            f"  - å—å¤§å°é…ç½®: {self.config.chunk_size:,} tokens"
        )
        
        # Mapé˜¶æ®µï¼šå¹¶è¡Œå¤„ç†æ¯ä¸ªå—
        map_tasks = []
        for i, chunk in enumerate(chunks):
            prompt = f"""
{instruction}

è¿™æ˜¯ç¬¬ {i+1}/{len(chunks)} éƒ¨åˆ†ï¼š

{chunk}

æå–çš„å…³é”®ä¿¡æ¯ï¼š
"""
            map_tasks.append(self._call_llm_async(llm_instance, prompt))
        
        # å¹¶å‘æ‰§è¡Œï¼Œä½†é™åˆ¶å¹¶å‘æ•°
        mapped_results = await self._run_with_concurrency(map_tasks, max_concurrent=3)
        
        # Reduceé˜¶æ®µï¼šåˆå¹¶ç»“æœ
        if len(mapped_results) == 1:
            return mapped_results[0]
        
        combined_text = "\n\n".join([
            f"=== ç¬¬ {i+1} éƒ¨åˆ†æå–çš„ä¿¡æ¯ ===\n{result}"
            for i, result in enumerate(mapped_results)
        ])
        
        # å¦‚æœåˆå¹¶åstillå¤ªé•¿ï¼Œé€’å½’å¤„ç†
        if self.count_tokens(combined_text) > self.config.single_message_threshold:
            return await self._map_reduce_content(
                combined_text, 
                "è¯·ç»¼åˆä»¥ä¸‹å„éƒ¨åˆ†çš„ä¿¡æ¯ï¼Œç”Ÿæˆç»Ÿä¸€çš„æ€»ç»“ï¼š",
                llm_instance
            )
        
        # æœ€ç»ˆreduce
        reduce_prompt = f"""
è¯·ç»¼åˆä»¥ä¸‹å„éƒ¨åˆ†æå–çš„ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æ€»ç»“ï¼š

{combined_text}

ç»¼åˆæ€»ç»“ï¼š
"""
        
        return await self._call_llm_async(llm_instance, reduce_prompt)
    
    async def _compress_messages(
        self,
        messages: List[Dict[str, str]],
        target_tokens: int,
        llm_instance: Any,
        optimization_info: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """å‹ç¼©æ¶ˆæ¯å†å²"""
        original_count = len(messages)
        original_tokens = self.count_messages_tokens(messages)
        
        logger.info(
            f"ğŸ“¦ å¼€å§‹å‹ç¼©å†å²æ¶ˆæ¯:\n"
            f"  - åŸå§‹æ¶ˆæ¯æ•°: {original_count}\n"
            f"  - åŸå§‹ tokens: {original_tokens:,}\n"
            f"  - ç›®æ ‡ tokens: {target_tokens:,}\n"
            f"  - å‹ç¼©ç­–ç•¥: ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ + æœ€è¿‘ {self.config.keep_recent_messages} æ¡æ¶ˆæ¯"
        )
        # åˆ†ç¦»ä¸åŒç±»å‹çš„æ¶ˆæ¯
        system_messages = []
        other_messages = []
        
        for msg in messages:
            msg_role = msg.get("role", msg.get("type", "")).lower()
            # system role æˆ– system type éƒ½è§†ä¸ºç³»ç»Ÿæ¶ˆæ¯
            if msg_role == "system" and self.config.keep_system_messages:
                system_messages.append(msg)
            else:
                other_messages.append(msg)
        
        # ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
        if len(other_messages) <= self.config.keep_recent_messages:
            return messages
        
        recent_messages = other_messages[-self.config.keep_recent_messages:]
        historical_messages = other_messages[:-self.config.keep_recent_messages]
        
        # å¯¹å†å²æ¶ˆæ¯ç”Ÿæˆæ‘˜è¦
        if llm_instance and len(historical_messages) > 2:
            try:
                summary = await self._summarize_messages(historical_messages, llm_instance)
                
                # åˆ›å»ºæ‘˜è¦æ¶ˆæ¯ï¼Œä¿æŒå­—æ®µæ ¼å¼ä¸€è‡´
                summary_msg = {"content": f"[å†å²å¯¹è¯æ‘˜è¦]\n{summary}"}
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¶ˆæ¯çš„æ ¼å¼æ¥å†³å®šç”¨ role è¿˜æ˜¯ type
                if messages and "role" in messages[0]:
                    summary_msg["role"] = "system"
                elif messages and "type" in messages[0]:
                    summary_msg["type"] = "system"
                else:
                    summary_msg["role"] = "system"  # é»˜è®¤ä½¿ç”¨ role
                
                compressed_messages = (
                    system_messages + 
                    [summary_msg] +
                    recent_messages
                )
                
                # è®¡ç®—å‹ç¼©æ•ˆæœ
                compressed_count = len(compressed_messages)
                compressed_tokens = self.count_messages_tokens(compressed_messages)
                
                logger.info(
                    f"âœ… å†å²å‹ç¼©å®Œæˆ:\n"
                    f"  - æ¶ˆæ¯æ•°: {original_count} â†’ {compressed_count}\n"
                    f"  - Tokens: {original_tokens:,} â†’ {compressed_tokens:,}\n"
                    f"  - å‹ç¼©ç‡: {((original_tokens - compressed_tokens) / original_tokens * 100):.1f}%\n"
                    f"  - å†å²æ¶ˆæ¯ç”Ÿæˆæ‘˜è¦: {len(historical_messages)} æ¡ â†’ 1 æ¡æ‘˜è¦"
                )
                
                optimization_info["strategies_used"].append("compression")
                return compressed_messages
                
            except Exception as e:
                logger.error(f"æ¶ˆæ¯å‹ç¼©å¤±è´¥: {e}")
        
        # å¦‚æœå‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£
        return self._sliding_window_compression(messages, target_tokens, optimization_info)
    
    def _sliding_window_compression(
        self,
        messages: List[Dict[str, str]],
        target_tokens: int,
        optimization_info: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """æ»‘åŠ¨çª—å£å‹ç¼©"""
        system_messages = []
        other_messages = []
        
        for m in messages:
            msg_role = m.get("role", m.get("type", "")).lower()
            if msg_role == "system":
                system_messages.append(m)
            else:
                other_messages.append(m)
        
        # è®¡ç®—ç³»ç»Ÿæ¶ˆæ¯çš„tokenæ•°
        system_tokens = self.count_messages_tokens(system_messages)
        available_tokens = target_tokens - system_tokens
        
        # ä»åå¾€å‰ä¿ç•™æ¶ˆæ¯
        result = []
        current_tokens = 0
        
        for msg in reversed(other_messages):
            msg_tokens = self.count_tokens(msg.get("content", "")) + 10
            if current_tokens + msg_tokens <= available_tokens:
                result.insert(0, msg)
                current_tokens += msg_tokens
            else:
                break
        
        optimization_info["strategies_used"].append("sliding_window")
        return system_messages + result
    
    def _split_text(self, text: str) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—"""
        chunks = []
        
        # ä¼˜å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        current_chunk = ""
        current_tokens = 0
        
        for para in paragraphs:
            para_tokens = self.count_tokens(para)
            
            if current_tokens + para_tokens <= self.config.chunk_size:
                current_chunk += para + "\n\n"
                current_tokens += para_tokens
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                if para_tokens > self.config.chunk_size:
                    # æ®µè½å¤ªé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                    sub_chunks = self._split_long_text(para, self.config.chunk_size)
                    chunks.extend(sub_chunks)
                    current_chunk = ""
                    current_tokens = 0
                else:
                    current_chunk = para + "\n\n"
                    current_tokens = para_tokens
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _split_long_text(self, text: str, max_tokens: int) -> List[str]:
        """åˆ†å‰²è¶…é•¿æ–‡æœ¬"""
        chunks = []
        sentences = text.replace('ã€‚', 'ã€‚\n').replace('. ', '.\n').split('\n')
        
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = self.count_tokens(sentence)
            
            if current_tokens + sentence_tokens <= max_tokens:
                current_chunk += sentence
                current_tokens += sentence_tokens
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
                current_tokens = sentence_tokens
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    async def _summarize_messages(
        self,
        messages: List[Dict[str, str]],
        llm_instance: Any
    ) -> str:
        """ç”Ÿæˆæ¶ˆæ¯æ‘˜è¦"""
        conversation_parts = []
        for msg in messages:
            # è·å–è§’è‰²ï¼Œå…¼å®¹ role å’Œ type
            role = msg.get('role', msg.get('type', 'unknown')).upper()
            content = msg.get('content', '')
            if len(content) > 500:
                conversation_parts.append(f"{role}: {content[:500]}...")
            else:
                conversation_parts.append(f"{role}: {content}")
        
        conversation = "\n".join(conversation_parts)
        
        prompt = f"""
è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²æ€»ç»“ä¸ºå…³é”®è¦ç‚¹ï¼Œä¿ç•™é‡è¦ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼š

{conversation}

å…³é”®è¦ç‚¹æ€»ç»“ï¼š
"""
        
        return await self._call_llm_async(llm_instance, prompt)
    
    async def _call_llm_async(self, llm_instance: Any, prompt: str) -> str:
        """å¼‚æ­¥è°ƒç”¨LLM"""
        try:
            # æ ¹æ®ä¸åŒçš„LLMå®ä¾‹ç±»å‹è°ƒç”¨
            if hasattr(llm_instance, 'ainvoke'):
                response = await llm_instance.ainvoke(prompt)
                return response.content if hasattr(response, 'content') else str(response)
            elif hasattr(llm_instance, 'invoke'):
                # åŒæ­¥æ–¹æ³•ï¼Œä½¿ç”¨çº¿ç¨‹æ± 
                import concurrent.futures
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    response = await loop.run_in_executor(
                        executor, llm_instance.invoke, prompt
                    )
                return response.content if hasattr(response, 'content') else str(response)
            else:
                raise ValueError("Unsupported LLM instance type")
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def _run_with_concurrency(
        self,
        tasks: List[asyncio.Task],
        max_concurrent: int = 3
    ) -> List[Any]:
        """é™åˆ¶å¹¶å‘æ‰§è¡Œä»»åŠ¡"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def bounded_task(task):
            async with semaphore:
                return await task
        
        return await asyncio.gather(*[bounded_task(task) for task in tasks])


# å…¨å±€å®ä¾‹
message_optimizer = MessageOptimizer()


def get_optimizer_config() -> OptimizerConfig:
    """ä»é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è·å–ä¼˜åŒ–å™¨é…ç½®"""
    return OptimizerConfig(
        enabled=getattr(settings, 'MESSAGE_OPTIMIZER_ENABLED', False),
        # å•æ¡æ¶ˆæ¯ä¼˜åŒ–é…ç½®
        enable_map_reduce=getattr(settings, 'SINGLE_MSG_ENABLE_MAP_REDUCE', True),
        single_message_threshold=getattr(settings, 'SINGLE_MSG_TOKEN_THRESHOLD', 50000),
        chunk_size=getattr(settings, 'SINGLE_MSG_CHUNK_SIZE', 20000),
        # å¤šè½®ä¼šè¯ä¼˜åŒ–é…ç½®
        enable_compression=getattr(settings, 'MULTI_TURN_ENABLE_COMPRESSION', True),
        total_context_threshold=getattr(settings, 'MULTI_TURN_CONTEXT_THRESHOLD', 0.8),
        compression_target=getattr(settings, 'MULTI_TURN_COMPRESSION_TARGET', 0.6),
    )


def get_optimizer_llm():
    """è·å–æ¶ˆæ¯ä¼˜åŒ–å™¨ä½¿ç”¨çš„ LLM å®ä¾‹"""
    try:
        from langchain_openai import ChatOpenAI
        from src.shared.db.config import get_sync_db
        from src.apps.ai_model.models import AIModelConfig
        
        # è·å–é…ç½®çš„æ¨¡å‹åç§°
        model_name = getattr(settings, 'MESSAGE_OPTIMIZER_MODEL_NAME', 'deepseek-chat')
        
        # ä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½®
        db_gen = get_sync_db()
        db = next(db_gen)
        try:
            # æ³¨æ„ï¼šmodel_type å­—æ®µå­˜å‚¨çš„æ˜¯å®é™…çš„æ¨¡å‹æ ‡è¯†ï¼ˆå¦‚ deepseek-chatï¼‰
            # è€Œä¸æ˜¯ name å­—æ®µï¼ˆé‚£æ˜¯ç”¨æˆ·å‹å¥½çš„æ˜¾ç¤ºåç§°ï¼‰
            model = db.query(AIModelConfig).filter(
                AIModelConfig.model_type == model_name
            ).first()
            
            if not model:
                logger.warning(f"æœªæ‰¾åˆ°é…ç½®çš„ä¼˜åŒ–å™¨æ¨¡å‹: {model_name}")
                return None
                
            # åˆ›å»º ChatOpenAI å®ä¾‹
            import httpx
            
            # åˆ›å»ºä¸€ä¸ªç¦ç”¨SSLéªŒè¯çš„httpxå®¢æˆ·ç«¯
            # æ³¨æ„ï¼šChatOpenAIä¼šç®¡ç†è¿™ä¸ªclientçš„ç”Ÿå‘½å‘¨æœŸ
            http_client = httpx.Client(
                verify=False,  # å…³é—­ SSL æ ¡éªŒ
                timeout=30.0
            )
            
            llm = ChatOpenAI(
                model=model.model_type,
                api_key=model.api_key_value,  # æ³¨æ„ï¼šæ•°æ®åº“å­—æ®µæ˜¯ api_key_value
                base_url=model.endpoint_url,   # æ³¨æ„ï¼šæ•°æ®åº“å­—æ®µæ˜¯ endpoint_url
                temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„æ‘˜è¦
                timeout=30,
                http_client=http_client
            )
            
            logger.info(f"æˆåŠŸåˆ›å»ºä¼˜åŒ–å™¨ LLM: {model_name}")
            return llm
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"åˆ›å»ºä¼˜åŒ–å™¨ LLM å¤±è´¥: {e}")
        return None


async def optimize_messages_if_needed(
    graph_input: Dict[str, Any],
    max_context_length: Optional[int] = None
) -> None:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ ¹æ®é…ç½®ä¼˜åŒ–æ¶ˆæ¯
    ç›´æ¥ä¿®æ”¹ graph_input ä¸­çš„ messages
    
    Args:
        graph_input: åŒ…å« messages çš„è¾“å…¥å­—å…¸
        max_context_length: å¯é€‰çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼ˆå¦‚æœä¸æä¾›ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    """
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨
    config = get_optimizer_config()
    if not config.enabled:
        return
    
    if not graph_input or "messages" not in graph_input:
        return
    
    # ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡é•¿åº¦æˆ–é»˜è®¤å€¼
    if max_context_length is None:
        max_context_length = 128000
        
        # å°è¯•ä»ä¼˜åŒ–å™¨æ¨¡å‹é…ç½®è·å–ä¸Šä¸‹æ–‡é•¿åº¦
        try:
            from src.shared.db.config import get_sync_db
            from src.apps.ai_model.models import AIModelConfig
            
            model_name = getattr(settings, 'MESSAGE_OPTIMIZER_MODEL_NAME', 'deepseek-chat')
            
            db_gen = get_sync_db()
            db = next(db_gen)
            try:
                model = db.query(AIModelConfig).filter(
                    AIModelConfig.model_type == model_name
                ).first()
                
                if model and model.config_data:
                    config_data = json.loads(model.config_data) if isinstance(model.config_data, str) else model.config_data
                    context_length = config_data.get("context_length")
                    if context_length:
                        max_context_length = int(context_length)
                        logger.info(f"ä½¿ç”¨ä¼˜åŒ–å™¨æ¨¡å‹ {model_name} çš„ä¸Šä¸‹æ–‡é•¿åº¦: {max_context_length}")
            finally:
                db.close()
        except Exception as e:
            logger.warning(f"è·å–ä¼˜åŒ–å™¨æ¨¡å‹é…ç½®å¤±è´¥: {e}")
    
    # è·å– LLM å®ä¾‹ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
    llm_instance = None
    if config.enable_map_reduce or config.enable_compression:
        llm_instance = get_optimizer_llm()
        if not llm_instance:
            logger.warning("æ— æ³•è·å–ä¼˜åŒ–å™¨ LLM å®ä¾‹ï¼Œå°†è·³è¿‡éœ€è¦ LLM çš„ä¼˜åŒ–")
            # å¦‚æœæ— æ³•è·å– LLMï¼Œç¦ç”¨éœ€è¦ LLM çš„åŠŸèƒ½
            config.enable_map_reduce = False
            config.enable_compression = False
        else:
            logger.info(f"âœ… LLM å®ä¾‹å·²å‡†å¤‡å°±ç»ªï¼ŒMapReduce={config.enable_map_reduce}, Compression={config.enable_compression}")
    
    # æ‰§è¡Œä¼˜åŒ–
    optimizer = message_optimizer
    optimizer.config = config
    
    messages = graph_input["messages"]
    message_count = len(messages)
    total_tokens = optimizer.count_messages_tokens(messages)
    
    
    # åˆ†ææ¯æ¡æ¶ˆæ¯çš„é•¿åº¦
    message_details = []
    max_single_tokens = 0
    for i, msg in enumerate(messages):
        msg_tokens = optimizer.count_tokens(msg.get("content", ""))
        max_single_tokens = max(max_single_tokens, msg_tokens)
        if msg_tokens > 100:  # åªè®°å½•è¾ƒé•¿çš„æ¶ˆæ¯
            role = msg.get('role', msg.get('type', 'unknown'))  # å…¼å®¹ role æˆ– type å­—æ®µ
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸”è§’è‰²æœªçŸ¥ï¼Œè¾“å‡ºæ¶ˆæ¯çš„é”®
            if i == 0 and role == 'unknown':
                msg_keys = list(msg.keys()) if isinstance(msg, dict) else []
                message_details.append(f"    - {role}[{i}]: {msg_tokens:,} tokens (keys: {msg_keys})")
            else:
                message_details.append(f"    - {role}[{i}]: {msg_tokens:,} tokens")
    
    log_message = (
        f"ğŸš€ å¼€å§‹æ¶ˆæ¯ä¼˜åŒ–:\n"
        f"  - æ¶ˆæ¯æ•°é‡: {message_count}\n"
        f"  - æ€» tokens: {total_tokens:,}\n"
        f"  - æœ€å¤§å•æ¡: {max_single_tokens:,} tokens\n"
        f"  - ä¸Šä¸‹æ–‡é™åˆ¶: {max_context_length:,}\n"
        f"  - ä½¿ç”¨ç‡: {(total_tokens / max_context_length * 100):.1f}%\n"
        f"  - MapReduceé˜ˆå€¼: {config.single_message_threshold:,} tokens"
    )
    
    if message_details:
        log_message += f"\n  - è¯¦ç»†æ¶ˆæ¯é•¿åº¦:\n" + "\n".join(message_details)
    
    logger.info(log_message)
    
    optimized_messages, info = await optimizer.optimize_messages(
        messages, 
        max_context_length,
        llm_instance
    )
    
    # æ›´æ–°æ¶ˆæ¯
    graph_input["messages"] = optimized_messages
    
    # è®°å½•æ—¥å¿—ï¼ˆç®€æ´ç‰ˆï¼Œè¯¦ç»†ä¿¡æ¯å·²åœ¨ optimize_messages ä¸­è¾“å‡ºï¼‰
    if info.get("optimized"):
        logger.info(
            f"ğŸ¯ ä¼˜åŒ–å™¨æ‰§è¡Œå®Œæˆ - "
            f"ç­–ç•¥: {', '.join(info.get('strategies_used', []))}"
        )