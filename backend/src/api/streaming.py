"""
æµå¼å¤„ç†ç›¸å…³æ¥å£å’Œå‡½æ•°
"""
import json
import logging
from typing import Dict, Any, List
from fastapi import HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional

from .utils import (
    should_use_postgres_mode, 
    prepare_graph_config, 
    serialize_value,
    POSTGRES_CONNECTION_STRING,
    recover_thread_from_postgres
)
from .user_threads_db import (
    check_user_thread_exists,
    create_user_thread_mapping,
    init_user_threads_db
)

logger = logging.getLogger(__name__)

# å­˜å‚¨å¼•ç”¨ - ä»app.pyå¯¼å…¥æ—¶è®¾ç½®
threads_store = None
thread_messages = None
thread_interrupts = None
ASSISTANTS = None

def init_refs(ts, tm, ti, assistants):
    """åˆå§‹åŒ–å¼•ç”¨"""
    global threads_store, thread_messages, thread_interrupts, ASSISTANTS
    threads_store = ts
    thread_messages = tm
    thread_interrupts = ti
    ASSISTANTS = assistants

class RunCreate(BaseModel):
    assistant_id: str
    input: Optional[Dict[str, Any]] = None
    config: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None
    stream_mode: Optional[List[str]] = ["values"]
    interrupt_before: Optional[List[str]] = None
    interrupt_after: Optional[List[str]] = None
    on_disconnect: Optional[str] = None
    command: Optional[Dict[str, Any]] = None
    checkpoint: Optional[Dict[str, Any]] = None
    user_name: Optional[str] = None  # ç”¨æˆ·åï¼Œç”¨äºçº¿ç¨‹å…³è”

async def process_stream_chunk(chunk, event_id, thread_id):
    """å¤„ç†å•ä¸ªæµå¼æ•°æ®å—"""    
    # Handle tuple format from LangGraph streaming
    if isinstance(chunk, tuple) and len(chunk) >= 2:
        if len(chunk) == 2:
            # æ ‡å‡†æ ¼å¼: (event_type, data)
            event_type, data = chunk
        elif len(chunk) == 3:
            # å­å›¾æ ¼å¼: (namespace, event_type, data)
            namespace, event_type, data = chunk
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•è·å–æœ€åä¸¤ä¸ªå…ƒç´ 
            event_type, data = chunk[-2:]
            logger.warning(f"âš ï¸ æœªçŸ¥çš„chunkæ ¼å¼ï¼Œé•¿åº¦={len(chunk)}, å°è¯•ä½¿ç”¨åä¸¤ä¸ªå…ƒç´ ")
        
        serialized_data = serialize_value(data)
        
        # Save messages to thread history from LangGraph state
        if event_type == "values" and isinstance(data, dict) and "messages" in data:
            if thread_messages is not None:
                if thread_id not in thread_messages:
                    thread_messages[thread_id] = []
                thread_messages[thread_id] = [serialize_value(msg) for msg in data["messages"]]
                logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(data['messages'])} æ¡æ¶ˆæ¯åˆ°çº¿ç¨‹ {thread_id}")
        
        # Also save messages from updates events (when nodes return message updates)
        elif event_type == "updates" and isinstance(data, dict):
            for node_name, node_data in data.items():
                if isinstance(node_data, dict) and "messages" in node_data:
                    if thread_messages is not None:
                        if thread_id not in thread_messages:
                            thread_messages[thread_id] = []
                        # Append new messages instead of replacing
                        new_messages = [serialize_value(msg) for msg in node_data["messages"]]
                        thread_messages[thread_id].extend(new_messages)
                        logger.info(f"ğŸ’¾ ä»èŠ‚ç‚¹ {node_name} è¿½åŠ äº† {len(new_messages)} æ¡æ¶ˆæ¯åˆ°çº¿ç¨‹ {thread_id}")
                    break  # Only process the first node with messages
        
        # Check for interrupts
        has_interrupt = False
        if event_type == "updates" and isinstance(data, dict) and "__interrupt__" in data:
            logger.info(f"Interrupt detected: {data}")
            if thread_interrupts is not None:
                if thread_id not in thread_interrupts:
                    thread_interrupts[thread_id] = []
                thread_interrupts[thread_id].append(data["__interrupt__"][0])
            has_interrupt = True
        
        return f"id: {event_id}\nevent: {event_type}\ndata: {json.dumps(serialized_data, ensure_ascii=False)}\n\n", has_interrupt
    else:
        # Handle dict format (fallback)
        serializable_chunk = {}
        for key, value in chunk.items():
            serializable_chunk[key] = serialize_value(value)
        
        event_type = list(serializable_chunk.keys())[0] if serializable_chunk else "data"
        return f"id: {event_id}\nevent: {event_type}\ndata: {json.dumps(serializable_chunk[event_type], ensure_ascii=False)}\n\n", False

async def stream_with_graph_postgres(graph, request_body, thread_id):
    """PostgreSQLæ¨¡å¼ä¸“ç”¨çš„å›¾æµåª’ä½“å¤„ç†å‡½æ•°"""
    config, graph_input, stream_modes, checkpoint = prepare_graph_config(request_body, thread_id)
    logger.info(f"Starting stream with modes: {stream_modes}, checkpoint: {checkpoint}")
    
    event_id = 0
    has_interrupt = False
    
    async for chunk in graph.astream(graph_input, config=config, stream_mode=stream_modes, subgraphs=True):
        try:
            event_id += 1
            sse_data, chunk_has_interrupt = await process_stream_chunk(chunk, event_id, thread_id)
            yield sse_data
            if chunk_has_interrupt:
                has_interrupt = True
        except Exception as e:
            logger.error(f"Serialization error: {e}, chunk type: {type(chunk)}, chunk: {chunk}")
            event_id += 1
            yield f"id: {event_id}\nevent: error\ndata: {json.dumps({'error': str(e), 'chunk_type': str(type(chunk)), 'chunk': str(chunk)}, ensure_ascii=False)}\n\n"
    
    # End event - only send if no interrupt occurred
    if not has_interrupt:
        event_id += 1
        yield f"id: {event_id}\nevent: end\ndata: {json.dumps({'status': 'completed'}, ensure_ascii=False)}\n\n"
    else:
        logger.info("Skipping end event due to interrupt - waiting for user approval")

async def stream_with_graph(graph, request_body, thread_id):
    """é€šç”¨çš„å›¾æµåª’ä½“å¤„ç†å‡½æ•°"""
    config, graph_input, stream_modes, checkpoint = prepare_graph_config(request_body, thread_id)
    logger.info(f"Starting stream with modes: {stream_modes}, checkpoint: {checkpoint}")
    
    event_id = 0
    has_interrupt = False
    
    async for chunk in graph.astream(graph_input, config=config, stream_mode=stream_modes, subgraphs=True):
        try:
            event_id += 1
            sse_data, chunk_has_interrupt = await process_stream_chunk(chunk, event_id, thread_id)
            yield sse_data
            if chunk_has_interrupt:
                has_interrupt = True
        except Exception as e:
            logger.error(f"Serialization error: {e}, chunk type: {type(chunk)}, chunk: {chunk}")
            event_id += 1
            yield f"id: {event_id}\nevent: error\ndata: {json.dumps({'error': str(e), 'chunk_type': str(type(chunk)), 'chunk': str(chunk)}, ensure_ascii=False)}\n\n"
    
    # End event - only send if no interrupt occurred
    if not has_interrupt:
        event_id += 1
        yield f"id: {event_id}\nevent: end\ndata: {json.dumps({'status': 'completed'}, ensure_ascii=False)}\n\n"
    else:
        logger.info("Skipping end event due to interrupt - waiting for user approval")

async def handle_postgres_streaming(request_body, thread_id):
    """å¤„ç†PostgreSQLæ¨¡å¼çš„æµå¼å“åº”"""
    from src.agents.diagnostic_agent.main_graph import builder
    from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
    
    logger.info(f"ğŸ” PostgreSQLæ¨¡å¼ - æŒ‰ç…§å®˜æ–¹æ¨¡å¼ä½¿ç”¨async with")
    if thread_id in threads_store:
        threads_store[thread_id]["streaming_status"] = "starting"
    
    # æŒ‰ç…§å®˜æ–¹æ¨¡å¼ï¼šåœ¨async withå†…å®Œæˆæ•´ä¸ªè¯·æ±‚å‘¨æœŸ
    async with AsyncPostgresSaver.from_conn_string(POSTGRES_CONNECTION_STRING) as checkpointer:
        await checkpointer.setup()
        graph = builder.compile(checkpointer=checkpointer, name="diagnostic-agent")
        
        # åœ¨åŒä¸€ä¸ªasync withå†…æ‰§è¡Œå®Œæ•´çš„æµå¼å¤„ç†
        async for item in stream_with_graph_postgres(graph, request_body, thread_id):
            yield item

async def handle_memory_streaming(request_body, thread_id):
    """å¤„ç†å†…å­˜æ¨¡å¼çš„æµå¼å“åº”"""
    assistant = ASSISTANTS[request_body.assistant_id]
    graph = assistant["graph"]
    
    # ä½¿ç”¨ç°æœ‰å›¾è¿›è¡Œæµå¼å¤„ç†
    async for item in stream_with_graph(graph, request_body, thread_id):
        yield item

async def stream_run_standard(thread_id: str, request_body: RunCreate):
    """Standard LangGraph streaming endpoint"""
    if thread_id not in threads_store:
        # å°è¯•ä»PostgreSQLæ¢å¤çº¿ç¨‹
        recovered = await recover_thread_from_postgres(thread_id)
        if not recovered:
            raise HTTPException(status_code=404, detail="Thread not found")
    if request_body.assistant_id not in ASSISTANTS: 
        raise HTTPException(status_code=400, detail="Invalid assistant_id")
    
    # åˆ›å»ºç”¨æˆ·çº¿ç¨‹å…³è”ï¼ˆå¦‚æœæä¾›äº†ç”¨æˆ·åä¸”å…³è”ä¸å­˜åœ¨ï¼‰
    # ç”¨æˆ·åå¯èƒ½åœ¨ request_body.user_name æˆ– request_body.input.user_name ä¸­
    user_name = None
    if request_body.user_name:
        user_name = request_body.user_name
    elif request_body.input and isinstance(request_body.input, dict) and "user_name" in request_body.input:
        user_name = request_body.input["user_name"]
    
    if user_name:
        logger.info(f"ğŸ” å¼€å§‹å¤„ç†ç”¨æˆ·çº¿ç¨‹å…³è”: {user_name} -> {thread_id}")
        try:
            exists = await check_user_thread_exists(user_name, thread_id)
            logger.info(f"ğŸ” æ£€æŸ¥ç”¨æˆ·çº¿ç¨‹æ˜¯å¦å­˜åœ¨: {exists}")
            if not exists:
                # å°è¯•ä»è¾“å…¥å†…å®¹ä¸­æå–æ ‡é¢˜
                thread_title = None
                if request_body.input and "messages" in request_body.input:
                    messages = request_body.input["messages"]
                    if messages and len(messages) > 0:
                        last_msg = messages[-1]
                        if isinstance(last_msg, dict) and "content" in last_msg:
                            content = str(last_msg["content"])
                            # å–å‰20ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
                            thread_title = content[:20] + "..." if len(content) > 20 else content
                
                logger.info(f"ğŸ” å‡†å¤‡åˆ›å»ºç”¨æˆ·çº¿ç¨‹å…³è”ï¼Œæ ‡é¢˜: {thread_title}")
                success = await create_user_thread_mapping(
                    user_name, 
                    thread_id, 
                    thread_title
                )
                if success:
                    logger.info(f"âœ… å·²åˆ›å»ºç”¨æˆ·çº¿ç¨‹å…³è”: {user_name} -> {thread_id}")
                else:
                    logger.warning(f"âŒ åˆ›å»ºç”¨æˆ·çº¿ç¨‹å…³è”å¤±è´¥: {user_name} -> {thread_id}")
            else:
                logger.info(f"â„¹ï¸ ç”¨æˆ·çº¿ç¨‹å…³è”å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º: {user_name} -> {thread_id}")
        except Exception as e:
            logger.error(f"å¤„ç†ç”¨æˆ·çº¿ç¨‹å…³è”æ—¶å‡ºé”™: {e}")
            # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
    else:
        logger.warning(f"âš ï¸ è¯·æ±‚ä¸­æ²¡æœ‰æä¾›ç”¨æˆ·åï¼Œè·³è¿‡ç”¨æˆ·çº¿ç¨‹å…³è”åˆ›å»º")

    async def generate():
        try:
            # æ ¹æ®åŠ©æ‰‹ç±»å‹å’Œcheckpointerç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥
            if should_use_postgres_mode(request_body.assistant_id):
                async for item in handle_postgres_streaming(request_body, thread_id):
                    yield item
            else:
                async for item in handle_memory_streaming(request_body, thread_id):
                    yield item
                
        except Exception as e:
            logger.error(f"Error in streaming: {e}")
            yield f"event: error\n"
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Content-Type": "text/event-stream"
        }
    )