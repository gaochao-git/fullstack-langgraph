"""
æ™®é€šé—®ç­”å­å›¾ - å¤„ç†è¿ç»´æŠ€æœ¯é—®ç­”å’Œæ—¥å¸¸å¯¹è¯
"""

import logging
from typing import Dict, Any, Literal
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode
from langchain_core.messages import AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig

from .configuration import Configuration
from .state import DiagnosticState
from .tools import all_tools
from .utils import extract_diagnosis_results_from_messages

logger = logging.getLogger(__name__)


def analyze_question_context_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """
    åˆ†æé—®é¢˜ä¸Šä¸‹æ–‡èŠ‚ç‚¹ - ç†è§£ç”¨æˆ·é—®é¢˜å¹¶å‡†å¤‡å›ç­”
    """
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: analyze_question_context_node")
    print(f"ğŸ” analyze_question_context_node - è¾“å…¥çŠ¶æ€: {list(state.keys())}")
    
    messages = state.get("messages", [])
    print(f"ğŸ” analyze_question_context_node - æ¶ˆæ¯æ•°é‡: {len(messages)}")
    
    if not messages:
        print(f"ğŸ” analyze_question_context_node - æ— æ¶ˆæ¯ï¼Œè¿”å›é»˜è®¤ä¸Šä¸‹æ–‡")
        return {"qa_context": "æ— å†å²å¯¹è¯"}
    
    # è·å–ç”¨æˆ·é—®é¢˜
    user_question = messages[-1].content if messages else ""
    print(f"ğŸ” analyze_question_context_node - ç”¨æˆ·é—®é¢˜: {user_question}")
    
    # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
    context_parts = []
    
    # æ·»åŠ è¯Šæ–­å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    diagnosis_results = extract_diagnosis_results_from_messages(messages, max_results=3)
    print(f"ğŸ” analyze_question_context_node - è¯Šæ–­å†å²æ•°é‡: {len(diagnosis_results)}")
    if diagnosis_results:
        context_parts.append("ç›¸å…³è¯Šæ–­å†å²ï¼š")
        context_parts.extend(diagnosis_results[:3])  # æœ€è¿‘3ä¸ªè¯Šæ–­ç»“æœ
    
    # æ·»åŠ æœ€è¿‘å¯¹è¯
    if len(messages) > 1:
        context_parts.append("\næœ€è¿‘å¯¹è¯ï¼š")
        recent_messages = messages[-6:] if len(messages) > 6 else messages[:-1]
        print(f"ğŸ” analyze_question_context_node - æœ€è¿‘å¯¹è¯æ•°é‡: {len(recent_messages)}")
        for i, msg in enumerate(recent_messages):
            role = "ç”¨æˆ·" if i % 2 == 0 else "åŠ©æ‰‹"
            content = getattr(msg, 'content', str(msg))[:150]  # é™åˆ¶é•¿åº¦
            context_parts.append(f"{role}: {content}")
    
    qa_context = "\n".join(context_parts) if context_parts else "æ— å†å²å¯¹è¯"
    print(f"ğŸ” analyze_question_context_node - æ„å»ºçš„ä¸Šä¸‹æ–‡é•¿åº¦: {len(qa_context)}")
    
    logger.info(f"é—®ç­”ä¸Šä¸‹æ–‡åˆ†æå®Œæˆï¼Œå†å²è¯Šæ–­: {len(diagnosis_results)}, å¯¹è¯è½®æ¬¡: {len(messages)}")
    
    result = {
        "qa_context": qa_context,
        "user_question": user_question
    }
    print(f"ğŸ” analyze_question_context_node - è¿”å›ç»“æœ: {list(result.keys())}")
    print(f"ğŸ” analyze_question_context_node - qa_context: {result['qa_context'][:100]}...")
    print(f"ğŸ” analyze_question_context_node - user_question: {result['user_question']}")
    
    return result


def plan_qa_tools_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """
    é—®ç­”å·¥å…·è§„åˆ’èŠ‚ç‚¹ - è®©LLMè‡ªå·±å†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    """
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: plan_qa_tools_node")
    
    configurable = Configuration.from_runnable_config(config)
    user_question = state.get("user_question", "")
    qa_context = state.get("qa_context", "")
    messages = state.get("messages", [])
    

    
    # å¦‚æœæ²¡æœ‰ç”¨æˆ·é—®é¢˜ï¼Œä»æ¶ˆæ¯ä¸­è·å–
    if not user_question and messages:
        user_question = messages[-1].content if messages else ""
        print(f"ğŸ” plan_qa_tools_node - ä»æ¶ˆæ¯ä¸­è·å–ç”¨æˆ·é—®é¢˜: {user_question}")
    
    # åˆ›å»ºå¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆåªåŒ…å«å®‰å…¨çš„æŸ¥è¯¢å·¥å…·ï¼‰
    # æ™®é€šé—®ç­”åŠ©æ‰‹åªéœ€è¦ï¼šé€šç”¨å·¥å…·ï¼ˆå¦‚æ—¶é—´æŸ¥è¯¢ï¼‰
    # ä¸åŒ…å«SSHã€MySQLã€Elasticsearchã€Zabbixç­‰ç³»ç»Ÿè¯Šæ–­å·¥å…·
    # ä¹Ÿä¸åŒ…å«SOPå·¥å…·ï¼Œå› ä¸ºSOPæ˜¯è¯Šæ–­ä¸“ç”¨çš„
    available_tools = all_tools
    
    # åˆ›å»ºå¸¦å·¥å…·çš„LLM
    llm = configurable.create_llm(
        model_name=configurable.query_generator_model,
        temperature=0.3
    )
    llm_with_tools = llm.bind_tools(available_tools)
    print(f"ğŸ” plan_qa_tools_node - LLMæ¨¡å‹: {configurable.query_generator_model}")
    
    # æ„å»ºå·¥å…·è§„åˆ’æç¤º - è®©LLMè‡ªå·±å†³å®šæ˜¯å¦éœ€è¦å·¥å…·
    tool_planning_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¿ç»´åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œè‡ªä¸»å†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·æ¥è·å–ä¿¡æ¯ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯ä¸Šä¸‹æ–‡ï¼š
{qa_context}

è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼š
1. å¦‚æœéœ€è¦è·å–å®æ—¶ä¿¡æ¯ï¼ˆå¦‚å½“å‰æ—¶é—´ã€SOPæ–‡æ¡£å†…å®¹ç­‰ï¼‰ï¼Œè¯·è°ƒç”¨ç›¸åº”å·¥å…·
2. å¦‚æœæ˜¯æ¦‚å¿µæ€§é—®é¢˜ã€æŠ€æœ¯è§£é‡Šæˆ–ä¸éœ€è¦å®æ—¶æ•°æ®ï¼Œè¯·ç›´æ¥å›ç­”ï¼Œä¸è¦è°ƒç”¨å·¥å…·
3. å¦‚æœä¸ç¡®å®šï¼Œä¼˜å…ˆå°è¯•è°ƒç”¨å·¥å…·è·å–å‡†ç¡®ä¿¡æ¯

æ³¨æ„ï¼šåªè°ƒç”¨å®‰å…¨çš„æŸ¥è¯¢å·¥å…·ï¼Œä¸è¦æ‰§è¡Œä»»ä½•å¯èƒ½å½±å“ç³»ç»Ÿçš„æ“ä½œã€‚
"""
    
    # æ„å»ºæ¶ˆæ¯
    system_message = SystemMessage(content=tool_planning_prompt)
    messages_with_system = [system_message] + messages
    
    # è°ƒç”¨LLMç”Ÿæˆå·¥å…·è°ƒç”¨
    response = llm_with_tools.invoke(messages_with_system)
    
    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å·¥å…·è°ƒç”¨
    has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
    if hasattr(response, 'tool_calls'):
        print(f"ğŸ” plan_qa_tools_node - tool_callså€¼: {response.tool_calls}")
    
    if has_tool_calls:
        for i, tool_call in enumerate(response.tool_calls):
            print(f"  å·¥å…·è°ƒç”¨ {i+1}: {tool_call.get('name', 'unknown')} - {tool_call.get('args', {})}")
    else:
        print(f"  LLMå›ç­”: {response.content[:200]}...")
        
    result = {
        "messages": [response]
    }
    
    return result


def generate_answer_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """
    ç”Ÿæˆå›ç­”èŠ‚ç‚¹ - åŸºäºç”¨æˆ·é—®é¢˜å’Œä¸Šä¸‹æ–‡ç”Ÿæˆä¸“ä¸šå›ç­”
    """
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: generate_answer_node")
    print(f"ğŸ” generate_answer_node - è¾“å…¥çŠ¶æ€: {list(state.keys())}")
    
    configurable = Configuration.from_runnable_config(config)
    
    # è·å–çŠ¶æ€ä¿¡æ¯
    user_question = state.get("user_question", "")
    qa_context = state.get("qa_context", "")
    messages = state.get("messages", [])
    
    print(f"ğŸ” generate_answer_node - user_question: {user_question}")
    print(f"ğŸ” generate_answer_node - qa_context: {qa_context[:100]}...")
    print(f"ğŸ” generate_answer_node - messagesæ•°é‡: {len(messages)}")
    
    # å¦‚æœæ²¡æœ‰ç”¨æˆ·é—®é¢˜ï¼Œä»æ¶ˆæ¯ä¸­è·å–
    if not user_question and messages:
        user_question = messages[-1].content if messages else ""
        print(f"ğŸ” generate_answer_node - ä»æ¶ˆæ¯ä¸­è·å–ç”¨æˆ·é—®é¢˜: {user_question}")
    
    # åˆ›å»ºLLMå®ä¾‹
    llm = configurable.create_llm(
        model_name=configurable.answer_model,
        temperature=configurable.final_report_temperature
    )
    print(f"ğŸ” generate_answer_node - LLMæ¨¡å‹: {configurable.answer_model}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·ç»“æœ
    tool_results = []
    print(f"ğŸ” æ£€æŸ¥æ¶ˆæ¯ä¸­çš„å·¥å…·ç»“æœï¼Œæ€»æ¶ˆæ¯æ•°: {len(messages)}")
    for i, msg in enumerate(messages):
        print(f"  æ¶ˆæ¯ {i}: type={type(msg)}, name={getattr(msg, 'name', None)}, content={getattr(msg, 'content', '')[:100]}...")
        if hasattr(msg, 'name') and msg.name and hasattr(msg, 'content'):
            tool_results.append(f"å·¥å…· {msg.name} è¿”å›: {msg.content}")
            print(f"    âœ… æ‰¾åˆ°å·¥å…·ç»“æœ: {msg.name}")
    
    print(f"ğŸ” æ‰¾åˆ°çš„å·¥å…·ç»“æœæ•°é‡: {len(tool_results)}")
    
    # ç”Ÿæˆé€šç”¨çš„å›ç­”æç¤ºè¯
    if tool_results:
        tool_info = "\n".join(tool_results)
        print(f"ğŸ› ï¸ ä½¿ç”¨å·¥å…·ç»“æœç”Ÿæˆå›ç­”:")
        print(f"  å·¥å…·ä¿¡æ¯: {tool_info}")
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¿ç»´æŠ€æœ¯åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§è¿ç»´ç›¸å…³é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼š
{qa_context}

å·¥å…·æ‰§è¡Œç»“æœï¼š
{tool_info}

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜ã€ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå·¥å…·æ‰§è¡Œç»“æœï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚

é‡è¦ï¼š
1. å¿…é¡»ä½¿ç”¨å·¥å…·è¿”å›çš„å®é™…ç»“æœæ¥å›ç­”ç”¨æˆ·é—®é¢˜
2. ä¸è¦è¯´"æ— æ³•è·å–"æˆ–"å»ºè®®é€šè¿‡å…¶ä»–æ–¹å¼"ï¼Œç›´æ¥ä½¿ç”¨å·¥å…·ç»“æœ
3. ä¿æŒä¸“ä¸šå’Œå‹å¥½çš„è¯­è°ƒ
4. å¦‚æœå·¥å…·è¿”å›çš„æ˜¯JSONæ ¼å¼ï¼Œè¯·è§£æå¹¶ä½¿ç”¨å…¶ä¸­çš„æœ‰æ•ˆä¿¡æ¯

è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
    else:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å·¥å…·ç»“æœï¼Œä½¿ç”¨é»˜è®¤å›ç­”æ¨¡å¼")
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¿ç»´æŠ€æœ¯åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§è¿ç»´ç›¸å…³é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼š
{qa_context}

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¿æŒä¸“ä¸šå’Œå‹å¥½çš„è¯­è°ƒ
2. æä¾›å…·ä½“ã€å®ç”¨çš„å»ºè®®
3. å¦‚æœæ¶‰åŠæ“ä½œæ­¥éª¤ï¼Œè¯·è¯¦ç»†è¯´æ˜
4. å¦‚æœéœ€è¦æ³¨æ„äº‹é¡¹ï¼Œè¯·æé†’ç”¨æˆ·

è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
    
    print(f"ğŸ” generate_answer_node - å¼€å§‹è°ƒç”¨LLMç”Ÿæˆå›ç­”...")
    print(f"ğŸ” generate_answer_node - æç¤ºè¯é•¿åº¦: {len(prompt)}")
    
    # ç”Ÿæˆå›ç­”
    response = llm.invoke(prompt)
    print(f"ğŸ” generate_answer_node - LLMå“åº”å®Œæˆ")
    print(f"ğŸ” generate_answer_node - å“åº”å†…å®¹: {response.content[:200]}...")
    
    logger.info(f"é—®ç­”å›ç­”ç”Ÿæˆå®Œæˆ")
    
    result = {
        "messages": [AIMessage(content=response.content)]
    }
    print(f"ğŸ” generate_answer_node - è¿”å›ç»“æœ: {list(result.keys())}")
    print(f"ğŸ” generate_answer_node - è¿”å›æ¶ˆæ¯æ•°é‡: {len(result['messages'])}")
    
    return result


def determine_qa_type(user_question: str, qa_context: str, config: RunnableConfig) -> str:
    """
    ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­é—®ç­”ç±»å‹
    """
    configurable = Configuration.from_runnable_config(config)
    
    # åˆ›å»ºLLM
    llm = configurable.create_llm(
        model_name=configurable.query_generator_model,
        temperature=0.1
    )
    
    # æ„å»ºåˆ¤æ–­æç¤ºè¯
    classification_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”ç±»å‹åˆ†ç±»å™¨ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­å±äºä»¥ä¸‹å“ªç§ç±»å‹ï¼š

1. **technical_qa** - æŠ€æœ¯é—®ç­”ï¼š
   - è¯¢é—®æŠ€æœ¯çŸ¥è¯†ã€æ“ä½œæ–¹æ³•ã€æ¦‚å¿µè§£é‡Š
   - è¯¢é—®å¦‚ä½•é…ç½®ã€å®‰è£…ã€éƒ¨ç½²ã€ç›‘æ§ç­‰
   - è¯¢é—®å‘½ä»¤ç”¨æ³•ã€è„šæœ¬ç¼–å†™ã€æ¶æ„è®¾è®¡ç­‰
   - ä¾‹å¦‚ï¼š"å¦‚ä½•é…ç½®nginx"ã€"ä»€ä¹ˆæ˜¯docker"ã€"æ€ä¹ˆä¼˜åŒ–æ•°æ®åº“æ€§èƒ½"

2. **system_query** - ç³»ç»ŸæŸ¥è¯¢ï¼š
   - éœ€è¦æŸ¥è¯¢å®æ—¶ä¿¡æ¯ã€ç³»ç»ŸçŠ¶æ€ã€é…ç½®ä¿¡æ¯
   - éœ€è¦æœç´¢æ–‡æ¡£ã€SOPã€å†å²è®°å½•
   - éœ€è¦è·å–å½“å‰æ—¶é—´ã€ç‰ˆæœ¬ä¿¡æ¯ã€ç»Ÿè®¡æ•°æ®
   - ä¾‹å¦‚ï¼š"ç°åœ¨å‡ ç‚¹äº†"ã€"æŸ¥è¯¢SOPæ–‡æ¡£"ã€"æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"ã€"æœç´¢ç›¸å…³æ–‡æ¡£"

3. **follow_up** - åç»­é—®é¢˜ï¼š
   - å¯¹ä¹‹å‰å›ç­”çš„è¿½é—®å’Œè¡¥å……
   - è¦æ±‚æ›´è¯¦ç»†è§£é‡Šæˆ–ç›¸å…³å»¶ä¼¸
   - åŸºäºå†å²å¯¹è¯çš„ç»§ç»­è®¨è®º
   - ä¾‹å¦‚ï¼š"è¯¦ç»†è¯´æ˜ä¸€ä¸‹"ã€"è¿˜æœ‰å…¶ä»–æ–¹æ³•å—"ã€"ä¸ºä»€ä¹ˆä¼šè¿™æ ·"

4. **casual_chat** - æ—¥å¸¸èŠå¤©ï¼š
   - é—®å€™ã€æ„Ÿè°¢ã€é—²èŠ
   - ä¸æ¶‰åŠæŠ€æœ¯å†…å®¹çš„å¯¹è¯
   - ä¾‹å¦‚ï¼š"ä½ å¥½"ã€"è°¢è°¢"ã€"å†è§"

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯å†å²ï¼š{qa_context}

è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåªè¿”å›ç±»å‹åç§°ï¼štechnical_qaã€system_queryã€follow_upã€casual_chat
"""
    
    # è°ƒç”¨LLMåˆ†ç±»
    try:
        result = llm.invoke(classification_prompt)
        qa_type = result.content.strip().lower()
        
        # ç¡®ä¿è¿”å›æœ‰æ•ˆç±»å‹
        valid_types = ["technical_qa", "system_query", "follow_up", "casual_chat"]
        if qa_type in valid_types:
            return qa_type
        else:
            # å¦‚æœLLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆç±»å‹ï¼Œä½¿ç”¨ç®€å•çš„fallbacké€»è¾‘
            logger.warning(f"LLMè¿”å›æ— æ•ˆç±»å‹: {qa_type}ï¼Œä½¿ç”¨fallbacké€»è¾‘")
            return "technical_qa"  # é»˜è®¤ä¸ºæŠ€æœ¯é—®ç­”
            
    except Exception as e:
        logger.error(f"LLMåˆ†ç±»å¤±è´¥: {e}ï¼Œä½¿ç”¨fallbacké€»è¾‘")
        return "technical_qa"  # é»˜è®¤ä¸ºæŠ€æœ¯é—®ç­”


def check_qa_tool_calls(state: DiagnosticState, config: RunnableConfig) -> Literal["execute_tools", "END"]:
    """æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨éœ€è¦æ‰§è¡Œï¼Œå¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ä¸”å·²æœ‰å›å¤åˆ™ç›´æ¥ç»“æŸ"""
    print(f"âœ… æ‰§è¡Œè·¯ç”±å‡½æ•°: check_qa_tool_calls")
    
    messages = state.get("messages", [])
    print(f"ğŸ” è·¯ç”±æ£€æŸ¥ - æ¶ˆæ¯æ€»æ•°: {len(messages)}")
    
    if not messages:
        print(f"âŒ æ²¡æœ‰æ¶ˆæ¯ï¼Œç›´æ¥ç»“æŸ")
        return "END"
    
    last_message = messages[-1]
    print(f"ğŸ” æœ€åä¸€æ¡æ¶ˆæ¯ç±»å‹: {type(last_message)}")
    print(f"ğŸ” æ¶ˆæ¯å†…å®¹: {getattr(last_message, 'content', 'N/A')[:100]}...")
    
    has_tool_calls = hasattr(last_message, 'tool_calls') and last_message.tool_calls
    has_content = hasattr(last_message, 'content') and last_message.content.strip()
    
    print(f"ğŸ” hasattr(tool_calls): {hasattr(last_message, 'tool_calls')}")
    print(f"ğŸ” has_content: {has_content}")
    if hasattr(last_message, 'tool_calls'):
        print(f"ğŸ” tool_callså€¼: {last_message.tool_calls}")
    
    if has_tool_calls:
        print(f"âœ… æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œæ•°é‡: {len(last_message.tool_calls)}")
        logger.info(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œæ•°é‡: {len(last_message.tool_calls)}")
        return "execute_tools"
    elif has_content:
        print(f"âœ… æ— å·¥å…·è°ƒç”¨ä½†æœ‰å›å¤å†…å®¹ï¼Œç›´æ¥ç»“æŸ")
        logger.info("æ— å·¥å…·è°ƒç”¨ä½†æœ‰å›å¤å†…å®¹ï¼Œç›´æ¥ç»“æŸ")
        return "END"
    else:
        print(f"âŒ æ— å·¥å…·è°ƒç”¨ä¹Ÿæ— å›å¤å†…å®¹ï¼Œç›´æ¥ç»“æŸ")
        logger.info("æ— å·¥å…·è°ƒç”¨ä¹Ÿæ— å›å¤å†…å®¹ï¼Œç›´æ¥ç»“æŸ")
        return "END"


def generate_technical_qa_prompt(user_question: str, qa_context: str) -> str:
    """ç”ŸæˆæŠ€æœ¯é—®ç­”æç¤ºè¯"""
    return f"""æ‚¨æ˜¯ä¸“ä¸šçš„è¿ç»´æŠ€æœ¯ä¸“å®¶ï¼Œè¯·å›ç­”ç”¨æˆ·çš„æŠ€æœ¯é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯ä¸Šä¸‹æ–‡ï¼š
{qa_context}

è¯·æä¾›ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
2. å¦‚æœæ¶‰åŠæ“ä½œï¼Œæä¾›å…·ä½“æ­¥éª¤
3. å¦‚æœæœ‰é£é™©ï¼Œæé†’æ³¨æ„äº‹é¡¹
4. å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œä¸»åŠ¨è¯¢é—®

å›ç­”è¦æ±‚ï¼š
- ä¸“ä¸šå‡†ç¡®ï¼Œç®€æ´æ˜äº†
- æä¾›å®ç”¨çš„è§£å†³æ–¹æ¡ˆ
- åŒ…å«å…·ä½“çš„å‘½ä»¤æˆ–é…ç½®ç¤ºä¾‹ï¼ˆå¦‚é€‚ç”¨ï¼‰
- é¿å…è¿‡äºå¤æ‚çš„æœ¯è¯­è§£é‡Š
"""


def generate_system_query_prompt(user_question: str, qa_context: str) -> str:
    """ç”Ÿæˆç³»ç»ŸæŸ¥è¯¢æç¤ºè¯"""
    return f"""æ‚¨æ˜¯è¿ç»´ç³»ç»ŸåŠ©æ‰‹ï¼Œç”¨æˆ·æƒ³è¦æŸ¥è¯¢ç³»ç»Ÿä¿¡æ¯ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯ä¸Šä¸‹æ–‡ï¼š
{qa_context}

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼š
1. å¦‚æœæœ‰ç›¸å…³å†å²ä¿¡æ¯ï¼Œæä¾›æ‘˜è¦
2. å¦‚æœéœ€è¦å®æ—¶æŸ¥è¯¢ï¼Œè¯´æ˜æŸ¥è¯¢æ–¹æ³•
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯¢é—®æ›´å¤šç»†èŠ‚

å›ç­”è¦æ±‚ï¼š
- ç›´æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢
- æä¾›å…·ä½“çš„æŸ¥è¯¢å‘½ä»¤æˆ–æ–¹æ³•
- å¦‚æœæ¶‰åŠå†å²è¯Šæ–­ï¼Œå¼•ç”¨ç›¸å…³ç»“æœ
"""


def generate_follow_up_prompt(user_question: str, qa_context: str) -> str:
    """ç”Ÿæˆåç»­é—®é¢˜æç¤ºè¯"""
    return f"""ç”¨æˆ·å¯¹ä¹‹å‰çš„å¯¹è¯æœ‰åç»­é—®é¢˜ï¼Œè¯·åŸºäºä¸Šä¸‹æ–‡æä¾›è¯¦ç»†å›ç­”ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯ä¸Šä¸‹æ–‡ï¼š
{qa_context}

è¯·ï¼š
1. ç»“åˆä¹‹å‰çš„å¯¹è¯å†…å®¹å›ç­”
2. æä¾›æ›´è¯¦ç»†çš„è§£é‡Šæˆ–è¡¥å……ä¿¡æ¯
3. å¦‚æœç”¨æˆ·é—®é¢˜ä¸å¤Ÿæ¸…æ¥šï¼Œä¸»åŠ¨æ¾„æ¸…

å›ç­”è¦æ±‚ï¼š
- è¿è´¯æ€§å¼ºï¼Œä¸ä¹‹å‰å¯¹è¯å‘¼åº”
- æä¾›å…·ä½“çš„è¡¥å……ä¿¡æ¯
- ä¿æŒä¸“ä¸šå’Œå‹å¥½çš„è¯­è°ƒ
"""


def generate_casual_chat_prompt(user_question: str, qa_context: str) -> str:
    """ç”Ÿæˆæ—¥å¸¸èŠå¤©æç¤ºè¯"""
    return f"""æ‚¨æ˜¯å‹å¥½çš„è¿ç»´åŠ©æ‰‹ï¼Œç”¨æˆ·åœ¨è¿›è¡Œæ—¥å¸¸å¯¹è¯ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯ä¸Šä¸‹æ–‡ï¼š
{qa_context}

è¯·ï¼š
1. è‡ªç„¶å‹å¥½åœ°å›åº”ç”¨æˆ·
2. å¦‚æœæ¶‰åŠè¿ç»´ç›¸å…³å†…å®¹ï¼Œæä¾›ç®€å•è¯´æ˜
3. ä¿æŒä¸“ä¸šä½†è½»æ¾çš„è¯­è°ƒ

å›ç­”è¦æ±‚ï¼š
- ç®€æ´è‡ªç„¶
- å‹å¥½äº²åˆ‡
- å¦‚æœåˆé€‚ï¼Œå¯ä»¥è¯¢é—®æ˜¯å¦éœ€è¦æŠ€æœ¯å¸®åŠ©
"""


def create_general_qa_subgraph():
    """åˆ›å»ºæ™®é€šé—®ç­”å­å›¾"""
    
    # åˆ›å»ºå·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
    # æ™®é€šé—®ç­”åŠ©æ‰‹åªä½¿ç”¨é€šç”¨å·¥å…·ï¼ˆå¦‚æ—¶é—´æŸ¥è¯¢ï¼‰
    # ä¸åŒ…å«SSHã€MySQLã€Elasticsearchã€Zabbixç­‰ç³»ç»Ÿè¯Šæ–­å·¥å…·
    # ä¹Ÿä¸åŒ…å«SOPå·¥å…·ï¼Œå› ä¸ºSOPæ˜¯è¯Šæ–­ä¸“ç”¨çš„
    qa_safe_tools = all_tools
    tool_node = ToolNode(qa_safe_tools)
    
    # åŒ…è£…å·¥å…·èŠ‚ç‚¹ä»¥æ·»åŠ æ‰“å°
    def execute_qa_tools_node(state, config):
        print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: execute_qa_tools_node")
        print(f"ğŸ” execute_qa_tools_node - è¾“å…¥çŠ¶æ€: {list(state.keys())}")
        
        messages = state.get("messages", [])
        print(f"ğŸ” execute_qa_tools_node - messagesæ•°é‡: {len(messages)}")
        
        # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if messages:
            last_message = messages[-1]
            print(f"ğŸ” execute_qa_tools_node - æœ€åä¸€æ¡æ¶ˆæ¯ç±»å‹: {type(last_message)}")
            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                print(f"ğŸ” execute_qa_tools_node - æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨æ•°é‡: {len(last_message.tool_calls)}")
                for i, tool_call in enumerate(last_message.tool_calls):
                    print(f"  å·¥å…·è°ƒç”¨ {i+1}: {tool_call.get('name', 'unknown')} - {tool_call.get('args', {})}")
            else:
                print(f"ğŸ” execute_qa_tools_node - æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
        
        print(f"ğŸ” execute_qa_tools_node - å¼€å§‹æ‰§è¡Œå·¥å…·...")
        result = tool_node.invoke(state, config)
        print(f"ğŸ” execute_qa_tools_node - å·¥å…·æ‰§è¡Œå®Œæˆ")
        print(f"ğŸ” execute_qa_tools_node - è¿”å›ç»“æœ: {list(result.keys())}")
        
        if "messages" in result:
            print(f"ğŸ” execute_qa_tools_node - è¿”å›æ¶ˆæ¯æ•°é‡: {len(result['messages'])}")
            for i, msg in enumerate(result["messages"]):
                print(f"  è¿”å›æ¶ˆæ¯ {i}: type={type(msg)}, name={getattr(msg, 'name', None)}, content={getattr(msg, 'content', '')[:100]}...")
        
        return result
    
    # åˆ›å»ºå­å›¾
    builder = StateGraph(DiagnosticState, config_schema=Configuration)
    
    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("analyze_context", analyze_question_context_node)
    builder.add_node("plan_tools", plan_qa_tools_node)
    builder.add_node("execute_tools", execute_qa_tools_node)
    builder.add_node("generate_answer", generate_answer_node)
    
    # è®¾ç½®æµç¨‹
    builder.add_edge(START, "analyze_context")
    builder.add_edge("analyze_context", "plan_tools")
    builder.add_conditional_edges("plan_tools", check_qa_tool_calls, {"execute_tools": "execute_tools", "END": END})
    builder.add_edge("execute_tools", "generate_answer")
    builder.add_edge("generate_answer", END)
    return builder.compile()