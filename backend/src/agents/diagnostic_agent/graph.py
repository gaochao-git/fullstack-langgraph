"""
æ•…éšœè¯Šæ–­ä»£ç† - ä½¿ç”¨LangGraphæ¶æ„ï¼ŒåŸºäºSOPçŸ¥è¯†åº“å’Œæ™ºèƒ½å·¥å…·é€‰æ‹©
é‡æ„ç‰ˆæœ¬ï¼šå‚è€ƒè°ƒç ”agentçš„ç»“æ„ï¼Œä¼˜åŒ–çŠ¶æ€ç®¡ç†å’ŒèŠ‚ç‚¹èŒè´£
"""

import os
import json
import logging
from typing import Dict, Any
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode
from langgraph.types import interrupt
from langchain_core.messages import SystemMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from .configuration import Configuration
from .state import (DiagnosticState,QuestionAnalysis,DiagnosisProgress,SOPDetail,SOPStep)
from .prompts import (get_current_datetime,get_question_analysis_prompt,get_missing_info_prompt,tool_planning_instructions,diagnosis_report_instructions,reflection_instructions)
from .schemas import QuestionInfoExtraction, DiagnosisReflectionOutput
from .tools import all_tools
from .utils import (merge_field, check_approval_needed, is_already_approved,process_sop_loading, update_diagnosis_step, check_diagnosis_completion,check_info_sufficient, check_tool_calls, save_graph_image, compile_graph_with_checkpointer, extract_diagnosis_results_from_messages, format_diagnosis_results_for_prompt, is_sop_loaded)
logger = logging.getLogger(__name__)


# èŠ‚ç‚¹å‡½æ•° - å‚è€ƒè°ƒç ”agentçš„æ¸…æ™°ç»“æ„
def analyze_question_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """é—®é¢˜åˆ†æèŠ‚ç‚¹ - æ”¯æŒå¤šè½®è¡¥å……å››è¦ç´ """
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: analyze_question_node")
    configurable = Configuration.from_runnable_config(config)
    
    messages = state.get("messages", [])
    user_question = messages[-1].content if messages else ""
    report_generated = state.get("report_generated", False)
    
    
    # å››è¦ç´ åˆ†ææµç¨‹
    llm = configurable.create_llm(model_name=configurable.query_generator_model,temperature=configurable.question_analysis_temperature)
    # è·å–å½“å‰å·²æœ‰çš„å››è¦ç´ ä¿¡æ¯
    current_analysis = state.get("question_analysis", QuestionAnalysis())
    # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿å‡½æ•°ç”Ÿæˆæç¤ºè¯
    prompt = get_question_analysis_prompt(user_question, current_analysis)
    
    # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
    # æ³¨æ„ï¼šé…åˆ schemas.py ä¸­çš„ pydantic_v1 ä½¿ç”¨
    # è¿™æ · LangChain ä¼šè‡ªåŠ¨é™çº§ä½¿ç”¨æç¤ºè¯æ–¹å¼è€Œä¸æ˜¯ response_format
    # é¿å… DeepSeek API çš„å…¼å®¹æ€§é—®é¢˜
    structured_llm = llm.with_structured_output(QuestionInfoExtraction)
    result = structured_llm.invoke(prompt)
    merged_analysis = QuestionAnalysis(
        fault_ip=merge_field(result.fault_ip, current_analysis.fault_ip),
        fault_time=merge_field(result.fault_time, current_analysis.fault_time, "fault_time"),
        fault_info=merge_field(result.fault_info, current_analysis.fault_info),
        sop_id=merge_field(result.sop_id, current_analysis.sop_id)
    )
    
    # æ£€æŸ¥å››è¦ç´ æ˜¯å¦éƒ½å®Œæ•´
    info_sufficient = (
        merged_analysis.fault_ip and merged_analysis.fault_ip != "å¾…æå–" and
        merged_analysis.fault_time and merged_analysis.fault_time != "å¾…æå–" and
        merged_analysis.fault_info and merged_analysis.fault_info != "å¾…æå–" and
        merged_analysis.sop_id and merged_analysis.sop_id != "å¾…æå–"
    )
    # ç”Ÿæˆç¼ºå¤±å­—æ®µåˆ—è¡¨
    missing_fields = []
    if not merged_analysis.fault_ip or merged_analysis.fault_ip == "å¾…æå–": missing_fields.append("æ•…éšœIP")
    if not merged_analysis.fault_time or merged_analysis.fault_time == "å¾…æå–": missing_fields.append("æ•…éšœæ—¶é—´")
    if not merged_analysis.fault_info or merged_analysis.fault_info == "å¾…æå–": missing_fields.append("æ•…éšœç°è±¡")
    if not merged_analysis.sop_id or merged_analysis.sop_id == "å¾…æå–": missing_fields.append("æ’æŸ¥SOPç¼–å·")
    merged_analysis.missing_fields = missing_fields
    merged_analysis.info_sufficient = info_sufficient
    logger.info(f"å››è¦ç´ åˆ†æ: å……è¶³={info_sufficient}, ç¼ºå¤±={missing_fields}")
    
    return {
        "question_analysis": merged_analysis
    }


def plan_diagnosis_tools_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """å·¥å…·è§„åˆ’èŠ‚ç‚¹ - ä¸¥æ ¼æŒ‰ç…§SOPæ‰§è¡Œ"""
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: plan_diagnosis_tools_node")
    configurable = Configuration.from_runnable_config(config)
    llm = configurable.create_llm(model_name=configurable.query_generator_model,temperature=configurable.tool_planning_temperature)
    # ç»‘å®šå·¥å…·åˆ°LLM
    llm_with_tools = llm.bind_tools(all_tools)
    # æ„å»ºå·¥å…·è§„åˆ’æç¤º
    question_analysis = state.get("question_analysis", QuestionAnalysis())
    sop_detail = state.get("sop_detail", SOPDetail())
    sop_state = "loaded" if is_sop_loaded(sop_detail) else "none"
    
    formatted_prompt = tool_planning_instructions.format(
        fault_ip=question_analysis.fault_ip or "",
        fault_time=question_analysis.fault_time or "",
        fault_info=question_analysis.fault_info or "",
        sop_id=question_analysis.sop_id or "",
        sop_state=sop_state,
        sop_content=sop_detail.description if sop_detail else ""
    )

    # æ„å»ºæ¶ˆæ¯
    messages = state.get("messages", [])
    system_message = SystemMessage(content=formatted_prompt)
    messages_with_system = [system_message] + messages
    # è°ƒç”¨LLMç”Ÿæˆå·¥å…·è°ƒç”¨
    response = llm_with_tools.invoke(messages_with_system)
    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å·¥å…·è°ƒç”¨
    has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
    logger.info(f"å·¥å…·è§„åˆ’ç»“æœ: ç”Ÿæˆäº† {len(response.tool_calls) if has_tool_calls else 0} ä¸ªå·¥å…·è°ƒç”¨")
    if not has_tool_calls: logger.warning("LLMæ²¡æœ‰ç”Ÿæˆä»»ä½•å·¥å…·è°ƒç”¨ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯Šæ–­æå‰ç»“æŸ")
    # è¿”å›æ–°çš„æ¶ˆæ¯ï¼ŒLangGraphä¼šå°†å…¶æ·»åŠ åˆ°çŠ¶æ€ä¸­
    return {"messages": [response]}


def approval_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """
    SOPæ‰§è¡Œç¡®è®¤èŠ‚ç‚¹ - ç®€åŒ–ç‰ˆæœ¬
    1. æ£€æŸ¥æ˜¯å¦éœ€è¦å®¡æ‰¹
    2. æ£€æŸ¥æ˜¯å¦å·²å®¡æ‰¹è¿‡  
    3. æ‰§è¡Œå®¡æ‰¹æµç¨‹
    """
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: approval_node")
    # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦å®¡æ‰¹
    approval_info = check_approval_needed(state)
    if not approval_info: return {}  # æ— éœ€å®¡æ‰¹ï¼Œç›´æ¥ç»§ç»­
    
    # 2. æ£€æŸ¥æ˜¯å¦å·²å®¡æ‰¹è¿‡
    if is_already_approved(state, approval_info):
        logger.info(f"æ­¥éª¤å·²å®¡æ‰¹è¿‡ï¼Œè·³è¿‡: {approval_info['step_id']}")
        return {}  # å·²å®¡æ‰¹ï¼Œç›´æ¥ç»§ç»­
    
    # 3. æ‰§è¡Œå®¡æ‰¹æµç¨‹
    step_info = approval_info["step_info"]
    step_id = approval_info["step_id"]
    tool_calls = approval_info["tool_calls"]
    sop_id = approval_info["sop_id"]
    
    logger.info(f"è§¦å‘å®¡æ‰¹æµç¨‹: SOP {sop_id}, æ­¥éª¤: {step_info.action}")
    
    # æ„å»ºå·¥å…·æè¿°
    tool_descriptions = [
        f"å·¥å…·: {tc.get('name', '')}, å‚æ•°: {tc.get('args', {})}"
        for tc in tool_calls
    ]
    
    # ä¸­æ–­å¹¶è¯·æ±‚ç”¨æˆ·ç¡®è®¤
    interrupt_info = {
        "message": f"æŒ‰ç…§SOP '{sop_id}' è¦æ±‚ï¼Œå³å°†æ‰§è¡Œéœ€è¦å®¡æ‰¹çš„æ­¥éª¤:\n\n"
                   f"**æ­¥éª¤è¯¦æƒ…:** {step_info.action}\n"
                   f"**è®¡åˆ’æ“ä½œ:**\n" + "\n".join(tool_descriptions) +
                   f"\n\nç¡®è®¤æ‰§è¡Œï¼Ÿ",
        "tool_calls": tool_calls,
        "sop_id": sop_id,
        "current_sop_step": step_info.action,
        "suggestion_type": "sop_execution"
    }
    
    # è°ƒç”¨interruptå¹¶å¤„ç†ç”¨æˆ·ç¡®è®¤ç»“æœ
    user_approved = interrupt(interrupt_info)
    logger.info(f"ç”¨æˆ·å®¡æ‰¹ç»“æœ: {user_approved}")
    
    if user_approved:
        # å®¡æ‰¹é€šè¿‡ï¼Œæ›´æ–°SOPæ­¥éª¤çš„å®¡æ‰¹çŠ¶æ€
        sop_detail = state.get("sop_detail", SOPDetail())
        updated_steps = []
        
        for step in sop_detail.steps:
            if step.action == step_info.action:
                # æ›´æ–°åŒ¹é…æ­¥éª¤çš„å®¡æ‰¹çŠ¶æ€
                step.approved = True
                step.approved_at = get_current_datetime()
                step.approval_id = step_id
                logger.info(f"æ­¥éª¤å®¡æ‰¹é€šè¿‡ï¼Œæ›´æ–°å®¡æ‰¹çŠ¶æ€: {step_id}")
            updated_steps.append(step)
        
        updated_sop_detail = SOPDetail(
            sop_id=sop_detail.sop_id,
            title=sop_detail.title,
            description=sop_detail.description,
            steps=updated_steps,
            total_steps=sop_detail.total_steps
        )
        
        return {"sop_detail": updated_sop_detail}
    else:
        # ç”¨æˆ·å–æ¶ˆï¼Œä¸­æ­¢æ‰§è¡Œ
        diagnosis_progress = state.get("diagnosis_progress", DiagnosisProgress())
        return {
            "messages": [AIMessage(content="ç”¨æˆ·å–æ¶ˆäº†SOPæ­¥éª¤æ‰§è¡Œï¼Œè¯Šæ–­æµç¨‹å·²ä¸­æ­¢ã€‚")],
            "diagnosis_progress": DiagnosisProgress(
                current_step=diagnosis_progress.current_step,
                max_steps=diagnosis_progress.max_steps,
                is_complete=True,
                termination_reason="user_cancelled"
            )
        }


def reflect_diagnosis_progress_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """è¯Šæ–­åæ€èŠ‚ç‚¹ - ä½¿ç”¨LLMæ™ºèƒ½å†³ç­–ä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: reflect_diagnosis_progress_node")
    configurable = Configuration.from_runnable_config(config)
    
    # è·å–å½“å‰çŠ¶æ€
    diagnosis_progress = state.get("diagnosis_progress", DiagnosisProgress())
    sop_detail = state.get("sop_detail", SOPDetail())
    messages = state.get("messages", [])
    question_analysis = state.get("question_analysis", QuestionAnalysis())
    report_generated = state.get("report_generated", False)
    
    # 1. å¤„ç†SOPåŠ è½½ç»“æœ
    updated_sop_detail = process_sop_loading(messages, sop_detail)
    
    # 2. æ›´æ–°è¯Šæ–­æ­¥éª¤
    current_step, has_new_execution, tool_name = update_diagnosis_step(
        messages, diagnosis_progress.current_step
    )
    
    # 3. æ›´æ–°è¯Šæ–­ç»“æœ
    diagnosis_results = extract_diagnosis_results_from_messages(messages)
    # 4. è·å–ç”¨æˆ·æœ€æ–°è¾“å…¥
    user_input = ""
    if messages:
        user_input = messages[-1].content if hasattr(messages[-1], 'content') else str(messages[-1])
    
    # 5. ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–
    llm = configurable.create_llm(
        model_name=configurable.query_generator_model,
        temperature=0.3
    )
    
    structured_llm = llm.with_structured_output(DiagnosisReflectionOutput)
    
    formatted_prompt = reflection_instructions.format(
        fault_info=question_analysis.fault_info or 'æœªæä¾›',
        current_step=current_step,
        total_steps=updated_sop_detail.total_steps,
        sop_state="loaded" if is_sop_loaded(updated_sop_detail) else "none",
        report_generated=report_generated,
        diagnosis_results=format_diagnosis_results_for_prompt(diagnosis_results),
        user_input=user_input
    )
    
    reflection_result = structured_llm.invoke(formatted_prompt)
    logger.info(f"åæ€å†³ç­–ç»“æœ: {reflection_result.action}, å®ŒæˆçŠ¶æ€: {reflection_result.is_complete}")
    
    # 6. æ ¹æ®LLMå†³ç­–æ‰§è¡Œç›¸åº”è¡ŒåŠ¨
    if reflection_result.action == "answer_question":
        # åŸºäºå†å²ä¿¡æ¯å›ç­”ç”¨æˆ·è¿½é—®
        # è®¾ç½®ä¸ºå®ŒæˆçŠ¶æ€ï¼Œé¿å…ç»§ç»­å¾ªç¯
        completed_progress = DiagnosisProgress(
            current_step=diagnosis_progress.current_step,
            max_steps=diagnosis_progress.max_steps,
            is_complete=True,
            termination_reason="answer_completed"
        )
        return {
            "messages": [AIMessage(content=reflection_result.response_content)],
            "diagnosis_progress": completed_progress,
            "sop_detail": updated_sop_detail
        }
    
    elif reflection_result.action == "generate_report":
        # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        logger.info("LLMå†³ç­–ï¼šç”Ÿæˆè¯Šæ–­æŠ¥å‘Š")
        
        report_llm = configurable.create_llm(
            model_name=configurable.answer_model,
            temperature=configurable.final_report_temperature
        )
        
        formatted_prompt = diagnosis_report_instructions.format(
            current_date=get_current_datetime(),
            fault_ip=question_analysis.fault_ip or 'æœªæä¾›',
            fault_time=question_analysis.fault_time or 'æœªæä¾›',
            fault_info=question_analysis.fault_info or 'æœªæä¾›',
            sop_id=question_analysis.sop_id or 'æœªæŒ‡å®š',
            current_step=current_step,
            total_steps=updated_sop_detail.total_steps,
            completion_status='å·²å®Œæˆ',
            diagnosis_results='\n'.join(diagnosis_results) if diagnosis_results else 'æœªè¿›è¡Œè¯Šæ–­'
        )
        
        response = report_llm.invoke(formatted_prompt)
        
        final_message = f"""
{response.content}

ğŸ“Š è¯Šæ–­æ‰§è¡Œæ‘˜è¦ï¼š
- ä½¿ç”¨SOPï¼š{question_analysis.sop_id}
- æ‰§è¡Œæ­¥éª¤ï¼š{current_step}/{updated_sop_detail.total_steps}
- å®ŒæˆçŠ¶æ€ï¼šâœ… å·²å®Œæˆ

âš ï¸ é‡è¦æé†’ï¼š
ä»¥ä¸Šè¯Šæ–­ç»“æœåŸºäºSOPæ‰§è¡Œã€‚åœ¨æ‰§è¡Œä»»ä½•æ“ä½œå‰ï¼Œè¯·ç¡®è®¤ç³»ç»ŸçŠ¶æ€å¹¶è¯„ä¼°é£é™©ã€‚
"""
        
        updated_progress = DiagnosisProgress(
            current_step=current_step,
            max_steps=diagnosis_progress.max_steps,
            is_complete=True,
            termination_reason=reflection_result.termination_reason
        )
        
        return {
            "messages": [AIMessage(content=final_message)],
            "diagnosis_progress": updated_progress,
            "sop_detail": updated_sop_detail,
            "final_diagnosis": response.content,
            "report_generated": True
        }
    
    else:  # continue
        # ç»§ç»­è¯Šæ–­
        logger.info("LLMå†³ç­–ï¼šç»§ç»­è¯Šæ–­")
        
        updated_progress = DiagnosisProgress(
            current_step=current_step,
            max_steps=diagnosis_progress.max_steps,
            is_complete=False,
            termination_reason="continue"
        )
        
        return {
            "diagnosis_progress": updated_progress,
            "sop_detail": updated_sop_detail
        }


def handle_insufficient_info_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """å¤„ç†ä¿¡æ¯ä¸è¶³çš„æƒ…å†µï¼Œæç¤ºç”¨æˆ·è¡¥å……ç¼ºå¤±ä¿¡æ¯"""
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: handle_insufficient_info_node")
    question_analysis = state.get("question_analysis", QuestionAnalysis())
    
    # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿å‡½æ•°ç”Ÿæˆç¼ºå¤±ä¿¡æ¯æç¤º
    missing_info_prompt = get_missing_info_prompt(question_analysis)
    
    return {
        "messages": [AIMessage(content=missing_info_prompt)]
    }


def finalize_diagnosis_report_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """æ™ºèƒ½æœ€ç»ˆå›ç­”èŠ‚ç‚¹ - æ”¯æŒSOPè¯Šæ–­ã€è¿ç»´é—®ç­”ã€æ™®é€šèŠå¤©"""
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: finalize_diagnosis_report_node")
    configurable = Configuration.from_runnable_config(config)
    
    # è·å–çŠ¶æ€ä¿¡æ¯
    messages = state.get("messages", [])
    user_question = messages[-1].content if messages else ""  # åŠ¨æ€è·å–
    question_analysis = state.get("question_analysis", QuestionAnalysis())
    sop_detail = state.get("sop_detail", SOPDetail())
    diagnosis_progress = state.get("diagnosis_progress", DiagnosisProgress())
    report_generated = state.get("report_generated", False)
    
    # åˆ¤æ–­å¯¹è¯ç±»å‹å’Œå›ç­”ç­–ç•¥
    response_type = determine_response_type(
        user_question, messages, question_analysis, 
        diagnosis_progress, sop_detail, report_generated
    )
    
    logger.info(f"å“åº”ç±»å‹åˆ¤æ–­: {response_type}")
    
    # åˆå§‹åŒ–æ¨ç†æ¨¡å‹
    llm = configurable.create_llm(
        model_name=configurable.answer_model,
        temperature=configurable.final_report_temperature
    )
    
    if response_type == "diagnosis_report":
        # ç”Ÿæˆå®Œæ•´çš„SOPè¯Šæ–­æŠ¥å‘Š
        formatted_prompt = diagnosis_report_instructions.format(
            current_date=get_current_datetime(),
            fault_ip=question_analysis.fault_ip or 'æœªæä¾›',
            fault_time=question_analysis.fault_time or 'æœªæä¾›',
            fault_info=question_analysis.fault_info or 'æœªæä¾›',
            sop_id=question_analysis.sop_id or 'æœªæŒ‡å®š',
            current_step=diagnosis_progress.current_step,
            total_steps=sop_detail.total_steps,
            completion_status='å·²å®Œæˆ' if diagnosis_progress.is_complete else 'è¿›è¡Œä¸­',
            diagnosis_results=format_diagnosis_results_for_prompt(messages, max_results=50)
        )
        
        response = llm.invoke(formatted_prompt)
        
        final_message = f"""
{response.content}

ğŸ“Š è¯Šæ–­æ‰§è¡Œæ‘˜è¦ï¼š
- ä½¿ç”¨SOPï¼š{question_analysis.sop_id}
- æ‰§è¡Œæ­¥éª¤ï¼š{diagnosis_progress.current_step}/{sop_detail.total_steps}
- å®ŒæˆçŠ¶æ€ï¼š{'âœ… å·²å®Œæˆ' if diagnosis_progress.is_complete else 'ğŸ”„ è¿›è¡Œä¸­'}

âš ï¸ é‡è¦æé†’ï¼š
ä»¥ä¸Šè¯Šæ–­ç»“æœåŸºäºSOPæ‰§è¡Œã€‚åœ¨æ‰§è¡Œä»»ä½•æ“ä½œå‰ï¼Œè¯·ç¡®è®¤ç³»ç»ŸçŠ¶æ€å¹¶è¯„ä¼°é£é™©ã€‚
"""
        
        return {
            "messages": [AIMessage(content=final_message)],
            "final_diagnosis": response.content,
            "report_generated": True
        }
    
    else:
        # è¿ç»´é—®ç­”æˆ–æ™®é€šèŠå¤©
        conversation_context = build_conversation_context(messages)
        
        prompt = f"""æ‚¨æ˜¯ä¸“ä¸šçš„è¿ç»´æŠ€æœ¯åŠ©æ‰‹ï¼Œæ”¯æŒæ•…éšœè¯Šæ–­ã€è¿ç»´é—®ç­”å’Œæ—¥å¸¸äº¤æµã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼š
{conversation_context}

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜ç±»å‹å›ç­”ï¼š
- å¦‚æœæ˜¯è¿ç»´æŠ€æœ¯é—®é¢˜ï¼Œæä¾›ä¸“ä¸šçš„æŠ€æœ¯æŒ‡å¯¼
- å¦‚æœæ˜¯æ™®é€šèŠå¤©ï¼Œè‡ªç„¶å‹å¥½åœ°å›åº”
- å¦‚æœæ¶‰åŠä¹‹å‰çš„è¯Šæ–­å†…å®¹ï¼Œå¯ä»¥å¼•ç”¨ç›¸å…³ä¿¡æ¯
- ä¿æŒç®€æ´æ˜äº†ï¼Œä¸éœ€è¦ç”ŸæˆæŠ¥å‘Šæ ¼å¼

è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        
        response = llm.invoke(prompt)
        
        return {
            "messages": [AIMessage(content=response.content)]
        }


def determine_response_type(user_question, messages, question_analysis, diagnosis_progress, sop_detail, report_generated=False):
    """åˆ¤æ–­å›ç­”ç±»å‹ï¼šæ˜¯å¦éœ€è¦ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
    
    # 1. ç”¨æˆ·æ˜ç¡®è¦æ±‚ç”ŸæˆæŠ¥å‘Š
    report_keywords = ["ç”ŸæˆæŠ¥å‘Š", "è¯Šæ–­æŠ¥å‘Š", "æ•…éšœæŠ¥å‘Š", "è¾“å‡ºæŠ¥å‘Š", "æ€»ç»“æŠ¥å‘Š"]
    if any(keyword in user_question for keyword in report_keywords):
        return "diagnosis_report"
    
    # 2. å®Œæˆäº†å®Œæ•´çš„SOPè¯Šæ–­æµç¨‹ä¸”æœªç”Ÿæˆè¿‡æŠ¥å‘Š
    if (diagnosis_progress and diagnosis_progress.is_complete and 
        is_sop_loaded(sop_detail) and len(extract_diagnosis_results_from_messages(messages)) >= 2 and not report_generated):
        return "diagnosis_report"
    
    # 3. å…¶ä»–æƒ…å†µéƒ½æ˜¯æ™®é€šå›ç­”
    return "general_answer"


def build_conversation_context(messages):
    """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
    context_parts = []
    
    # æ·»åŠ è¯Šæ–­å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    diagnosis_results = extract_diagnosis_results_from_messages(messages, max_results=3)
    if diagnosis_results:
        context_parts.append("è¯Šæ–­å†å²ï¼š")
        context_parts.extend(diagnosis_results)  # æœ€è¿‘3ä¸ªè¯Šæ–­ç»“æœ
    
    # æ·»åŠ æœ€è¿‘å¯¹è¯
    if messages and len(messages) > 1:
        context_parts.append("\næœ€è¿‘å¯¹è¯ï¼š")
        recent_messages = messages[-4:] if len(messages) > 4 else messages[:-1]
        for i, msg in enumerate(recent_messages):
            role = "ç”¨æˆ·" if i % 2 == 0 else "åŠ©æ‰‹"
            content = getattr(msg, 'content', str(msg))[:100]
            context_parts.append(f"{role}: {content}")
    
    return "\n".join(context_parts) if context_parts else "æ— å†å²å¯¹è¯"


# è·¯ç”±å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬
def evaluate_diagnosis_progress(state: DiagnosticState, config: RunnableConfig) -> str:
    """è¯„ä¼°è¯Šæ–­è¿›åº¦ï¼Œæ ¹æ®æ‰§è¡Œæƒ…å†µå†³å®šä¸‹ä¸€æ­¥"""
    print(f"âœ… æ‰§è¡Œè·¯ç”±å‡½æ•°: evaluate_diagnosis_progress")
    diagnosis_progress = state.get("diagnosis_progress", DiagnosisProgress())
    
    # å¦‚æœè¯Šæ–­å·²æ ‡è®°ä¸ºå®Œæˆï¼Œç›´æ¥ç»“æŸ
    if diagnosis_progress.is_complete:
        logger.info(f"è¯Šæ–­å®Œæˆï¼Œæµç¨‹ç»“æŸ: {diagnosis_progress.termination_reason}")
        print(f"âœ… è·¯ç”±ç»“æœ: END (è¯Šæ–­å®Œæˆ)")
        return END
    
    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
    if diagnosis_progress.current_step >= diagnosis_progress.max_steps:
        logger.warning(f"è¾¾åˆ°æœ€å¤§æ­¥éª¤é™åˆ¶ï¼Œå¼ºåˆ¶ç»“æŸ: {diagnosis_progress.current_step}/{diagnosis_progress.max_steps}")
        print(f"âœ… è·¯ç”±ç»“æœ: END (è¾¾åˆ°æœ€å¤§æ­¥éª¤)")
        return END
    
    # ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥
    logger.info(f"ç»§ç»­æ‰§è¡Œï¼Œå½“å‰æ­¥éª¤: {diagnosis_progress.current_step}")
    print(f"âœ… è·¯ç”±ç»“æœ: plan_tools (ç»§ç»­è¯Šæ–­)")
    return "plan_tools"


# åˆ›å»ºå·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
tool_node = ToolNode(all_tools)

# åŒ…è£…å·¥å…·èŠ‚ç‚¹ä»¥æ·»åŠ æ‰“å°
def execute_tools_node(state, config):
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: execute_tools_node")
    return tool_node.invoke(state, config)

# åˆ›å»ºè¯Šæ–­Agentå›¾ - ç®€åŒ–ç‰ˆæœ¬
builder = StateGraph(DiagnosticState, config_schema=Configuration)
# æ·»åŠ èŠ‚ç‚¹
builder.add_node("analyze_question", analyze_question_node)
builder.add_node("handle_insufficient_info", handle_insufficient_info_node)
builder.add_node("plan_tools", plan_diagnosis_tools_node)
builder.add_node("approval", approval_node)
builder.add_node("execute_tools", execute_tools_node)
builder.add_node("reflection", reflect_diagnosis_progress_node)
builder.add_edge(START, "analyze_question")
builder.add_conditional_edges("analyze_question", check_info_sufficient, ["plan_tools", "handle_insufficient_info"])
# ä¿®æ”¹ï¼šä¿¡æ¯ä¸è¶³æ—¶ç­‰å¾…ç”¨æˆ·è¡¥å……ï¼Œç”¨æˆ·è¡¥å……åé‡æ–°å›åˆ°analyze_questionåˆ†æ
builder.add_edge("handle_insufficient_info", END)
builder.add_conditional_edges("plan_tools",check_tool_calls,{"approval": "approval","reflection": "reflection"})
builder.add_edge("approval", "execute_tools")
builder.add_edge("execute_tools", "reflection")
builder.add_conditional_edges("reflection", evaluate_diagnosis_progress, ["plan_tools", END])


# ç¼–è¯‘å›¾ - æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦ä½¿ç”¨checkpointer
checkpointer_type = os.getenv("CHECKPOINTER_TYPE", "memory")

graph = compile_graph_with_checkpointer(builder, checkpointer_type)