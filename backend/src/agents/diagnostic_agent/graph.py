"""
æ•…éšœè¯Šæ–­ä»£ç† - ä½¿ç”¨LangGraphå­å›¾æ¶æ„
æŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„æ–¹æ³•å®ç°ï¼šä¸»å›¾è´Ÿè´£è·¯ç”±ï¼Œå­å›¾å¤„ç†å…·ä½“é€»è¾‘
"""

import os
import logging
from typing import Dict, Any, Literal
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode
from langchain_core.messages import SystemMessage, AIMessage
from langchain_core.runnables import RunnableConfig

from .configuration import Configuration
from .state import DiagnosticState, QuestionAnalysis, DiagnosisProgress, SOPDetail
from .schemas import IntentAnalysisOutput, QuestionInfoExtraction, DiagnosisReflectionOutput
from .prompts import (
    get_current_datetime, get_question_analysis_prompt, get_missing_info_prompt,
    tool_planning_instructions, diagnosis_report_instructions, reflection_instructions
)
from .tools import all_tools
from .utils import (
    merge_field, check_approval_needed, is_already_approved, process_sop_loading,
    update_diagnosis_step, check_info_sufficient, check_tool_calls, save_graph_image,
    compile_graph_with_checkpointer, extract_diagnosis_results_from_messages,
    format_diagnosis_results_for_prompt, is_sop_loaded
)

logger = logging.getLogger(__name__)


# ================================
# ä¸»å›¾èŠ‚ç‚¹ï¼šæ„å›¾åˆ†æå’Œè·¯ç”±
# ================================

def analyze_intent_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """
    æ„å›¾åˆ†æèŠ‚ç‚¹ - åˆ¤æ–­ç”¨æˆ·æ˜¯å¦éœ€è¦SOPè¯Šæ–­è¿˜æ˜¯æ™®é€šé—®ç­”
    """
    print(f"âœ… æ‰§è¡ŒèŠ‚ç‚¹: analyze_intent_node")
    
    configurable = Configuration.from_runnable_config(config)
    messages = state.get("messages", [])
    
    if not messages:
        return {"intent": "general_qa", "intent_reason": "æ— ç”¨æˆ·è¾“å…¥"}
    
    user_question = messages[-1].content if messages else ""
    
    # ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ„å›¾
    llm = configurable.create_llm(
        model_name=configurable.query_generator_model,
        temperature=0.1  # ä½æ¸©åº¦ç¡®ä¿åˆ†ç±»å‡†ç¡®
    )
    
    intent_analysis_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¿ç»´åŠ©æ‰‹æ„å›¾åˆ†æå™¨ã€‚è¯·åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦éœ€è¦æ•…éšœè¯Šæ–­SOPè¿˜æ˜¯æ™®é€šé—®ç­”ã€‚

åˆ¤æ–­æ ‡å‡†ï¼š

1. æ•…éšœè¯Šæ–­SOP (sop_diagnosis)ï¼š
   - ç”¨æˆ·æ˜ç¡®æåˆ°æ•…éšœã€æŠ¥é”™ã€å¼‚å¸¸ã€é—®é¢˜ç­‰éœ€è¦æ’æŸ¥çš„æƒ…å†µ
   - ç”¨æˆ·æåˆ°éœ€è¦æ’æŸ¥ã€è¯Šæ–­ã€è§£å†³å…·ä½“é—®é¢˜
   - ç”¨æˆ·æè¿°äº†å…·ä½“çš„æ•…éšœç°è±¡å’Œå½±å“
   - ç”¨æˆ·æåˆ°äº†IPã€æ—¶é—´ã€é”™è¯¯ä¿¡æ¯ç­‰æ•…éšœè¦ç´ 
   - ç”¨æˆ·æ˜ç¡®è¦æ±‚æ‰§è¡Œæ•…éšœè¯Šæ–­æµç¨‹æˆ–SOP
   - å…³é”®è¯ï¼šæ•…éšœã€æŠ¥é”™ã€å¼‚å¸¸ã€æ’æŸ¥ã€è¯Šæ–­ã€SOPã€é—®é¢˜è§£å†³ã€ä¿®å¤ã€æ¢å¤

2. æ™®é€šé—®ç­” (general_qa)ï¼š
   - ç”¨æˆ·è¯¢é—®æŠ€æœ¯çŸ¥è¯†ã€æ“ä½œæ–¹æ³•ã€æ¦‚å¿µè§£é‡Š
   - ç”¨æˆ·è¿›è¡Œæ—¥å¸¸èŠå¤©ã€é—®å€™ã€é—²èŠ
   - ç”¨æˆ·è¯¢é—®ç³»ç»Ÿä¿¡æ¯ã€é…ç½®è¯´æ˜ã€çŠ¶æ€æŸ¥è¯¢
   - ç”¨æˆ·è¯¢é—®å†å²è®°å½•ã€ç»Ÿè®¡ä¿¡æ¯
   - ç”¨æˆ·å’¨è¯¢å¦‚ä½•ä½¿ç”¨æŸä¸ªåŠŸèƒ½æˆ–å·¥å…·
   - ä¸æ¶‰åŠå…·ä½“æ•…éšœæ’æŸ¥çš„æŠ€æœ¯é—®é¢˜
   - å…³é”®è¯ï¼šå¦‚ä½•ã€ä»€ä¹ˆæ˜¯ã€æ€ä¹ˆã€é…ç½®ã€å®‰è£…ã€ä½¿ç”¨ã€æŸ¥è¯¢ã€çŠ¶æ€

æ³¨æ„ï¼š
- å¦‚æœç”¨æˆ·åªæ˜¯è¯¢é—®æ•…éšœç›¸å…³æ¦‚å¿µæˆ–æ–¹æ³•ï¼Œä¸æ¶‰åŠå…·ä½“æ•…éšœæ’æŸ¥ï¼Œåº”å½’ç±»ä¸ºgeneral_qa
- å¦‚æœç”¨æˆ·æè¿°äº†å…·ä½“çš„æ•…éšœç°è±¡å¹¶éœ€è¦æ’æŸ¥ï¼Œåº”å½’ç±»ä¸ºsop_diagnosis
- ä¼˜å…ˆè€ƒè™‘ç”¨æˆ·çš„å…·ä½“æ„å›¾ï¼Œè€Œä¸æ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

è¯·åˆ†æç”¨æˆ·æ„å›¾ï¼Œè¿”å›åˆ†ç±»ç»“æœå’Œç®€è¦ç†ç”±ã€‚
"""
    
    structured_llm = llm.with_structured_output(IntentAnalysisOutput)
    result = structured_llm.invoke(intent_analysis_prompt)
    
    logger.info(f"æ„å›¾åˆ†æç»“æœ: {result.intent} - {result.reason}")
    
    return {
        "intent": result.intent,
        "intent_reason": result.reason
    }


def route_to_subgraph(state: DiagnosticState, config: RunnableConfig) -> Literal["sop_diagnosis", "general_qa"]:
    """
    è·¯ç”±å‡½æ•° - æ ¹æ®æ„å›¾åˆ†æç»“æœå†³å®šè¿›å…¥å“ªä¸ªå­å›¾
    """
    print(f"âœ… æ‰§è¡Œè·¯ç”±å‡½æ•°: route_to_subgraph")
    
    intent = state.get("intent", "general_qa")
    
    # ç¡®ä¿intentå€¼æœ‰æ•ˆ
    if intent not in ["sop_diagnosis", "general_qa"]:
        logger.warning(f"æ— æ•ˆçš„æ„å›¾å€¼: {intent}ï¼Œé»˜è®¤ä½¿ç”¨general_qa")
        intent = "general_qa"
    
    logger.info(f"è·¯ç”±å†³ç­–: {intent}")
    print(f"âœ… è·¯ç”±ç»“æœ: {intent}")
    
    return intent


# ================================
# SOPè¯Šæ–­å­å›¾èŠ‚ç‚¹
# ================================

def analyze_question_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """é—®é¢˜åˆ†æèŠ‚ç‚¹ - æ”¯æŒå¤šè½®è¡¥å……å››è¦ç´ """
    print(f"âœ… æ‰§è¡ŒSOPå­å›¾èŠ‚ç‚¹: analyze_question_node")
    configurable = Configuration.from_runnable_config(config)
    
    messages = state.get("messages", [])
    user_question = messages[-1].content if messages else ""
    
    # å››è¦ç´ åˆ†ææµç¨‹
    llm = configurable.create_llm(
        model_name=configurable.query_generator_model,
        temperature=configurable.question_analysis_temperature
    )
    
    # è·å–å½“å‰å·²æœ‰çš„å››è¦ç´ ä¿¡æ¯
    current_analysis = state.get("question_analysis", QuestionAnalysis())
    
    # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿å‡½æ•°ç”Ÿæˆæç¤ºè¯
    prompt = get_question_analysis_prompt(user_question, current_analysis)
    
    # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
    structured_llm = llm.with_structured_output(QuestionInfoExtraction)
    result = structured_llm.invoke(prompt)
    
    merged_analysis = QuestionAnalysis(
        fault_ip=merge_field(result.fault_ip, current_analysis.fault_ip),
        fault_time=merge_field(result.fault_time, current_analysis.fault_time, "fault_time"),
        fault_info=merge_field(result.fault_info, current_analysis.fault_info),
        sop_id=merge_field(result.sop_id, current_analysis.sop_id)
    )
    
    # æ£€æŸ¥å››è¦ç´ æ˜¯å¦éƒ½å®Œæ•´
    info_sufficient = (
        merged_analysis.fault_ip and merged_analysis.fault_ip != "å¾…æå–" and
        merged_analysis.fault_time and merged_analysis.fault_time != "å¾…æå–" and
        merged_analysis.fault_info and merged_analysis.fault_info != "å¾…æå–" and
        merged_analysis.sop_id and merged_analysis.sop_id != "å¾…æå–"
    )
    
    # ç”Ÿæˆç¼ºå¤±å­—æ®µåˆ—è¡¨
    missing_fields = []
    if not merged_analysis.fault_ip or merged_analysis.fault_ip == "å¾…æå–": missing_fields.append("æ•…éšœIP")
    if not merged_analysis.fault_time or merged_analysis.fault_time == "å¾…æå–": missing_fields.append("æ•…éšœæ—¶é—´")
    if not merged_analysis.fault_info or merged_analysis.fault_info == "å¾…æå–": missing_fields.append("æ•…éšœç°è±¡")
    if not merged_analysis.sop_id or merged_analysis.sop_id == "å¾…æå–": missing_fields.append("æ’æŸ¥SOPç¼–å·")
    
    merged_analysis.missing_fields = missing_fields
    merged_analysis.info_sufficient = info_sufficient
    
    logger.info(f"å››è¦ç´ åˆ†æ: å……è¶³={info_sufficient}, ç¼ºå¤±={missing_fields}")
    
    return {
        "question_analysis": merged_analysis
    }


def handle_insufficient_info_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """å¤„ç†ä¿¡æ¯ä¸è¶³çš„æƒ…å†µï¼Œæç¤ºç”¨æˆ·è¡¥å……ç¼ºå¤±ä¿¡æ¯"""
    print(f"âœ… æ‰§è¡ŒSOPå­å›¾èŠ‚ç‚¹: handle_insufficient_info_node")
    question_analysis = state.get("question_analysis", QuestionAnalysis())
    
    # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿å‡½æ•°ç”Ÿæˆç¼ºå¤±ä¿¡æ¯æç¤º
    missing_info_prompt = get_missing_info_prompt(question_analysis)
    
    return {
        "messages": [AIMessage(content=missing_info_prompt)]
    }


def plan_diagnosis_tools_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """å·¥å…·è§„åˆ’èŠ‚ç‚¹ - ä¸¥æ ¼æŒ‰ç…§SOPæ‰§è¡Œ"""
    print(f"âœ… æ‰§è¡ŒSOPå­å›¾èŠ‚ç‚¹: plan_diagnosis_tools_node")
    configurable = Configuration.from_runnable_config(config)
    llm = configurable.create_llm(
        model_name=configurable.query_generator_model,
        temperature=configurable.tool_planning_temperature
    )
    
    # ç»‘å®šå·¥å…·åˆ°LLM
    llm_with_tools = llm.bind_tools(all_tools)
    
    # æ„å»ºå·¥å…·è§„åˆ’æç¤º
    question_analysis = state.get("question_analysis", QuestionAnalysis())
    sop_detail = state.get("sop_detail", SOPDetail())
    sop_state = "loaded" if is_sop_loaded(sop_detail) else "none"
    
    formatted_prompt = tool_planning_instructions.format(
        fault_ip=question_analysis.fault_ip or "",
        fault_time=question_analysis.fault_time or "",
        fault_info=question_analysis.fault_info or "",
        sop_id=question_analysis.sop_id or "",
        sop_state=sop_state,
        sop_content=sop_detail.description if sop_detail else ""
    )

    # æ„å»ºæ¶ˆæ¯
    messages = state.get("messages", [])
    system_message = SystemMessage(content=formatted_prompt)
    messages_with_system = [system_message] + messages
    
    # è°ƒç”¨LLMç”Ÿæˆå·¥å…·è°ƒç”¨
    response = llm_with_tools.invoke(messages_with_system)
    
    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å·¥å…·è°ƒç”¨
    has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
    logger.info(f"å·¥å…·è§„åˆ’ç»“æœ: ç”Ÿæˆäº† {len(response.tool_calls) if has_tool_calls else 0} ä¸ªå·¥å…·è°ƒç”¨")
    
    if not has_tool_calls: 
        logger.warning("LLMæ²¡æœ‰ç”Ÿæˆä»»ä½•å·¥å…·è°ƒç”¨ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯Šæ–­æå‰ç»“æŸ")
    
    return {"messages": [response]}


def approval_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """SOPæ‰§è¡Œç¡®è®¤èŠ‚ç‚¹"""
    print(f"âœ… æ‰§è¡ŒSOPå­å›¾èŠ‚ç‚¹: approval_node")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å®¡æ‰¹
    approval_info = check_approval_needed(state)
    if not approval_info: 
        return {}  # æ— éœ€å®¡æ‰¹ï¼Œç›´æ¥ç»§ç»­
    
    # æ£€æŸ¥æ˜¯å¦å·²å®¡æ‰¹è¿‡
    if is_already_approved(state, approval_info):
        logger.info(f"æ­¥éª¤å·²å®¡æ‰¹è¿‡ï¼Œè·³è¿‡: {approval_info['step_id']}")
        return {}  # å·²å®¡æ‰¹ï¼Œç›´æ¥ç»§ç»­
    
    # æ‰§è¡Œå®¡æ‰¹æµç¨‹
    step_info = approval_info["step_info"]
    step_id = approval_info["step_id"]
    tool_calls = approval_info["tool_calls"]
    sop_id = approval_info["sop_id"]
    
    logger.info(f"è§¦å‘å®¡æ‰¹æµç¨‹: SOP {sop_id}, æ­¥éª¤: {step_info.action}")
    
    # æ„å»ºå·¥å…·æè¿°
    tool_descriptions = [
        f"å·¥å…·: {tc.get('name', '')}, å‚æ•°: {tc.get('args', {})}"
        for tc in tool_calls
    ]
    
    # ä¸­æ–­å¹¶è¯·æ±‚ç”¨æˆ·ç¡®è®¤
    from langgraph.types import interrupt
    interrupt_info = {
        "message": f"æŒ‰ç…§SOP '{sop_id}' è¦æ±‚ï¼Œå³å°†æ‰§è¡Œéœ€è¦å®¡æ‰¹çš„æ­¥éª¤:\n\n"
                   f"**æ­¥éª¤è¯¦æƒ…:** {step_info.action}\n"
                   f"**è®¡åˆ’æ“ä½œ:**\n" + "\n".join(tool_descriptions) +
                   f"\n\nç¡®è®¤æ‰§è¡Œï¼Ÿ",
        "tool_calls": tool_calls,
        "sop_id": sop_id,
        "current_sop_step": step_info.action,
        "suggestion_type": "sop_execution"
    }
    
    # è°ƒç”¨interruptå¹¶å¤„ç†ç”¨æˆ·ç¡®è®¤ç»“æœ
    user_approved = interrupt(interrupt_info)
    logger.info(f"ç”¨æˆ·å®¡æ‰¹ç»“æœ: {user_approved}")
    
    if user_approved:
        # å®¡æ‰¹é€šè¿‡ï¼Œæ›´æ–°SOPæ­¥éª¤çš„å®¡æ‰¹çŠ¶æ€
        sop_detail = state.get("sop_detail", SOPDetail())
        updated_steps = []
        
        for step in sop_detail.steps:
            if step.action == step_info.action:
                # æ›´æ–°åŒ¹é…æ­¥éª¤çš„å®¡æ‰¹çŠ¶æ€
                step.approved = True
                step.approved_at = get_current_datetime()
                step.approval_id = step_id
                logger.info(f"æ­¥éª¤å®¡æ‰¹é€šè¿‡ï¼Œæ›´æ–°å®¡æ‰¹çŠ¶æ€: {step_id}")
            updated_steps.append(step)
        
        updated_sop_detail = SOPDetail(
            sop_id=sop_detail.sop_id,
            title=sop_detail.title,
            description=sop_detail.description,
            steps=updated_steps,
            total_steps=sop_detail.total_steps
        )
        
        return {"sop_detail": updated_sop_detail}
    else:
        # ç”¨æˆ·å–æ¶ˆï¼Œä¸­æ­¢æ‰§è¡Œ
        diagnosis_progress = state.get("diagnosis_progress", DiagnosisProgress())
        return {
            "messages": [AIMessage(content="ç”¨æˆ·å–æ¶ˆäº†SOPæ­¥éª¤æ‰§è¡Œï¼Œè¯Šæ–­æµç¨‹å·²ä¸­æ­¢ã€‚")],
            "diagnosis_progress": DiagnosisProgress(
                current_step=diagnosis_progress.current_step,
                max_steps=diagnosis_progress.max_steps,
                is_complete=True,
                termination_reason="user_cancelled"
            )
        }


def reflect_diagnosis_progress_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """è¯Šæ–­åæ€èŠ‚ç‚¹ - ä½¿ç”¨LLMæ™ºèƒ½å†³ç­–ä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
    print(f"âœ… æ‰§è¡ŒSOPå­å›¾èŠ‚ç‚¹: reflect_diagnosis_progress_node")
    configurable = Configuration.from_runnable_config(config)
    
    # è·å–å½“å‰çŠ¶æ€
    diagnosis_progress = state.get("diagnosis_progress", DiagnosisProgress())
    sop_detail = state.get("sop_detail", SOPDetail())
    messages = state.get("messages", [])
    question_analysis = state.get("question_analysis", QuestionAnalysis())
    report_generated = state.get("report_generated", False)
    
    # 1. å¤„ç†SOPåŠ è½½ç»“æœ
    updated_sop_detail = process_sop_loading(messages, sop_detail)
    
    # 2. æ›´æ–°è¯Šæ–­æ­¥éª¤
    current_step, has_new_execution, tool_name = update_diagnosis_step(
        messages, diagnosis_progress.current_step
    )
    
    # 3. æ›´æ–°è¯Šæ–­ç»“æœ
    diagnosis_results = extract_diagnosis_results_from_messages(messages)
    
    # 4. è·å–ç”¨æˆ·æœ€æ–°è¾“å…¥
    user_input = ""
    if messages:
        user_input = messages[-1].content if hasattr(messages[-1], 'content') else str(messages[-1])
    
    # 5. ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–
    llm = configurable.create_llm(
        model_name=configurable.query_generator_model,
        temperature=0.3
    )
    
    structured_llm = llm.with_structured_output(DiagnosisReflectionOutput)
    
    formatted_prompt = reflection_instructions.format(
        fault_info=question_analysis.fault_info or 'æœªæä¾›',
        current_step=current_step,
        total_steps=updated_sop_detail.total_steps,
        sop_state="loaded" if is_sop_loaded(updated_sop_detail) else "none",
        report_generated=report_generated,
        diagnosis_results=format_diagnosis_results_for_prompt(diagnosis_results),
        user_input=user_input
    )
    
    reflection_result = structured_llm.invoke(formatted_prompt)
    logger.info(f"åæ€å†³ç­–ç»“æœ: {reflection_result.action}, å®ŒæˆçŠ¶æ€: {reflection_result.is_complete}")
    
    # 6. æ ¹æ®LLMå†³ç­–æ‰§è¡Œç›¸åº”è¡ŒåŠ¨
    if reflection_result.action == "answer_question":
        # åŸºäºå†å²ä¿¡æ¯å›ç­”ç”¨æˆ·è¿½é—®
        completed_progress = DiagnosisProgress(
            current_step=diagnosis_progress.current_step,
            max_steps=diagnosis_progress.max_steps,
            is_complete=True,
            termination_reason="answer_completed"
        )
        return {
            "messages": [AIMessage(content=reflection_result.response_content)],
            "diagnosis_progress": completed_progress,
            "sop_detail": updated_sop_detail
        }
    
    elif reflection_result.action == "generate_report":
        # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        logger.info("LLMå†³ç­–ï¼šç”Ÿæˆè¯Šæ–­æŠ¥å‘Š")
        
        report_llm = configurable.create_llm(
            model_name=configurable.answer_model,
            temperature=configurable.final_report_temperature
        )
        
        formatted_prompt = diagnosis_report_instructions.format(
            current_date=get_current_datetime(),
            fault_ip=question_analysis.fault_ip or 'æœªæä¾›',
            fault_time=question_analysis.fault_time or 'æœªæä¾›',
            fault_info=question_analysis.fault_info or 'æœªæä¾›',
            sop_id=question_analysis.sop_id or 'æœªæŒ‡å®š',
            current_step=current_step,
            total_steps=updated_sop_detail.total_steps,
            completion_status='å·²å®Œæˆ',
            diagnosis_results='\n'.join(diagnosis_results) if diagnosis_results else 'æœªè¿›è¡Œè¯Šæ–­'
        )
        
        response = report_llm.invoke(formatted_prompt)
        
        final_message = f"""
{response.content}

ğŸ“Š è¯Šæ–­æ‰§è¡Œæ‘˜è¦ï¼š
- ä½¿ç”¨SOPï¼š{question_analysis.sop_id}
- æ‰§è¡Œæ­¥éª¤ï¼š{current_step}/{updated_sop_detail.total_steps}
- å®ŒæˆçŠ¶æ€ï¼šâœ… å·²å®Œæˆ

âš ï¸ é‡è¦æé†’ï¼š
ä»¥ä¸Šè¯Šæ–­ç»“æœåŸºäºSOPæ‰§è¡Œã€‚åœ¨æ‰§è¡Œä»»ä½•æ“ä½œå‰ï¼Œè¯·ç¡®è®¤ç³»ç»ŸçŠ¶æ€å¹¶è¯„ä¼°é£é™©ã€‚
"""
        
        updated_progress = DiagnosisProgress(
            current_step=current_step,
            max_steps=diagnosis_progress.max_steps,
            is_complete=True,
            termination_reason=reflection_result.termination_reason
        )
        
        return {
            "messages": [AIMessage(content=final_message)],
            "diagnosis_progress": updated_progress,
            "sop_detail": updated_sop_detail,
            "final_diagnosis": response.content,
            "report_generated": True
        }
    
    else:  # continue
        # ç»§ç»­è¯Šæ–­
        logger.info("LLMå†³ç­–ï¼šç»§ç»­è¯Šæ–­")
        
        updated_progress = DiagnosisProgress(
            current_step=current_step,
            max_steps=diagnosis_progress.max_steps,
            is_complete=False,
            termination_reason="continue"
        )
        
        return {
            "diagnosis_progress": updated_progress,
            "sop_detail": updated_sop_detail
        }


def evaluate_diagnosis_progress(state: DiagnosticState, config: RunnableConfig) -> str:
    """è¯„ä¼°è¯Šæ–­è¿›åº¦ï¼Œæ ¹æ®æ‰§è¡Œæƒ…å†µå†³å®šä¸‹ä¸€æ­¥"""
    print(f"âœ… æ‰§è¡ŒSOPå­å›¾è·¯ç”±: evaluate_diagnosis_progress")
    diagnosis_progress = state.get("diagnosis_progress", DiagnosisProgress())
    
    # å¦‚æœè¯Šæ–­å·²æ ‡è®°ä¸ºå®Œæˆï¼Œç›´æ¥ç»“æŸ
    if diagnosis_progress.is_complete:
        logger.info(f"è¯Šæ–­å®Œæˆï¼Œæµç¨‹ç»“æŸ: {diagnosis_progress.termination_reason}")
        print(f"âœ… è·¯ç”±ç»“æœ: END (è¯Šæ–­å®Œæˆ)")
        return END
    
    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
    if diagnosis_progress.current_step >= diagnosis_progress.max_steps:
        logger.warning(f"è¾¾åˆ°æœ€å¤§æ­¥éª¤é™åˆ¶ï¼Œå¼ºåˆ¶ç»“æŸ: {diagnosis_progress.current_step}/{diagnosis_progress.max_steps}")
        print(f"âœ… è·¯ç”±ç»“æœ: END (è¾¾åˆ°æœ€å¤§æ­¥éª¤)")
        return END
    
    # ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥
    logger.info(f"ç»§ç»­æ‰§è¡Œï¼Œå½“å‰æ­¥éª¤: {diagnosis_progress.current_step}")
    print(f"âœ… è·¯ç”±ç»“æœ: plan_tools (ç»§ç»­è¯Šæ–­)")
    return "plan_tools"


# ================================
# æ™®é€šé—®ç­”å­å›¾èŠ‚ç‚¹
# ================================

def analyze_question_context_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """åˆ†æé—®é¢˜ä¸Šä¸‹æ–‡èŠ‚ç‚¹ - ç†è§£ç”¨æˆ·é—®é¢˜å¹¶å‡†å¤‡å›ç­”"""
    print(f"âœ… æ‰§è¡Œé—®ç­”å­å›¾èŠ‚ç‚¹: analyze_question_context_node")
    
    messages = state.get("messages", [])
    if not messages:
        return {"qa_context": "æ— å†å²å¯¹è¯"}
    
    # è·å–ç”¨æˆ·é—®é¢˜
    user_question = messages[-1].content if messages else ""
    
    # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
    context_parts = []
    
    # æ·»åŠ è¯Šæ–­å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    diagnosis_results = extract_diagnosis_results_from_messages(messages, max_results=3)
    if diagnosis_results:
        context_parts.append("ç›¸å…³è¯Šæ–­å†å²ï¼š")
        context_parts.extend(diagnosis_results[:3])  # æœ€è¿‘3ä¸ªè¯Šæ–­ç»“æœ
    
    # æ·»åŠ æœ€è¿‘å¯¹è¯
    if len(messages) > 1:
        context_parts.append("\næœ€è¿‘å¯¹è¯ï¼š")
        recent_messages = messages[-6:] if len(messages) > 6 else messages[:-1]
        for i, msg in enumerate(recent_messages):
            role = "ç”¨æˆ·" if i % 2 == 0 else "åŠ©æ‰‹"
            content = getattr(msg, 'content', str(msg))[:150]  # é™åˆ¶é•¿åº¦
            context_parts.append(f"{role}: {content}")
    
    qa_context = "\n".join(context_parts) if context_parts else "æ— å†å²å¯¹è¯"
    
    logger.info(f"é—®ç­”ä¸Šä¸‹æ–‡åˆ†æå®Œæˆï¼Œå†å²è¯Šæ–­: {len(diagnosis_results)}, å¯¹è¯è½®æ¬¡: {len(messages)}")
    
    return {
        "qa_context": qa_context,
        "user_question": user_question
    }


def generate_answer_node(state: DiagnosticState, config: RunnableConfig) -> Dict[str, Any]:
    """ç”Ÿæˆå›ç­”èŠ‚ç‚¹ - åŸºäºç”¨æˆ·é—®é¢˜å’Œä¸Šä¸‹æ–‡ç”Ÿæˆä¸“ä¸šå›ç­”"""
    print(f"âœ… æ‰§è¡Œé—®ç­”å­å›¾èŠ‚ç‚¹: generate_answer_node")
    
    configurable = Configuration.from_runnable_config(config)
    
    # è·å–çŠ¶æ€ä¿¡æ¯
    user_question = state.get("user_question", "")
    qa_context = state.get("qa_context", "")
    messages = state.get("messages", [])
    
    # å¦‚æœæ²¡æœ‰ç”¨æˆ·é—®é¢˜ï¼Œä»æ¶ˆæ¯ä¸­è·å–
    if not user_question and messages:
        user_question = messages[-1].content if messages else ""
    
    # åˆ›å»ºLLMå®ä¾‹
    llm = configurable.create_llm(
        model_name=configurable.answer_model,
        temperature=configurable.final_report_temperature
    )
    
    # ç”Ÿæˆå›ç­”æç¤ºè¯
    prompt = f"""æ‚¨æ˜¯ä¸“ä¸šçš„è¿ç»´æŠ€æœ¯åŠ©æ‰‹ï¼Œæ”¯æŒæ•…éšœè¯Šæ–­ã€è¿ç»´é—®ç­”å’Œæ—¥å¸¸äº¤æµã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_question}

å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼š
{qa_context}

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜ç±»å‹å›ç­”ï¼š
- å¦‚æœæ˜¯è¿ç»´æŠ€æœ¯é—®é¢˜ï¼Œæä¾›ä¸“ä¸šçš„æŠ€æœ¯æŒ‡å¯¼
- å¦‚æœæ˜¯æ™®é€šèŠå¤©ï¼Œè‡ªç„¶å‹å¥½åœ°å›åº”
- å¦‚æœæ¶‰åŠä¹‹å‰çš„è¯Šæ–­å†…å®¹ï¼Œå¯ä»¥å¼•ç”¨ç›¸å…³ä¿¡æ¯
- ä¿æŒç®€æ´æ˜äº†ï¼Œä¸éœ€è¦ç”ŸæˆæŠ¥å‘Šæ ¼å¼

è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
    
    # ç”Ÿæˆå›ç­”
    response = llm.invoke(prompt)
    
    logger.info(f"é—®ç­”å›ç­”ç”Ÿæˆå®Œæˆ")
    
    return {
        "messages": [AIMessage(content=response.content)]
    }


# ================================
# å­å›¾åˆ›å»ºå‡½æ•°
# ================================

def create_sop_diagnosis_subgraph():
    """åˆ›å»ºSOPè¯Šæ–­å­å›¾"""
    
    # åˆ›å»ºå·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
    tool_node = ToolNode(all_tools)
    
    # åŒ…è£…å·¥å…·èŠ‚ç‚¹ä»¥æ·»åŠ æ‰“å°
    def execute_tools_node(state, config):
        print(f"âœ… æ‰§è¡ŒSOPå­å›¾èŠ‚ç‚¹: execute_tools_node")
        return tool_node.invoke(state, config)
    
    # åˆ›å»ºå­å›¾
    builder = StateGraph(DiagnosticState, config_schema=Configuration)
    
    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("analyze_question", analyze_question_node)
    builder.add_node("handle_insufficient_info", handle_insufficient_info_node)
    builder.add_node("plan_tools", plan_diagnosis_tools_node)
    builder.add_node("approval", approval_node)
    builder.add_node("execute_tools", execute_tools_node)
    builder.add_node("reflection", reflect_diagnosis_progress_node)
    
    # è®¾ç½®æµç¨‹
    builder.add_edge(START, "analyze_question")
    builder.add_conditional_edges(
        "analyze_question", 
        check_info_sufficient, 
        ["plan_tools", "handle_insufficient_info"]
    )
    builder.add_edge("handle_insufficient_info", END)
    builder.add_conditional_edges(
        "plan_tools",
        check_tool_calls,
        {"approval": "approval", "reflection": "reflection"}
    )
    builder.add_edge("approval", "execute_tools")
    builder.add_edge("execute_tools", "reflection")
    builder.add_conditional_edges(
        "reflection", 
        evaluate_diagnosis_progress, 
        ["plan_tools", END]
    )
    
    return builder.compile()


# æ—§ç‰ˆæœ¬çš„create_general_qa_subgraphå·²ç§»åŠ¨åˆ°general_qa_subgraph.pyæ–‡ä»¶ä¸­
# è¿™é‡Œä¿ç•™å¯¼å…¥ä»¥ä¿æŒå…¼å®¹æ€§
from .general_qa_subgraph import create_general_qa_subgraph


# ================================
# ä¸»å›¾åˆ›å»ºå’Œç¼–è¯‘
# ================================

def create_main_graph():
    """åˆ›å»ºä¸»å›¾ - åŒ…å«è·¯ç”±é€»è¾‘å’Œä¸¤ä¸ªå­å›¾"""
    
    # åˆ›å»ºä¸»å›¾
    builder = StateGraph(DiagnosticState, config_schema=Configuration)
    
    # æ·»åŠ æ„å›¾åˆ†æèŠ‚ç‚¹
    builder.add_node("analyze_intent", analyze_intent_node)
    
    # åˆ›å»ºå¹¶æ·»åŠ å­å›¾ - æŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„æ–¹å¼
    sop_diagnosis_subgraph = create_sop_diagnosis_subgraph()
    general_qa_subgraph = create_general_qa_subgraph()
    
    # å°†å­å›¾ä½œä¸ºèŠ‚ç‚¹æ·»åŠ åˆ°ä¸»å›¾
    builder.add_node("sop_diagnosis", sop_diagnosis_subgraph)
    builder.add_node("general_qa", general_qa_subgraph)
    
    # è®¾ç½®è·¯ç”±
    builder.add_edge(START, "analyze_intent")
    builder.add_conditional_edges(
        "analyze_intent",
        route_to_subgraph,
        {
            "sop_diagnosis": "sop_diagnosis",
            "general_qa": "general_qa"
        }
    )
    
    # ä¸¤ä¸ªå­å›¾æ‰§è¡Œå®Œæˆåéƒ½ç»“æŸ
    builder.add_edge("sop_diagnosis", END)
    builder.add_edge("general_qa", END)
    
    return builder


def compile_main_graph():
    """ç¼–è¯‘ä¸»å›¾"""
    builder = create_main_graph()
    checkpointer_type = os.getenv("CHECKPOINTER_TYPE", "memory")
    compiled_graph_tuple = compile_graph_with_checkpointer(builder, checkpointer_type)
    
    # compile_graph_with_checkpointer è¿”å› (graph, mode_name) å…ƒç»„
    compiled_graph = compiled_graph_tuple[0] if isinstance(compiled_graph_tuple, tuple) else compiled_graph_tuple
    
    # ä¿å­˜å›¾ç‰‡çš„é€»è¾‘å·²ç»åœ¨ compile_graph_with_checkpointer ä¸­å¤„ç†äº†
    
    return compiled_graph_tuple


# åˆ›å»ºbuilderå¹¶å¯¼å‡ºï¼ˆç”¨äºPostgreSQLæ¨¡å¼ï¼‰
builder = create_main_graph()

# å¯¼å‡ºç¼–è¯‘åçš„å›¾
graph = compile_main_graph()

# ä¿æŒå…¼å®¹æ€§ï¼Œæä¾›åŸæœ‰çš„æ¥å£
def get_diagnostic_agent():
    """è·å–è¯Šæ–­ä»£ç†å›¾å®ä¾‹ - ä¿æŒå‘åå…¼å®¹æ€§"""
    return graph


# å¯¼å‡ºä¸»è¦ç»„ä»¶
__all__ = ["graph", "builder", "get_diagnostic_agent", "create_main_graph", "compile_main_graph"]