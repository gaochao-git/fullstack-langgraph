import os
import requests
import json

from .tools_and_schemas import SearchQueryList, Reflection
from dotenv import load_dotenv
from langchain_core.messages import AIMessage
from langgraph.types import Send, interrupt, Command
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.postgres import PostgresSaver

from .state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
)
from .configuration import Configuration
from .prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
)
from langchain_deepseek import ChatDeepSeek
from .utils import (
    get_citations,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
)

load_dotenv()

if os.getenv("DEEPSEEK_API_KEY") is None:
    raise ValueError("DEEPSEEK_API_KEY is not set")

# DeepSeek å®¢æˆ·ç«¯åˆå§‹åŒ–ï¼ˆå¦‚æœå°†æ¥éœ€è¦ç”¨äºç½‘ç»œæœç´¢é›†æˆï¼‰


# èŠ‚ç‚¹
def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """LangGraph èŠ‚ç‚¹ï¼ŒåŸºäºç”¨æˆ·é—®é¢˜ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚

    ä½¿ç”¨ DeepSeek æ¨¡å‹æ ¹æ®ç”¨æˆ·é—®é¢˜åˆ›å»ºä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºç½‘ç»œç ”ç©¶ã€‚

    å‚æ•°ï¼š
        state: åŒ…å«ç”¨æˆ·é—®é¢˜çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œé…ç½®ï¼ŒåŒ…æ‹¬ LLM æä¾›å•†è®¾ç½®

    è¿”å›ï¼š
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬ search_query é”®ï¼Œå…¶ä¸­åŒ…å«ç”Ÿæˆçš„æŸ¥è¯¢
    """
    configurable = Configuration.from_runnable_config(config)

    # æ£€æŸ¥è‡ªå®šä¹‰åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries

    # åˆå§‹åŒ– DeepSeek Chat
    llm = ChatDeepSeek(
        model=configurable.query_generator_model,
        temperature=1.0,
        max_retries=2,
        api_key=os.getenv("DEEPSEEK_API_KEY"),
    )
    structured_llm = llm.with_structured_output(SearchQueryList)

    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = query_writer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        number_queries=state["initial_search_query_count"],
    )
    # ç”Ÿæˆæœç´¢æŸ¥è¯¢
    result = structured_llm.invoke(formatted_prompt)
    return {"search_query": result.query}


def continue_to_web_research(state: QueryGenerationState):
    """LangGraph èŠ‚ç‚¹ï¼Œå°†æœç´¢æŸ¥è¯¢å‘é€åˆ°ç½‘ç»œç ”ç©¶èŠ‚ç‚¹ã€‚

    ç”¨äºç”Ÿæˆ n ä¸ªç½‘ç»œç ”ç©¶èŠ‚ç‚¹ï¼Œæ¯ä¸ªæœç´¢æŸ¥è¯¢å¯¹åº”ä¸€ä¸ªã€‚
    """
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(state["search_query"])
    ]


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph èŠ‚ç‚¹ï¼Œç›´æ¥è°ƒç”¨ searchapi.io ç™¾åº¦å¼•æ“è¿›è¡Œç½‘ç»œç ”ç©¶ã€‚"""
    # å…ˆè¯¢é—®ç”¨æˆ·æ˜¯å¦å…è®¸æœç´¢
    human_response = interrupt({
        "message": f"æ˜¯å¦å…è®¸ä½¿ç”¨ç™¾åº¦æœç´¢ä»¥ä¸‹å†…å®¹ï¼Ÿ\n\næœç´¢å†…å®¹: {state['search_query']}\n\né€‰æ‹©'ç»§ç»­'å…è®¸æœç´¢ï¼Œé€‰æ‹©'å–æ¶ˆ'ç»“æŸæœç´¢ã€‚",
        "current_query": state["search_query"],
    })

    if not human_response:
        return {
            "sources_gathered": [],
            "search_query": [state["search_query"]],
            "web_research_result": ["ç”¨æˆ·å–æ¶ˆäº†æœç´¢æ“ä½œ"],
            "messages": [AIMessage(content="ç”¨æˆ·å–æ¶ˆäº†æœç´¢æ“ä½œï¼Œç ”ç©¶è¿‡ç¨‹å·²ç»“æŸã€‚")]
        }

    url = "https://www.searchapi.io/api/v1/search"
    params = {"engine": "baidu","q": state["search_query"],"api_key": os.getenv("SEARCHAPI_API_KEY")}
    response = requests.get(url, params=params)
    sources_gathered = []
    if response.status_code == 200:
        try:
            data = response.json()
            results = data.get("organic_results", [])
            if not results:
                result = "æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
            else:
                for idx, item in enumerate(results[:5]):
                    title = item.get("title", "")
                    link = item.get("link", "")
                    display_link = item.get("display_link", "")
                    date = item.get("date", "")
                    snippet = item.get("snippet", "")
                    sources_gathered.append({
                        "label": title or display_link or f"æ¥æº{idx+1}",
                        "short_url": link,
                        "value": link,
                        "title": title,
                        "snippet": snippet,
                        "display_link": display_link,
                        "date": date
                    })
                # ç”¨ sources_gathered æ‹¼æ¥ result
                format_str = "ã€{title}ã€‘\n{short_url}\n{display_link}\n{date}\n{snippet}\n"
                result = "\n".join([format_str.format(**src) for src in sources_gathered])
        except Exception as e:
            result = f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}"
    else:
        result = f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, é”™è¯¯ä¿¡æ¯: {response.text}"
    return {
        "sources_gathered": sources_gathered,
        "search_query": [state["search_query"]],
        "web_research_result": [result]
    }


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """LangGraph èŠ‚ç‚¹ï¼Œè¯†åˆ«çŸ¥è¯†å·®è·å¹¶ç”Ÿæˆæ½œåœ¨çš„åç»­æŸ¥è¯¢ã€‚

    åˆ†æå½“å‰æ‘˜è¦ä»¥è¯†åˆ«éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„é¢†åŸŸå¹¶ç”Ÿæˆæ½œåœ¨çš„åç»­æŸ¥è¯¢ã€‚
    ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºä»¥ JSON æ ¼å¼æå–åç»­æŸ¥è¯¢ã€‚

    å‚æ•°ï¼š
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œç ”ç©¶ä¸»é¢˜çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œé…ç½®ï¼ŒåŒ…æ‹¬ LLM æä¾›å•†è®¾ç½®

    è¿”å›ï¼š
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬ search_query é”®ï¼Œå…¶ä¸­åŒ…å«ç”Ÿæˆçš„åç»­æŸ¥è¯¢
    """
    # å¢åŠ ç ”ç©¶å¾ªç¯è®¡æ•°
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    research_loop_count = state["research_loop_count"]
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model", configurable.reflection_model)

    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = reflection_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n\n---\n\n".join(state["web_research_result"]),
    )
    # åˆå§‹åŒ–æ¨ç†æ¨¡å‹
    llm = ChatDeepSeek(
        model=reasoning_model,
        temperature=1.0,
        max_retries=2,
        api_key=os.getenv("DEEPSEEK_API_KEY"),
    )
    result = llm.with_structured_output(Reflection).invoke(formatted_prompt)

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": result.follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
    }


def evaluate_research(
    state: ReflectionState,
    config: RunnableConfig,
) -> OverallState:
    """LangGraph è·¯ç”±å‡½æ•°ï¼Œç¡®å®šç ”ç©¶æµç¨‹ä¸­çš„ä¸‹ä¸€æ­¥ã€‚

    é€šè¿‡å†³å®šæ˜¯å¦ç»§ç»­æ”¶é›†ä¿¡æ¯æˆ–æ ¹æ®é…ç½®çš„æœ€å¤§ç ”ç©¶å¾ªç¯æ•°å®Œæˆæ‘˜è¦æ¥æ§åˆ¶ç ”ç©¶å¾ªç¯ã€‚

    å‚æ•°ï¼š
        state: åŒ…å«ç ”ç©¶å¾ªç¯è®¡æ•°çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œé…ç½®ï¼ŒåŒ…æ‹¬ max_research_loops è®¾ç½®

    è¿”å›ï¼š
        æŒ‡ç¤ºè¦è®¿é—®çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„å­—ç¬¦ä¸²å­—é¢é‡ï¼ˆ"web_research" æˆ– "finalize_summary"ï¼‰
    """
    configurable = Configuration.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )
    if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
        return "finalize_answer"
    else:
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state["number_of_ran_queries"] + int(idx),
                },
            )
            for idx, follow_up_query in enumerate(state["follow_up_queries"])
        ]


def finalize_answer(state: OverallState, config: RunnableConfig):
    """LangGraph èŠ‚ç‚¹ï¼Œå®Œæˆç ”ç©¶æ‘˜è¦ã€‚

    é€šè¿‡å»é‡å’Œæ ¼å¼åŒ–æ¥æºï¼Œç„¶åå°†å®ƒä»¬ä¸è¿è¡Œæ‘˜è¦ç»“åˆèµ·æ¥åˆ›å»ºå…·æœ‰é€‚å½“å¼•ç”¨çš„ç»“æ„è‰¯å¥½çš„ç ”ç©¶æŠ¥å‘Šï¼Œå‡†å¤‡æœ€ç»ˆè¾“å‡ºã€‚

    å‚æ•°ï¼š
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œæ”¶é›†æ¥æºçš„å½“å‰å›¾çŠ¶æ€

    è¿”å›ï¼š
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬ running_summary é”®ï¼Œå…¶ä¸­åŒ…å«æ ¼å¼åŒ–çš„å¸¦æœ‰æ¥æºçš„æœ€ç»ˆæ‘˜è¦
    """
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­è¯¢é—®ç”¨æˆ·ï¼ˆå½“ç ”ç©¶å¾ªç¯æ¬¡æ•°è¾ƒå¤šæ—¶ï¼‰  
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model

    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    formatted_prompt = answer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n---\n\n".join(state["web_research_result"]),
    )

    # åˆå§‹åŒ–æ¨ç†æ¨¡å‹ï¼Œé»˜è®¤ä¸º DeepSeek Chat
    llm = ChatDeepSeek(
        model=reasoning_model,
        temperature=0,
        max_retries=2,
        api_key=os.getenv("DEEPSEEK_API_KEY"),
    )
    result = llm.invoke(formatted_prompt)

    # ç”¨åŸå§‹ URL æ›¿æ¢çŸ­ URLï¼Œå¹¶å°†æ‰€æœ‰ä½¿ç”¨çš„ URL æ·»åŠ åˆ° sources_gathered
    unique_sources = []
    for source in state["sources_gathered"]:
        if source["short_url"] in result.content:
            result.content = result.content.replace(
                source["short_url"], source["value"]
            )
            unique_sources.append(source)

    return {
        "messages": [AIMessage(content=result.content)],
        "sources_gathered": unique_sources,
    }


# åˆ›å»ºæˆ‘ä»¬çš„ Agent å›¾
builder = StateGraph(OverallState, config_schema=Configuration)

# å®šä¹‰æˆ‘ä»¬å°†å¾ªç¯çš„èŠ‚ç‚¹
builder.add_node("generate_query", generate_query)
builder.add_node("web_research", web_research)
builder.add_node("reflection", reflection)
builder.add_node("finalize_answer", finalize_answer)

# å°†å…¥å£ç‚¹è®¾ç½®ä¸º `generate_query`
# è¿™æ„å‘³ç€è¿™ä¸ªèŠ‚ç‚¹æ˜¯ç¬¬ä¸€ä¸ªè¢«è°ƒç”¨çš„
builder.add_edge(START, "generate_query")
# æ·»åŠ æ¡ä»¶è¾¹ä»¥åœ¨å¹¶è¡Œåˆ†æ”¯ä¸­ç»§ç»­æœç´¢æŸ¥è¯¢
builder.add_conditional_edges(
    "generate_query", continue_to_web_research, ["web_research"]
)
# åæ€ç½‘ç»œç ”ç©¶
builder.add_edge("web_research", "reflection")
# è¯„ä¼°ç ”ç©¶
builder.add_conditional_edges(
    "reflection", evaluate_research, ["web_research", "finalize_answer"]
)
# å®Œæˆç­”æ¡ˆ
builder.add_edge("finalize_answer", END)

# ç¼–è¯‘å›¾ - æ·»åŠ å†…å­˜checkpointerä»¥æ”¯æŒä¸­æ–­å’Œæ¢å¤
from langgraph.checkpoint.memory import MemorySaver
checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer, name="pro-search-agent")

# ç”Ÿæˆå¹¶ä¿å­˜å›¾ç‰‡åˆ°å½“å‰ä»£ç æ–‡ä»¶æ‰€åœ¨ç›®å½•
graph_image = graph.get_graph().draw_mermaid_png()
current_dir = os.path.dirname(os.path.abspath(__file__))
graph_image_path = os.path.join(current_dir, "research_agent_graph.png")
with open(graph_image_path, "wb") as f: f.write(graph_image)
print(f"ğŸ“ ç ”ç©¶ä»£ç†å›¾å·²ä¿å­˜åˆ° {graph_image_path}")
